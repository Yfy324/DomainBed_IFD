trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3249148647  0.3343373494  0.3373493976  0.3285024155  0.3245823389  0.3358433735  0.3313253012  0.3308383234  0.3273809524  0.3368421053  0.3353293413  0.4427710843  0.4457831325  0.4410876133  0.4457831325  0.4441087613  0.4457831325  0.4452773613  0.4371257485  0.3180722892  0.3285024155  0.0000000000  9.8018226624  0.2909131050  0             8.9770052433 
0.4964156477  0.6551204819  0.6566265060  0.4661835749  0.4844868735  0.6987951807  0.6746987952  0.7440119760  0.7559523810  0.6375939850  0.6227544910  0.7936746988  0.7771084337  0.5528700906  0.6024096386  0.8126888218  0.7951807229  0.6896551724  0.6826347305  0.5132530120  0.5217391304  0.4833836858  9.0295150757  0.6689839363  10            0.1423694134 
0.5824331535  0.7635542169  0.7409638554  0.5869565217  0.5680190931  0.7590361446  0.7108433735  0.8592814371  0.8452380952  0.7067669173  0.7185628743  0.8328313253  0.8012048193  0.5679758308  0.6024096386  0.8247734139  0.7831325301  0.6746626687  0.6706586826  0.6506024096  0.5241545894  0.9667673716  7.9738317013  0.6689839363  20            0.1318763494 
0.6105657037  0.8433734940  0.8554216867  0.5700483092  0.6587112172  0.8403614458  0.8072289157  0.8802395210  0.8690476190  0.8330827068  0.8143712575  0.7364457831  0.7048192771  0.6888217523  0.7108433735  0.7703927492  0.7469879518  0.8680659670  0.8443113772  0.6096385542  0.6038647343  1.4501510574  6.8004337788  0.6689839363  30            0.1321704388 
0.6956721042  0.8674698795  0.8795180723  0.7004830918  0.6348448687  0.8403614458  0.8192771084  0.8892215569  0.8809523810  0.8000000000  0.8263473054  0.8403614458  0.8072289157  0.7477341390  0.8132530120  0.8323262840  0.8132530120  0.8170914543  0.8562874251  0.7927710843  0.6545893720  1.9335347432  5.7251540661  0.6689839363  40            0.1316635132 
0.7444597351  0.8554216867  0.8554216867  0.7681159420  0.6754176611  0.8253012048  0.8192771084  0.9176646707  0.8869047619  0.8406015038  0.8562874251  0.8689759036  0.8373493976  0.8232628399  0.8493975904  0.8761329305  0.8554216867  0.9025487256  0.9221556886  0.7975903614  0.7367149758  2.4169184290  5.2775163651  0.6689839363  50            0.1334007502 
0.7653848489  0.8177710843  0.8012048193  0.7391304348  0.7541766110  0.8448795181  0.8253012048  0.8772455090  0.8690476190  0.8857142857  0.8922155689  0.8900602410  0.8734939759  0.8247734139  0.8674698795  0.8972809668  0.8674698795  0.9640179910  0.9520958084  0.7518072289  0.8164251208  2.9003021148  4.9711043358  0.6689839363  60            0.1316703081 
0.7755626220  0.8478915663  0.8493975904  0.7367149758  0.7828162291  0.8765060241  0.8433734940  0.8802395210  0.8690476190  0.8842105263  0.8982035928  0.9337349398  0.9277108434  0.8036253776  0.8433734940  0.9305135952  0.9036144578  0.9700149925  0.9760479042  0.7542168675  0.8285024155  3.3836858006  4.6075893402  0.6689839363  70            0.1326710224 
0.7929679946  0.9051204819  0.8855421687  0.7560386473  0.8162291169  0.8614457831  0.8313253012  0.9535928144  0.9345238095  0.9548872180  0.9161676647  0.9759036145  0.9759036145  0.8051359517  0.8493975904  0.9622356495  0.9578313253  0.9490254873  0.9401197605  0.7638554217  0.8357487923  3.8670694864  4.2597911835  0.6689839363  80            0.1337933064 
0.7647592271  0.9262048193  0.9096385542  0.7657004831  0.7565632458  0.8418674699  0.8012048193  0.9625748503  0.9583333333  0.9563909774  0.8982035928  0.9849397590  0.9879518072  0.7598187311  0.7891566265  0.9773413897  0.9819277108  0.9055472264  0.9221556886  0.7759036145  0.7608695652  4.3504531722  3.9730290890  0.6689839363  90            0.1309231520 
0.7900781709  0.9141566265  0.9096385542  0.7946859903  0.7661097852  0.8840361446  0.8373493976  0.9535928144  0.9345238095  0.9729323308  0.9281437126  0.9879518072  1.0000000000  0.7598187311  0.7891566265  0.9864048338  0.9939759036  0.9550224888  0.9161676647  0.8000000000  0.7995169082  4.8338368580  3.9861331224  0.6689839363  100           0.1345989704 
0.7901845978  0.9141566265  0.9096385542  0.8164251208  0.7231503580  0.8463855422  0.8192771084  0.9461077844  0.9166666667  0.9609022556  0.9520958084  0.9969879518  0.9879518072  0.6978851964  0.7349397590  0.9924471299  0.9939759036  0.9415292354  0.9101796407  0.8385542169  0.7826086957  5.3172205438  3.7609659195  0.6689839363  110           0.1318936586 
0.7955301369  0.9246987952  0.9156626506  0.8091787440  0.7565632458  0.8719879518  0.8373493976  0.9565868263  0.9345238095  0.9759398496  0.9281437126  0.9879518072  1.0000000000  0.7719033233  0.8012048193  0.9939577039  1.0000000000  0.9580209895  0.9281437126  0.8192771084  0.7971014493  5.8006042296  3.5258513212  0.6689839363  120           0.1373147964 
0.7985838967  0.9066265060  0.9036144578  0.8212560386  0.7398568019  0.8584337349  0.8192771084  0.9326347305  0.8988095238  0.9819548872  0.9520958084  0.9939759036  1.0000000000  0.7356495468  0.7650602410  0.9818731118  0.9879518072  0.9640179910  0.9281437126  0.8457831325  0.7874396135  6.2839879154  3.3680377245  0.6689839363  130           0.1334715128 
0.8027116595  0.8990963855  0.8975903614  0.8115942029  0.7780429594  0.9126506024  0.8674698795  0.9236526946  0.8988095238  0.9609022556  0.9520958084  0.9939759036  1.0000000000  0.7688821752  0.8012048193  0.9743202417  0.9819277108  0.9790104948  0.9520958084  0.8192771084  0.8019323671  6.7673716012  3.3129409313  0.6689839363  140           0.1351806402 
0.7869436204  0.9231927711  0.9096385542  0.7898550725  0.8066825776  0.8990963855  0.8614457831  0.9520958084  0.9642857143  0.9849624060  0.9520958084  0.9849397590  1.0000000000  0.8006042296  0.8313253012  0.9818731118  0.9879518072  0.9730134933  0.9640718563  0.7879518072  0.7632850242  7.2507552870  3.2580876827  0.6689839363  150           0.1335297346 
0.7773508652  0.9021084337  0.8975903614  0.8043478261  0.7804295943  0.9006024096  0.8554216867  0.9401197605  0.9166666667  0.9789473684  0.9520958084  0.9939759036  1.0000000000  0.7824773414  0.8072289157  0.9894259819  0.9939759036  0.9790104948  0.9520958084  0.8048192771  0.7198067633  7.7341389728  3.1509783506  0.6689839363  160           0.1299692631 
0.8026165955  0.8885542169  0.8915662651  0.8115942029  0.8114558473  0.9352409639  0.8614457831  0.9116766467  0.8869047619  0.9533834586  0.9580838323  0.9909638554  1.0000000000  0.8202416918  0.8253012048  0.9712990937  0.9819277108  0.9865067466  0.9700598802  0.8096385542  0.7777777778  8.2175226586  3.2068575144  0.6689839363  170           0.1335854053 
0.7827768478  0.9051204819  0.8975903614  0.8260869565  0.7780429594  0.9156626506  0.8554216867  0.9386227545  0.9107142857  0.9804511278  0.9580838323  0.9969879518  1.0000000000  0.7492447130  0.7831325301  0.9848942598  0.9879518072  0.9790104948  0.9640718563  0.8313253012  0.6956521739  8.7009063444  3.1024514198  0.6689839363  180           0.1356004000 
0.7826560088  0.9171686747  0.8975903614  0.7995169082  0.8257756563  0.9307228916  0.8795180723  0.9446107784  0.9226190476  0.9834586466  0.9580838323  0.9909638554  1.0000000000  0.8308157100  0.8433734940  0.9818731118  0.9879518072  0.9865067466  0.9700598802  0.7927710843  0.7125603865  9.1842900302  2.9629331350  0.6689839363  190           0.1341784716 
0.7628107185  0.9472891566  0.9216867470  0.8067632850  0.8019093079  0.9066265060  0.8674698795  0.9640718563  0.9761904762  0.9879699248  0.9520958084  0.9969879518  1.0000000000  0.7522658610  0.7951807229  0.9894259819  0.9939759036  0.9775112444  0.9461077844  0.7759036145  0.6666666667  9.6676737160  2.8753795624  0.6689839363  200           0.1323156357 
0.7687873524  0.9036144578  0.8915662651  0.8019323671  0.8210023866  0.9201807229  0.8734939759  0.9341317365  0.9047619048  0.9759398496  0.9640718563  0.9969879518  1.0000000000  0.7990936556  0.8012048193  0.9879154079  0.9879518072  0.9865067466  0.9700598802  0.7831325301  0.6690821256  10.151057401  2.9552166462  0.6689839363  210           0.1421859264 
0.7876062411  0.9427710843  0.9156626506  0.8357487923  0.7756563246  0.9066265060  0.8554216867  0.9595808383  0.9345238095  0.9864661654  0.9700598802  0.9969879518  1.0000000000  0.7296072508  0.7771084337  0.9864048338  0.9939759036  0.9820089955  0.9520958084  0.8457831325  0.6932367150  10.634441087  2.8500604153  0.6689839363  220           0.1372458935 
0.7839227027  0.9171686747  0.9096385542  0.8309178744  0.8019093079  0.9292168675  0.8855421687  0.9461077844  0.9166666667  0.9759398496  0.9640718563  0.9969879518  1.0000000000  0.7779456193  0.8012048193  0.9848942598  0.9879518072  0.9895052474  0.9700598802  0.8144578313  0.6884057971  11.117824773  2.8746609211  0.6689839363  230           0.1329227209 
0.8007977953  0.9081325301  0.8975903614  0.8067632850  0.8138424821  0.9759036145  0.8915662651  0.9281437126  0.8928571429  0.9669172932  0.9640718563  0.9834337349  0.9819277108  0.8821752266  0.8795180723  0.9773413897  0.9819277108  0.9925037481  0.9820359281  0.8096385542  0.7729468599  11.601208459  2.8586269140  0.6689839363  240           0.1368826866 
0.7766806912  0.9066265060  0.9036144578  0.8212560386  0.8019093079  0.9382530120  0.8855421687  0.9371257485  0.9166666667  0.9759398496  0.9640718563  0.9969879518  1.0000000000  0.7870090634  0.7891566265  0.9879154079  0.9879518072  0.9940029985  0.9760479042  0.8072289157  0.6763285024  12.084592145  2.7876571417  0.6689839363  250           0.1399210215 
0.7681936039  0.9036144578  0.8915662651  0.8067632850  0.8186157518  0.9352409639  0.8734939759  0.9371257485  0.9047619048  0.9804511278  0.9700598802  0.9969879518  1.0000000000  0.7975830816  0.8072289157  0.9894259819  0.9939759036  0.9970014993  1.0000000000  0.7783132530  0.6690821256  12.567975830  2.7183266401  0.6689839363  260           0.1336056709 
0.7808688040  0.9111445783  0.8975903614  0.8140096618  0.8138424821  0.9442771084  0.8734939759  0.9311377246  0.9047619048  0.9789473684  0.9640718563  0.9969879518  1.0000000000  0.8036253776  0.8072289157  0.9894259819  0.9939759036  0.9940029985  0.9760479042  0.8120481928  0.6835748792  13.051359516  2.7529865742  0.6689839363  270           0.1330445528 
0.7719173988  0.9141566265  0.8975903614  0.8188405797  0.7756563246  0.9487951807  0.8795180723  0.9326347305  0.8988095238  0.9789473684  0.9640718563  0.9969879518  1.0000000000  0.7839879154  0.8012048193  0.9909365559  1.0000000000  0.9910044978  0.9640718563  0.8265060241  0.6666666667  13.534743202  2.6514115334  0.6689839363  280           0.1303610802 
0.7592162847  0.9487951807  0.9277108434  0.8115942029  0.7899761337  0.9382530120  0.8855421687  0.9535928144  0.9345238095  0.9909774436  0.9760479042  0.9969879518  1.0000000000  0.7613293051  0.7710843373  0.9894259819  0.9939759036  0.9925037481  0.9700598802  0.7879518072  0.6473429952  14.018126888  2.6256034136  0.6689839363  290           0.1324517965 
0.7597755277  0.9518072289  0.9156626506  0.7946859903  0.8066825776  0.9412650602  0.8855421687  0.9535928144  0.9345238095  0.9909774436  0.9760479042  0.9969879518  1.0000000000  0.7900302115  0.8012048193  0.9894259819  0.9939759036  0.9955022489  0.9820359281  0.7783132530  0.6594202899  14.501510574  2.6568859577  0.6689839363  300           0.1314445257 
0.7371685281  0.9864457831  0.9578313253  0.8309178744  0.6849642005  0.8629518072  0.8132530120  0.9940119760  0.9880952381  0.9939849624  0.9640718563  0.9924698795  0.9939759036  0.6344410876  0.6746987952  0.9909365559  0.9879518072  0.9610194903  0.9161676647  0.8240963855  0.6086956522  14.984894259  2.6284685850  0.6689839363  310           0.1363345861 
0.7736240212  0.8780120482  0.8614457831  0.7801932367  0.8186157518  0.9804216867  0.8975903614  0.8952095808  0.8690476190  0.9593984962  0.9580838323  0.9924698795  0.9939759036  0.8791540785  0.8795180723  0.9667673716  0.9638554217  1.0000000000  0.9880239521  0.7855421687  0.7101449275  15.468277945  2.5060200930  0.6689839363  320           0.1374365330 
0.7700627073  0.9593373494  0.9337349398  0.8164251208  0.7947494033  0.9352409639  0.8855421687  0.9640718563  0.9404761905  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.7280966767  0.7951807229  0.9939577039  1.0000000000  0.9940029985  0.9760479042  0.8024096386  0.6666666667  15.951661631  2.5874823093  0.6689839363  330           0.1377892017 
0.7664194254  0.9503012048  0.9337349398  0.8091787440  0.8042959427  0.9563253012  0.8975903614  0.9476047904  0.9345238095  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.7719033233  0.8012048193  0.9894259819  0.9939759036  1.0000000000  0.9880239521  0.7879518072  0.6642512077  16.435045317  2.5349662542  0.6689839363  340           0.1576049089 
0.7749295164  0.9442771084  0.9337349398  0.8333333333  0.7780429594  0.9623493976  0.8975903614  0.9446107784  0.9107142857  0.9879699248  0.9760479042  0.9969879518  1.0000000000  0.7265861027  0.7891566265  0.9894259819  0.9939759036  0.9985007496  0.9700598802  0.8265060241  0.6618357488  16.918429003  2.4693954706  0.6689839363  350           0.1338688850 
0.7688003093  0.9533132530  0.9216867470  0.8043478261  0.8162291169  0.9743975904  0.8975903614  0.9520958084  0.9285714286  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.8066465257  0.8313253012  0.9939577039  1.0000000000  1.0000000000  0.9880239521  0.7855421687  0.6690821256  17.401812688  2.3640891552  0.6689839363  360           0.1353270292 
0.7815647530  0.9006024096  0.8915662651  0.8212560386  0.7780429594  0.9728915663  0.8915662651  0.9281437126  0.8928571429  0.9729323308  0.9640718563  1.0000000000  1.0000000000  0.8111782477  0.8253012048  0.9818731118  0.9879518072  1.0000000000  0.9880239521  0.8385542169  0.6884057971  17.885196374  2.4056055546  0.6689839363  370           0.1399509907 
0.7490931107  0.9698795181  0.9397590361  0.8212560386  0.7374701671  0.8960843373  0.8373493976  0.9745508982  0.9702380952  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.6918429003  0.7349397590  0.9969788520  1.0000000000  0.9880059970  0.9640718563  0.8144578313  0.6231884058  18.368580060  2.3074011326  0.6689839363  380           0.1332459450 
0.7580704298  0.9834337349  0.9578313253  0.8309178744  0.7661097852  0.9307228916  0.8554216867  0.9940119760  0.9880952381  0.9954887218  0.9580838323  0.9939759036  1.0000000000  0.7099697885  0.7590361446  0.9954682779  0.9939759036  0.9895052474  0.9580838323  0.8048192771  0.6304347826  18.851963746  2.3839377880  0.6689839363  390           0.1341460466 


trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3321380989  0.4427710843  0.4457831325  0.4441087613  0.4457831325  0.4441087613  0.4457831325  0.4467766117  0.4431137725  0.3325301205  0.3333333333  0.3333333333  0.3293556086  0.0000000000  4.1191186905  0.1364407539  0             0.3850982189 
0.5529351747  0.7846385542  0.7168674699  0.6223564955  0.6265060241  0.8489425982  0.7951807229  0.7901049475  0.7485029940  0.5469879518  0.5000000000  0.6062801932  0.5584725537  0.4833836858  3.6833510876  0.2169575691  10            0.0771698236 
0.5626946373  0.8569277108  0.8012048193  0.7401812689  0.7108433735  0.8761329305  0.8313253012  0.7451274363  0.6886227545  0.6120481928  0.5362318841  0.5893719807  0.5131264916  0.9667673716  3.1346363306  0.2990632057  20            0.0739009142 
0.5892443143  0.8795180723  0.8192771084  0.7447129909  0.7530120482  0.8957703927  0.8493975904  0.7976011994  0.7784431138  0.6457831325  0.5603864734  0.6376811594  0.5131264916  1.4501510574  2.6345947981  0.2990632057  30            0.0770245552 
0.5935649762  0.9427710843  0.9397590361  0.8519637462  0.8795180723  0.9682779456  0.9457831325  0.8845577211  0.8982035928  0.6578313253  0.6111111111  0.6256038647  0.4797136038  1.9335347432  2.2146517754  0.3011770248  40            0.0748078823 
0.6056710256  0.9653614458  0.9578313253  0.8519637462  0.8795180723  0.9924471299  0.9578313253  0.9460269865  0.9401197605  0.6698795181  0.6328502415  0.6521739130  0.4677804296  2.4169184290  1.9436827779  0.3011770248  50            0.0728605270 
0.6063700237  0.9909638554  0.9759036145  0.8731117825  0.8915662651  1.0000000000  1.0000000000  0.9220389805  0.9401197605  0.6674698795  0.6763285024  0.6449275362  0.4367541766  2.9003021148  1.8111300230  0.3011770248  60            0.0748443365 
0.6178203110  0.9924698795  0.9819277108  0.8685800604  0.8855421687  1.0000000000  1.0000000000  0.9610194903  0.9640718563  0.6819277108  0.6956521739  0.6521739130  0.4415274463  3.3836858006  1.5559073687  0.3011770248  70            0.0752022505 
0.6423568335  0.9879518072  0.9759036145  0.8761329305  0.9036144578  0.9969788520  1.0000000000  0.9955022489  0.9820359281  0.6795180723  0.6956521739  0.6787439614  0.5155131265  3.8670694864  1.3678249717  0.3011770248  80            0.0768299818 
0.6466486019  0.9909638554  0.9759036145  0.8912386707  0.8915662651  1.0000000000  1.0000000000  0.9955022489  0.9820359281  0.7036144578  0.7125603865  0.6811594203  0.4892601432  4.3504531722  1.3370166421  0.3011770248  90            0.0757241249 
0.6346319351  0.9954819277  0.9819277108  0.8867069486  0.8855421687  1.0000000000  1.0000000000  0.9895052474  0.9820359281  0.6867469880  0.7101449275  0.6690821256  0.4725536993  4.8338368580  1.2972882152  0.3011770248  100           0.0756561756 
0.6651926929  0.9789156627  0.9638554217  0.9441087613  0.9337349398  0.9939577039  1.0000000000  0.9985007496  0.9940119760  0.6843373494  0.7342995169  0.6908212560  0.5513126492  5.3172205438  1.2412883878  0.3832826614  110           0.0777646303 
0.6520935008  0.9954819277  0.9819277108  0.9169184290  0.8975903614  1.0000000000  1.0000000000  0.9925037481  0.9820359281  0.6987951807  0.7222222222  0.7004830918  0.4868735084  5.8006042296  1.2047279716  0.3832826614  120           0.0759564877 
0.6743455889  0.9939759036  0.9759036145  0.9244712991  0.9277108434  1.0000000000  1.0000000000  0.9985007496  0.9820359281  0.7180722892  0.7391304348  0.7270531401  0.5131264916  6.2839879154  1.0986387014  0.3832826614  130           0.0796065807 
0.6562830687  0.9939759036  0.9759036145  0.9078549849  0.9216867470  1.0000000000  1.0000000000  0.9985007496  0.9820359281  0.7012048193  0.7149758454  0.7101449275  0.4988066826  6.7673716012  1.0720269620  0.3832826614  140           0.0767344475 
0.6773346334  0.9879518072  0.9759036145  0.9607250755  0.9397590361  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7204819277  0.7342995169  0.7318840580  0.5226730310  7.2507552870  1.0827932954  0.3832826614  150           0.0779492617 
0.6394268229  0.9969879518  1.0000000000  0.9003021148  0.9036144578  1.0000000000  1.0000000000  0.9985007496  0.9820359281  0.6867469880  0.7053140097  0.6811594203  0.4844868735  7.7341389728  1.0861074030  0.3832826614  160           0.0822290897 
0.6623129855  0.9954819277  0.9819277108  0.9592145015  0.9337349398  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7156626506  0.7222222222  0.7125603865  0.4988066826  8.2175226586  1.0338519037  0.3832826614  170           0.0751353502 
0.6226742326  1.0000000000  1.0000000000  0.9169184290  0.9096385542  1.0000000000  1.0000000000  0.9925037481  0.9820359281  0.6915662651  0.7028985507  0.6642512077  0.4319809069  8.7009063444  1.0583878338  0.3832826614  180           0.0767041922 
0.6538546528  0.9969879518  0.9879518072  0.9712990937  0.9337349398  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.6987951807  0.7125603865  0.7004830918  0.5035799523  9.1842900302  1.0298919380  0.3832826614  190           0.0764614105 
0.6430902678  1.0000000000  0.9879518072  0.9425981873  0.9277108434  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7036144578  0.7053140097  0.6956521739  0.4677804296  9.6676737160  1.0225949466  0.3832826614  200           0.0780181170 
0.6809346792  0.9939759036  0.9759036145  0.9743202417  0.9457831325  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7349397590  0.7246376812  0.7367149758  0.5274463007  10.151057401  0.9382536232  0.3832826614  210           0.0775214195 
0.6527505095  1.0000000000  1.0000000000  0.9531722054  0.9216867470  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7301204819  0.7101449275  0.7077294686  0.4630071599  10.634441087  0.9761257172  0.3832826614  220           0.0750865459 
0.6507704749  1.0000000000  0.9879518072  0.9622356495  0.9216867470  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.6987951807  0.6908212560  0.6884057971  0.5250596659  11.117824773  0.9901221871  0.3832826614  230           0.0758622885 
0.6628995280  0.9969879518  0.9879518072  0.9788519637  0.9518072289  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7204819277  0.7149758454  0.7125603865  0.5035799523  11.601208459  0.9803270876  0.3832826614  240           0.0769379139 
0.6436451454  1.0000000000  0.9879518072  0.9561933535  0.9216867470  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7012048193  0.6956521739  0.6932367150  0.4844868735  12.084592145  0.9108646870  0.3832826614  250           0.0791538000 
0.6724575692  1.0000000000  0.9879518072  0.9712990937  0.9337349398  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7253012048  0.7125603865  0.7149758454  0.5369928401  12.567975830  0.9355276227  0.3832826614  260           0.0797814608 
0.6485551906  1.0000000000  0.9879518072  0.9622356495  0.9216867470  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7253012048  0.7053140097  0.7101449275  0.4534606205  13.051359516  0.9449706495  0.3832826614  270           0.0761394978 
0.6383010615  1.0000000000  0.9879518072  0.9592145015  0.9216867470  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7180722892  0.6884057971  0.6956521739  0.4510739857  13.534743202  0.9474331975  0.3832826614  280           0.0755908489 
0.6629656984  1.0000000000  0.9879518072  0.9864048338  0.9578313253  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7421686747  0.7077294686  0.7246376812  0.4773269690  14.018126888  0.9229729950  0.3832826614  290           0.0780792952 
0.6441769499  1.0000000000  0.9879518072  0.9697885196  0.9397590361  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7012048193  0.6859903382  0.6811594203  0.5083532220  14.501510574  0.8597900748  0.3832826614  300           0.0747243166 
0.6648147083  1.0000000000  0.9879518072  0.9833836858  0.9457831325  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7518072289  0.7053140097  0.7391304348  0.4630071599  14.984894259  0.9149310827  0.3832826614  310           0.0759730101 
0.6479685787  1.0000000000  0.9879518072  0.9864048338  0.9578313253  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7325301205  0.6932367150  0.7198067633  0.4463007160  15.468277945  0.8810000420  0.3832826614  320           0.0762720346 
0.6497182289  1.0000000000  0.9879518072  0.9879154079  0.9638554217  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7277108434  0.6956521739  0.7077294686  0.4677804296  15.951661631  0.8570035994  0.3832826614  330           0.0765352011 
0.6467867631  1.0000000000  0.9879518072  0.9848942598  0.9518072289  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7373493976  0.6932367150  0.7198067633  0.4367541766  16.435045317  0.8926410854  0.3832826614  340           0.0777554989 
0.6503753765  1.0000000000  0.9879518072  0.9909365559  0.9759036145  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7349397590  0.6980676329  0.7198067633  0.4486873508  16.918429003  0.8673420548  0.3832826614  350           0.0767572880 
0.6449780791  1.0000000000  0.9879518072  0.9864048338  0.9578313253  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7325301205  0.6932367150  0.7173913043  0.4367541766  17.401812688  0.8597647727  0.3832826614  360           0.0770507336 
0.6461368215  1.0000000000  0.9879518072  0.9879154079  0.9638554217  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7301204819  0.6932367150  0.7077294686  0.4534606205  17.885196374  0.8436748862  0.3832826614  370           0.0817651749 
0.6538487630  1.0000000000  0.9879518072  0.9803625378  0.9457831325  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7204819277  0.6884057971  0.7053140097  0.5011933174  18.368580060  0.8374804437  0.3832826614  380           0.0765974760 
0.6305156741  1.0000000000  1.0000000000  0.9637462236  0.9156626506  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7180722892  0.6690821256  0.7053140097  0.4295942721  18.851963746  0.8294047117  0.3832826614  390           0.0764504433 
0.6642050927  1.0000000000  0.9879518072  0.9909365559  0.9638554217  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7493975904  0.7028985507  0.7391304348  0.4653937947  19.335347432  0.8446766436  0.3832826614  400           0.0769172192 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3123273836  0.4442771084  0.4879518072  0.3067632850  0.2935560859  0.4728915663  0.4578313253  0.4431137725  0.4404761905  0.5007518797  0.4970059880  0.4879518072  0.4819277108  0.4637462236  0.4879518072  0.4818731118  0.4759036145  0.4752623688  0.4970059880  0.3180722892  0.3309178744  0.0000000000  9.8018226624  0.2909131050  0             0.4593205452 
0.6073299076  0.7515060241  0.7289156627  0.6183574879  0.5202863962  0.7996987952  0.7530120482  0.8413173653  0.8214285714  0.7022556391  0.6766467066  0.8177710843  0.7771084337  0.5951661631  0.6265060241  0.8066465257  0.7951807229  0.7166416792  0.6946107784  0.6602409639  0.6304347826  0.4833836858  8.4112134933  0.4781041145  10            0.1418802500 
0.7016404937  0.8509036145  0.8493975904  0.6787439614  0.6706443914  0.8478915663  0.8132530120  0.8862275449  0.8690476190  0.8045112782  0.8203592814  0.7816265060  0.7771084337  0.7945619335  0.8072289157  0.7824773414  0.7590361446  0.8755622189  0.8982035928  0.7301204819  0.7270531401  0.9667673716  6.3636736393  0.6689839363  20            0.1442508698 
0.7444565471  0.8644578313  0.8795180723  0.7560386473  0.6658711217  0.7801204819  0.7590361446  0.8952095808  0.8809523810  0.8481203008  0.8622754491  0.9593373494  0.9337349398  0.7643504532  0.7951807229  0.9365558912  0.9277108434  0.9100449775  0.9161676647  0.8506024096  0.7053140097  1.4501510574  5.2330739498  0.6689839363  30            0.1344221592 
0.7960823127  0.8569277108  0.8614457831  0.7632850242  0.7804295943  0.8885542169  0.8554216867  0.8952095808  0.8809523810  0.8947368421  0.9041916168  0.9337349398  0.9518072289  0.8172205438  0.8614457831  0.9335347432  0.8915662651  0.9700149925  0.9401197605  0.7855421687  0.8550724638  1.9335347432  4.3166639328  0.6689839363  40            0.1330962420 
0.7978651523  0.8493975904  0.8433734940  0.7608695652  0.7923627685  0.8719879518  0.8373493976  0.8817365269  0.8630952381  0.8962406015  0.9221556886  0.9683734940  0.9578313253  0.8429003021  0.8795180723  0.9592145015  0.9698795181  0.9850074963  0.9880239521  0.7734939759  0.8647342995  2.4169184290  3.8839556932  0.6689839363  50            0.1303567886 
0.7833538297  0.8554216867  0.8554216867  0.7801932367  0.8042959427  0.8780120482  0.8614457831  0.9146706587  0.8988095238  0.9338345865  0.9281437126  0.9924698795  0.9939759036  0.7885196375  0.8072289157  0.9818731118  0.9759036145  0.9775112444  0.9580838323  0.7445783133  0.8043478261  2.9003021148  3.7093802929  0.6689839363  60            0.1328128576 
0.7705416457  0.8990963855  0.8734939759  0.7753623188  0.8520286396  0.8855421687  0.8674698795  0.9610778443  0.9523809524  0.9714285714  0.9341317365  0.9849397590  0.9879518072  0.7885196375  0.8072289157  0.9788519637  0.9879518072  0.9700149925  0.9520958084  0.7228915663  0.7318840580  3.3836858006  3.6169949293  0.6689839363  70            0.1358640909 
0.7543868914  0.9503012048  0.9337349398  0.7874396135  0.7923627685  0.8795180723  0.8674698795  0.9940119760  0.9880952381  0.9819548872  0.9281437126  0.9774096386  0.9939759036  0.7809667674  0.8132530120  0.9712990937  0.9939759036  0.9655172414  0.9221556886  0.7734939759  0.6642512077  3.8670694864  3.2759988546  0.6689839363  80            0.1311500311 
0.7225911046  0.9713855422  0.9457831325  0.7922705314  0.7255369928  0.8042168675  0.7469879518  0.9895209581  0.9702380952  0.9593984962  0.8982035928  0.9653614458  0.9819277108  0.6752265861  0.6927710843  0.9501510574  0.9698795181  0.9025487256  0.8862275449  0.7614457831  0.6111111111  4.3504531722  3.1591630220  0.6689839363  90            0.1322351933 
0.7936796024  0.8975903614  0.8795180723  0.8333333333  0.7684964200  0.9126506024  0.8554216867  0.9461077844  0.9047619048  0.9744360902  0.9580838323  0.9969879518  1.0000000000  0.7311178248  0.7590361446  0.9833836858  0.9939759036  0.9790104948  0.9520958084  0.8240963855  0.7487922705  4.8338368580  3.3813821316  0.6689839363  100           0.1303773165 
0.7715281207  0.9367469880  0.9156626506  0.8502415459  0.6992840095  0.8855421687  0.8433734940  0.9640718563  0.9642857143  0.9909774436  0.9760479042  0.9969879518  1.0000000000  0.7024169184  0.7409638554  0.9954682779  0.9939759036  0.9715142429  0.9341317365  0.8530120482  0.6835748792  5.3172205438  3.2594683409  0.6689839363  110           0.1364153385 
0.7798784325  0.9623493976  0.9337349398  0.8454106280  0.7326968974  0.8930722892  0.8493975904  0.9940119760  0.9880952381  0.9954887218  0.9461077844  0.9864457831  0.9939759036  0.7356495468  0.7530120482  0.9848942598  0.9759036145  0.9685157421  0.9341317365  0.8578313253  0.6835748792  5.8006042296  3.0172693729  0.6689839363  120           0.1388420820 
0.7715380980  0.9563253012  0.9457831325  0.8671497585  0.6921241050  0.8448795181  0.7891566265  0.9760479042  0.9761904762  0.9954887218  0.9700598802  0.9954819277  0.9939759036  0.6057401813  0.6325301205  0.9848942598  0.9879518072  0.9595202399  0.9101796407  0.8722891566  0.6545893720  6.2839879154  2.7539482355  0.6689839363  130           0.1393537760 
0.7737420194  0.9472891566  0.9337349398  0.8309178744  0.7732696897  0.9186746988  0.8674698795  0.9760479042  0.9880952381  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.7220543807  0.7710843373  0.9894259819  0.9939759036  0.9865067466  0.9580838323  0.8168674699  0.6739130435  6.7673716012  2.7183576107  0.6689839363  140           0.1427792788 
0.7513603617  0.9367469880  0.9277108434  0.8019323671  0.7947494033  0.9186746988  0.8795180723  0.9655688623  0.9702380952  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.7688821752  0.8132530120  0.9954682779  0.9939759036  0.9895052474  0.9820359281  0.7734939759  0.6352657005  7.2507552870  2.6531002998  0.6689839363  150           0.1342765570 
0.7417705862  0.9231927711  0.8975903614  0.8043478261  0.7708830549  0.9246987952  0.8795180723  0.9565868263  0.9464285714  0.9909774436  0.9760479042  0.9969879518  1.0000000000  0.7719033233  0.8012048193  0.9954682779  0.9939759036  0.9880059970  0.9760479042  0.7734939759  0.6183574879  7.7341389728  2.5531229019  0.6689839363  160           0.1378289461 
0.7591587755  0.9156626506  0.8915662651  0.7971014493  0.8138424821  0.9457831325  0.8915662651  0.9341317365  0.9047619048  0.9834586466  0.9700598802  0.9969879518  1.0000000000  0.8141993958  0.8373493976  0.9909365559  0.9879518072  0.9970014993  1.0000000000  0.7638554217  0.6618357488  8.2175226586  2.6278716564  0.6689839363  170           0.1361904621 
0.7497488032  0.9201807229  0.9096385542  0.8212560386  0.7183770883  0.9277108434  0.8795180723  0.9401197605  0.9047619048  0.9834586466  0.9700598802  0.9969879518  1.0000000000  0.7280966767  0.7710843373  0.9894259819  0.9939759036  0.9895052474  0.9580838323  0.8240963855  0.6352657005  8.7009063444  2.5548637152  0.6689839363  180           0.1358700752 
0.7755492483  0.9518072289  0.9277108434  0.8236714976  0.7732696897  0.9774096386  0.8975903614  0.9520958084  0.9166666667  0.9894736842  0.9700598802  0.9939759036  0.9879518072  0.8398791541  0.8313253012  0.9924471299  0.9939759036  0.9910044978  0.9640718563  0.8240963855  0.6811594203  9.1842900302  2.3945407867  0.6689839363  190           0.1327135086 
0.7502649490  0.9367469880  0.9156626506  0.8212560386  0.7541766110  0.9277108434  0.8795180723  0.9595808383  0.9345238095  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.7341389728  0.7590361446  0.9954682779  0.9939759036  0.9895052474  0.9700598802  0.7903614458  0.6352657005  9.6676737160  2.3629396200  0.6689839363  200           0.1308965921 
0.7441731576  0.9533132530  0.9216867470  0.7995169082  0.7780429594  0.9548192771  0.8915662651  0.9715568862  0.9583333333  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.8021148036  0.8253012048  0.9954682779  0.9939759036  0.9970014993  0.9880239521  0.7590361446  0.6400966184  10.151057401  2.3983461142  0.6689839363  210           0.1342508316 
0.7370376425  0.9653614458  0.9216867470  0.8164251208  0.7374701671  0.9322289157  0.8734939759  0.9865269461  0.9702380952  0.9969924812  0.9760479042  0.9954819277  0.9939759036  0.7099697885  0.7469879518  1.0000000000  1.0000000000  0.9895052474  0.9580838323  0.7783132530  0.6159420290  10.634441087  2.3309751987  0.6689839363  220           0.1376555920 
0.7225983106  0.9593373494  0.9216867470  0.8019323671  0.7231503580  0.9397590361  0.8795180723  0.9760479042  0.9642857143  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.7371601208  0.7590361446  1.0000000000  1.0000000000  0.9910044978  0.9880239521  0.7614457831  0.6038647343  11.117824773  2.3211129189  0.6689839363  230           0.1323889017 
0.7430474657  0.9503012048  0.9216867470  0.8019323671  0.7470167064  0.9849397590  0.9036144578  0.9580838323  0.9404761905  0.9879699248  0.9640718563  0.9954819277  0.9939759036  0.8580060423  0.8313253012  0.9954682779  0.9939759036  1.0000000000  0.9880239521  0.7783132530  0.6449275362  11.601208459  2.3104955435  0.6689839363  240           0.1340789318 
0.7142493845  0.9442771084  0.9216867470  0.8019323671  0.6873508353  0.9382530120  0.8734939759  0.9610778443  0.9523809524  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.7280966767  0.7469879518  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7662650602  0.6014492754  12.084592145  2.3065865040  0.6689839363  250           0.1363277435 
0.7285174352  0.9518072289  0.9156626506  0.7801932367  0.7661097852  0.9638554217  0.9036144578  0.9685628743  0.9583333333  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.8081570997  0.8012048193  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7445783133  0.6231884058  12.567975830  2.2047649145  0.6689839363  260           0.1330592871 
0.7219958316  0.9442771084  0.9216867470  0.7826086957  0.7207637232  0.9638554217  0.9036144578  0.9520958084  0.9166666667  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.7990936556  0.8132530120  0.9969788520  1.0000000000  1.0000000000  1.0000000000  0.7710843373  0.6135265700  13.051359516  2.2080620527  0.6689839363  270           0.1374078989 
0.7286800552  0.9141566265  0.9096385542  0.8019323671  0.7040572792  0.9879518072  0.9036144578  0.9176646707  0.8869047619  0.9849624060  0.9760479042  0.9984939759  0.9939759036  0.8368580060  0.8192771084  0.9969788520  1.0000000000  1.0000000000  0.9880239521  0.7855421687  0.6231884058  13.534743202  2.1098768115  0.6689839363  280           0.1341832876 
0.7160163569  0.9653614458  0.9216867470  0.7898550725  0.7040572792  0.9698795181  0.9036144578  0.9595808383  0.9345238095  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.7628398792  0.7771084337  0.9969788520  1.0000000000  1.0000000000  1.0000000000  0.7566265060  0.6135265700  14.018126888  2.1374124765  0.6689839363  290           0.1341617107 
0.7166015138  0.9683734940  0.9216867470  0.7729468599  0.7112171838  0.9849397590  0.9036144578  0.9610778443  0.9285714286  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.8141993958  0.8253012048  0.9969788520  1.0000000000  1.0000000000  1.0000000000  0.7518072289  0.6304347826  14.501510574  2.1743551373  0.6689839363  300           0.1471072912 
0.6855246797  0.9939759036  0.9638554217  0.7801932367  0.5990453461  0.9126506024  0.8554216867  0.9970059880  0.9880952381  0.9984962406  0.9820359281  0.9984939759  0.9939759036  0.6465256798  0.6987951807  0.9984894260  0.9939759036  0.9850074963  0.9640718563  0.7759036145  0.5869565217  14.984894259  2.1330017567  0.6689839363  310           0.1371376991 
0.7286470742  0.8855421687  0.8554216867  0.7657004831  0.7207637232  0.9969879518  0.9277108434  0.8832335329  0.8809523810  0.9714285714  0.9700598802  0.9954819277  0.9939759036  0.9078549849  0.8975903614  0.9894259819  0.9939759036  1.0000000000  1.0000000000  0.7566265060  0.6714975845  15.468277945  2.0130770802  0.6689839363  320           0.1416865110 
0.7167555421  0.9743975904  0.9457831325  0.8188405797  0.6539379475  0.9728915663  0.8915662651  0.9805389222  0.9702380952  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.6722054381  0.7289156627  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7831325301  0.6111111111  15.951661631  2.1320615172  0.6689839363  330           0.1340830088 
0.7184792089  0.9141566265  0.9096385542  0.7777777778  0.6825775656  0.9924698795  0.9096385542  0.9056886228  0.8750000000  0.9789473684  0.9760479042  1.0000000000  1.0000000000  0.8126888218  0.8192771084  0.9848942598  0.9879518072  1.0000000000  1.0000000000  0.7855421687  0.6280193237  16.435045317  2.0601480484  0.6689839363  340           0.1361765385 
0.7171780095  0.9548192771  0.9277108434  0.7753623188  0.7207637232  0.9909638554  0.9156626506  0.9386227545  0.8869047619  0.9879699248  0.9760479042  1.0000000000  1.0000000000  0.8036253776  0.8313253012  0.9939577039  1.0000000000  1.0000000000  1.0000000000  0.7493975904  0.6231884058  16.918429003  2.1356146455  0.6689839363  350           0.1380263805 
0.7190845982  0.9623493976  0.9337349398  0.7874396135  0.6849642005  0.9924698795  0.9096385542  0.9745508982  0.9583333333  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.8247734139  0.8313253012  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7710843373  0.6328502415  17.401812688  1.9973293424  0.6689839363  360           0.1436953545 
0.7058313084  0.9171686747  0.9216867470  0.7681159420  0.6754176611  0.9939759036  0.9156626506  0.9221556886  0.8928571429  0.9789473684  0.9760479042  1.0000000000  1.0000000000  0.8232628399  0.8373493976  0.9939577039  1.0000000000  1.0000000000  1.0000000000  0.7662650602  0.6135265700  17.885196374  1.9796690345  0.6689839363  370           0.1432810068 
0.6818223640  0.9969879518  0.9759036145  0.7632850242  0.6300715990  0.9322289157  0.8734939759  0.9940119760  0.9761904762  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.6510574018  0.7048192771  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7518072289  0.5821256039  18.368580060  1.8894233704  0.6689839363  380           0.1450855017 
0.7022254422  0.9954819277  0.9698795181  0.7729468599  0.6706443914  0.9759036145  0.8915662651  0.9940119760  0.9761904762  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.6978851964  0.7349397590  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7614457831  0.6038647343  18.851963746  1.9490293741  0.6689839363  390           0.1355442762 
0.7039694109  0.9277108434  0.9036144578  0.7705314010  0.6968973747  0.9969879518  0.9277108434  0.9251497006  0.8928571429  0.9804511278  0.9700598802  0.9984939759  0.9939759036  0.8746223565  0.8493975904  0.9969788520  1.0000000000  1.0000000000  1.0000000000  0.7421686747  0.6062801932  19.335347432  1.9962107539  0.6689839363  400           0.1348049641 
0.6769584651  0.9954819277  0.9698795181  0.7512077295  0.6467780430  0.9698795181  0.8915662651  0.9970059880  0.9761904762  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.7190332326  0.7710843373  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7228915663  0.5869565217  19.818731117  1.9795769453  0.6689839363  410           0.1355200052 
0.6932770166  0.9518072289  0.9397590361  0.7729468599  0.6348448687  0.9698795181  0.8915662651  0.9775449102  0.9464285714  0.9909774436  0.9760479042  1.0000000000  1.0000000000  0.7054380665  0.7650602410  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7590361446  0.6062801932  20.302114803  1.9646850228  0.6689839363  420           0.1318562508 
0.7107400374  0.9954819277  0.9819277108  0.7971014493  0.6491646778  0.9819277108  0.8915662651  0.9985029940  0.9821428571  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.7129909366  0.7590361446  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7686746988  0.6280193237  20.785498489  1.9454986453  0.6689839363  430           0.1318539381 
0.6608427901  1.0000000000  0.9879518072  0.7487922705  0.5799522673  0.9382530120  0.8493975904  0.9985029940  0.9940476190  0.9984962406  0.9700598802  1.0000000000  1.0000000000  0.6253776435  0.6626506024  1.0000000000  1.0000000000  0.9940029985  0.9640718563  0.7445783133  0.5700483092  21.268882175  1.8525405645  0.6689839363  440           0.1337567806 
0.6781158913  0.9984939759  0.9819277108  0.7512077295  0.6682577566  0.9819277108  0.9156626506  1.0000000000  1.0000000000  0.9984962406  0.9700598802  1.0000000000  1.0000000000  0.7688821752  0.8132530120  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.6987951807  0.5942028986  21.752265861  1.9574332714  0.6689839363  450           0.1368113518 
0.6889837931  0.9909638554  0.9638554217  0.7608695652  0.6610978520  0.9804216867  0.8975903614  0.9985029940  0.9821428571  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.6978851964  0.7590361446  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7373493976  0.5966183575  22.235649546  1.9756937504  0.6689839363  460           0.1328456640 
0.6794098152  0.9683734940  0.9457831325  0.7512077295  0.6300715990  0.9939759036  0.9156626506  0.9655688623  0.9226190476  0.9954887218  0.9820359281  1.0000000000  1.0000000000  0.7311178248  0.7710843373  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7469879518  0.5893719807  22.719033232  1.8648617268  0.6689839363  470           0.1358503580 
0.6633129870  1.0000000000  0.9879518072  0.7705314010  0.5608591885  0.9216867470  0.8554216867  1.0000000000  1.0000000000  1.0000000000  0.9760479042  1.0000000000  1.0000000000  0.6314199396  0.6626506024  1.0000000000  1.0000000000  0.9850074963  0.9640718563  0.7493975904  0.5724637681  23.202416918  1.9536469340  0.6689839363  480           0.1362053156 
0.6710754401  0.9954819277  0.9819277108  0.7705314010  0.5942720764  0.9216867470  0.8674698795  0.9985029940  0.9940476190  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.6404833837  0.6867469880  1.0000000000  1.0000000000  0.9910044978  0.9880239521  0.7277108434  0.5917874396  23.685800604  1.9664438367  0.6689839363  490           0.1329150200 
0.6950843150  0.9939759036  0.9759036145  0.7681159420  0.6372315036  0.9984939759  0.9337349398  0.9880239521  0.9642857143  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.8066465257  0.8433734940  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7542168675  0.6207729469  24.169184290  1.9080843568  0.6689839363  500           0.1343694210 
0.6781618987  0.9728915663  0.9638554217  0.7512077295  0.6491646778  0.9819277108  0.9156626506  0.9850299401  0.9642857143  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.7703927492  0.8072289157  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7180722892  0.5942028986  24.652567975  1.8540266871  0.6689839363  510           0.1447335482 
0.6529855509  1.0000000000  0.9879518072  0.7391304348  0.5894988067  0.9427710843  0.8915662651  0.9985029940  0.9940476190  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.6389728097  0.6807228916  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7084337349  0.5748792271  25.135951661  1.8150141478  0.6689839363  520           0.1369403839 
0.6505585901  1.0000000000  0.9879518072  0.7294685990  0.5942720764  0.9397590361  0.8795180723  0.9985029940  0.9940476190  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.6435045317  0.6867469880  1.0000000000  1.0000000000  0.9880059970  0.9760479042  0.7036144578  0.5748792271  25.619335347  1.8082557082  0.6689839363  530           0.1362354517 
0.6805255299  0.9789156627  0.9638554217  0.7367149758  0.6682577566  0.9819277108  0.9156626506  0.9865269461  0.9583333333  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.7794561934  0.8313253012  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7084337349  0.6086956522  26.102719033  1.9046169877  0.6689839363  540           0.1344230652 
0.6698229499  0.9939759036  0.9759036145  0.7367149758  0.6062052506  0.9879518072  0.9036144578  0.9910179641  0.9642857143  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.7250755287  0.7710843373  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7421686747  0.5942028986  26.586102719  1.8854852915  0.6689839363  550           0.1333629131 
0.6919239200  0.9397590361  0.9036144578  0.7318840580  0.6897374702  0.9984939759  0.9337349398  0.9206586826  0.8750000000  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.8791540785  0.8674698795  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7253012048  0.6207729469  27.069486404  1.8503143787  0.6689839363  560           0.1402591705 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.0005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3123273836  0.4442771084  0.4879518072  0.3067632850  0.2935560859  0.4728915663  0.4578313253  0.4431137725  0.4404761905  0.5007518797  0.4970059880  0.4879518072  0.4819277108  0.4637462236  0.4879518072  0.4818731118  0.4759036145  0.4752623688  0.4970059880  0.3180722892  0.3309178744  0.0000000000  9.8018226624  0.2909131050  0             0.4861655235 
0.6145762150  0.7530120482  0.7349397590  0.6328502415  0.5178997613  0.7891566265  0.7228915663  0.8473053892  0.8333333333  0.6947368421  0.6946107784  0.8237951807  0.7891566265  0.5876132931  0.6204819277  0.8036253776  0.7951807229  0.7181409295  0.6886227545  0.6722891566  0.6352657005  0.4833836858  8.4277603626  0.4782485962  10            0.1398535490 
0.7016117391  0.8403614458  0.8433734940  0.6956521739  0.6825775656  0.8313253012  0.7831325301  0.8862275449  0.8690476190  0.8360902256  0.8263473054  0.7861445783  0.7951807229  0.7854984894  0.8192771084  0.7824773414  0.7590361446  0.8545727136  0.8742514970  0.7180722892  0.7101449275  0.9667673716  6.4095106602  0.6670284271  20            0.1377212286 
0.7371987379  0.8524096386  0.8554216867  0.7487922705  0.6730310263  0.8192771084  0.7951807229  0.8952095808  0.8809523810  0.8646616541  0.8682634731  0.9216867470  0.9156626506  0.7809667674  0.8132530120  0.9078549849  0.8614457831  0.9055472264  0.9341317365  0.8337349398  0.6932367150  1.4501510574  5.2610068798  0.6670284271  30            0.1361507416 
0.7877189745  0.8524096386  0.8554216867  0.7560386473  0.7494033413  0.8870481928  0.8493975904  0.8967065868  0.8750000000  0.9082706767  0.8982035928  0.9186746988  0.9156626506  0.8096676737  0.8554216867  0.9154078550  0.8795180723  0.9610194903  0.9401197605  0.7903614458  0.8550724638  1.9335347432  4.3653030634  0.6670284271  40            0.1401922941 
0.7974642335  0.8463855422  0.8433734940  0.7826086957  0.7159904535  0.8719879518  0.8253012048  0.8847305389  0.8630952381  0.8902255639  0.9101796407  0.9728915663  0.9759036145  0.8338368580  0.8674698795  0.9456193353  0.9397590361  0.9820089955  0.9760479042  0.8192771084  0.8719806763  2.4169184290  3.8749961138  0.6670284271  50            0.1382128954 
0.7684329272  0.9036144578  0.8795180723  0.7995169082  0.7422434368  0.8418674699  0.8012048193  0.9610778443  0.9523809524  0.9473684211  0.8862275449  0.9879518072  0.9879518072  0.7567975831  0.7650602410  0.9909365559  0.9879518072  0.9475262369  0.9221556886  0.7638554217  0.7681159420  2.9003021148  3.5783655882  0.6670284271  60            0.1357015610 
0.7802811537  0.8765060241  0.8433734940  0.7657004831  0.8210023866  0.9081325301  0.8734939759  0.9326347305  0.8869047619  0.9518796992  0.9281437126  0.9909638554  0.9879518072  0.8036253776  0.8192771084  0.9758308157  0.9759036145  0.9805097451  0.9580838323  0.7493975904  0.7850241546  3.3836858006  3.5113860846  0.6670284271  70            0.1432707548 
0.7901500923  0.9322289157  0.8975903614  0.8091787440  0.7374701671  0.9246987952  0.8554216867  0.9655688623  0.9464285714  0.9744360902  0.9341317365  0.9834337349  0.9939759036  0.8172205438  0.8373493976  0.9803625378  0.9939759036  0.9730134933  0.9401197605  0.8240963855  0.7898550725  3.8670694864  3.2035618544  0.6670284271  80            0.1441260815 
0.7341321601  0.9638554217  0.9277108434  0.7995169082  0.6992840095  0.8298192771  0.8012048193  0.9895209581  0.9821428571  0.9609022556  0.9041916168  0.9774096386  0.9939759036  0.6993957704  0.7289156627  0.9607250755  0.9759036145  0.9310344828  0.8922155689  0.7807228916  0.6570048309  4.3504531722  3.0553841829  0.6670284271  90            0.1367543459 
0.7696638687  0.8509036145  0.8373493976  0.7463768116  0.7398568019  0.9698795181  0.8795180723  0.8697604790  0.8511904762  0.8827067669  0.9041916168  0.9186746988  0.9156626506  0.8700906344  0.8674698795  0.8972809668  0.9036144578  0.9745127436  0.9341317365  0.7373493976  0.8550724638  4.8338368580  3.3405333757  0.6670284271  100           0.1461178780 
0.7856741552  0.8493975904  0.8192771084  0.7439613527  0.8353221957  0.9518072289  0.8795180723  0.8937125749  0.8750000000  0.9353383459  0.9461077844  0.9804216867  0.9698795181  0.8519637462  0.8795180723  0.9456193353  0.9277108434  0.9970014993  1.0000000000  0.7469879518  0.8164251208  5.3172205438  3.2761055946  0.6670284271  110           0.1380405426 
0.7910441531  0.8704819277  0.8192771084  0.7584541063  0.8591885442  0.9503012048  0.8734939759  0.8997005988  0.8750000000  0.9684210526  0.9700598802  0.9668674699  0.9638554217  0.8821752266  0.9036144578  0.9577039275  0.9638554217  0.9925037481  0.9940119760  0.7349397590  0.8115942029  5.8006042296  3.0309207678  0.6670284271  120           0.1360735416 
0.7671356763  0.9141566265  0.8855421687  0.8043478261  0.7661097852  0.9006024096  0.8674698795  0.9341317365  0.9047619048  0.9864661654  0.9700598802  0.9969879518  1.0000000000  0.7356495468  0.7650602410  0.9758308157  0.9759036145  0.9775112444  0.9700598802  0.7927710843  0.7053140097  6.2839879154  2.8334627151  0.6670284271  130           0.1360682011 
0.7875949476  0.9397590361  0.9156626506  0.8188405797  0.7875894988  0.9427710843  0.8915662651  0.9625748503  0.9583333333  0.9909774436  0.9760479042  0.9969879518  1.0000000000  0.7885196375  0.8072289157  0.9879154079  0.9879518072  0.9820089955  0.9640718563  0.8048192771  0.7391304348  6.7673716012  2.6578455448  0.6670284271  140           0.1335730076 
0.7605233740  0.9397590361  0.9277108434  0.8164251208  0.7541766110  0.9201807229  0.8734939759  0.9640718563  0.9642857143  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.7885196375  0.8192771084  0.9954682779  0.9939759036  0.9835082459  0.9700598802  0.8024096386  0.6690821256  7.2507552870  2.5655116081  0.6670284271  150           0.1370962381 
0.7527293255  0.9503012048  0.9337349398  0.8285024155  0.7350835322  0.9156626506  0.8674698795  0.9730538922  0.9761904762  0.9939849624  0.9520958084  0.9954819277  0.9939759036  0.7688821752  0.8132530120  0.9969788520  1.0000000000  0.9820089955  0.9640718563  0.8048192771  0.6425120773  7.7341389728  2.3915805101  0.6670284271  160           0.1386092663 
0.7527366010  0.9216867470  0.9156626506  0.8285024155  0.7350835322  0.9412650602  0.8855421687  0.9431137725  0.9047619048  0.9849624060  0.9760479042  0.9969879518  1.0000000000  0.7945619335  0.8192771084  0.9939577039  0.9879518072  0.9985007496  0.9940119760  0.7927710843  0.6545893720  8.2175226586  2.4603143930  0.6670284271  170           0.1335686922 
0.7510788267  0.9382530120  0.9337349398  0.8526570048  0.6706443914  0.9156626506  0.8554216867  0.9491017964  0.9166666667  0.9909774436  0.9760479042  0.9969879518  1.0000000000  0.7114803625  0.7650602410  0.9954682779  0.9939759036  0.9775112444  0.9461077844  0.8602409639  0.6207729469  8.7009063444  2.3806277037  0.6670284271  180           0.1366837740 
0.7745243712  0.9518072289  0.9156626506  0.8623188406  0.7064439141  0.9789156627  0.8915662651  0.9535928144  0.9345238095  0.9879699248  0.9760479042  0.9924698795  0.9939759036  0.8353474320  0.8132530120  0.9924471299  0.9939759036  0.9940029985  0.9640718563  0.8554216867  0.6739130435  9.1842900302  2.3045034647  0.6670284271  190           0.1366138458 
0.7522333428  0.9533132530  0.9216867470  0.8454106280  0.6921241050  0.9352409639  0.8734939759  0.9595808383  0.9345238095  0.9909774436  0.9760479042  0.9969879518  1.0000000000  0.7356495468  0.7530120482  0.9954682779  0.9939759036  0.9895052474  0.9580838323  0.8409638554  0.6304347826  9.6676737160  2.3096613646  0.6670284271  200           0.1327392817 
0.7188269143  0.9563253012  0.9096385542  0.7657004831  0.7804295943  0.9126506024  0.8674698795  0.9850299401  0.9761904762  0.9954887218  0.9461077844  0.9954819277  0.9939759036  0.8036253776  0.8192771084  0.9984894260  0.9939759036  0.9880059970  0.9880239521  0.7204819277  0.6086956522  10.151057401  2.2178819776  0.6670284271  210           0.1337035418 
0.7606802430  0.9683734940  0.9216867470  0.8429951691  0.6945107399  0.9563253012  0.8855421687  0.9760479042  0.9642857143  0.9969924812  0.9760479042  0.9969879518  1.0000000000  0.7432024169  0.7590361446  0.9924471299  0.9939759036  0.9940029985  0.9640718563  0.8409638554  0.6642512077  10.634441087  2.3099085331  0.6670284271  220           0.1362835169 
0.7288225817  0.9578313253  0.9397590361  0.8333333333  0.6515513126  0.9397590361  0.8795180723  0.9670658683  0.9642857143  0.9894736842  0.9700598802  0.9969879518  1.0000000000  0.7175226586  0.7289156627  0.9984894260  0.9939759036  0.9850074963  0.9520958084  0.8120481928  0.6183574879  11.117824773  2.3262855411  0.6670284271  230           0.1325271368 
0.7613344110  0.9804216867  0.9457831325  0.8502415459  0.6730310263  0.9713855422  0.8855421687  0.9865269461  0.9702380952  0.9939849624  0.9760479042  0.9954819277  0.9939759036  0.8021148036  0.8132530120  0.9954682779  0.9939759036  0.9880059970  0.9640718563  0.8650602410  0.6570048309  11.601208459  2.3591875553  0.6670284271  240           0.1355320692 
0.7234108028  0.9442771084  0.9337349398  0.8357487923  0.6420047733  0.9412650602  0.8734939759  0.9491017964  0.9166666667  0.9849624060  0.9760479042  1.0000000000  1.0000000000  0.6827794562  0.7108433735  1.0000000000  1.0000000000  0.9895052474  0.9580838323  0.8216867470  0.5942028986  12.084592145  2.2102750897  0.6670284271  250           0.1359836102 
0.7154384756  0.9593373494  0.9216867470  0.7898550725  0.6968973747  0.9533132530  0.8975903614  0.9685628743  0.9345238095  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.7809667674  0.7891566265  0.9984894260  0.9939759036  1.0000000000  1.0000000000  0.7493975904  0.6256038647  12.567975830  2.1562572479  0.6670284271  260           0.1325424671 
0.7275084253  0.9066265060  0.8795180723  0.7753623188  0.6945107399  0.9834337349  0.8975903614  0.8922155689  0.8809523810  0.9789473684  0.9640718563  0.9924698795  0.9939759036  0.8761329305  0.8795180723  0.9879154079  0.9759036145  1.0000000000  1.0000000000  0.7734939759  0.6666666667  13.051359516  2.1262004018  0.6670284271  270           0.1364878178 
0.7197014199  0.9036144578  0.8795180723  0.7874396135  0.6801909308  0.9849397590  0.9036144578  0.9056886228  0.8869047619  0.9774436090  0.9700598802  1.0000000000  1.0000000000  0.8716012085  0.8734939759  0.9939577039  0.9879518072  1.0000000000  1.0000000000  0.7734939759  0.6376811594  13.534743202  2.1791433573  0.6670284271  280           0.1368204832 
0.6974421952  0.9969879518  0.9638554217  0.7874396135  0.6587112172  0.9051204819  0.8614457831  0.9970059880  0.9880952381  0.9954887218  0.9461077844  0.9924698795  0.9939759036  0.6858006042  0.7108433735  0.9969788520  0.9879518072  0.9745127436  0.9461077844  0.7421686747  0.6014492754  14.018126888  2.2677106619  0.6670284271  290           0.1369488239 
0.7395566875  0.9262048193  0.9216867470  0.7995169082  0.6968973747  0.9819277108  0.9036144578  0.9206586826  0.8988095238  0.9834586466  0.9700598802  1.0000000000  1.0000000000  0.8111782477  0.8132530120  0.9803625378  0.9698795181  1.0000000000  0.9880239521  0.8096385542  0.6521739130  14.501510574  2.2723658442  0.6670284271  300           0.1356081486 
0.7198638315  0.9819277108  0.9518072289  0.8357487923  0.6109785203  0.9171686747  0.8253012048  0.9835329341  0.9702380952  0.9939849624  0.9760479042  0.9984939759  0.9939759036  0.6178247734  0.6686746988  1.0000000000  1.0000000000  0.9805097451  0.9341317365  0.8506024096  0.5821256039  14.984894259  2.2701641083  0.6670284271  310           0.1346875191 
0.7130575916  0.9608433735  0.9036144578  0.7971014493  0.6849642005  0.9849397590  0.9036144578  0.9535928144  0.9226190476  0.9909774436  0.9760479042  1.0000000000  1.0000000000  0.8534743202  0.8614457831  0.9984894260  0.9939759036  1.0000000000  1.0000000000  0.7518072289  0.6183574879  15.468277945  2.1630864143  0.6670284271  320           0.1330221891 
0.7354432673  0.9653614458  0.9337349398  0.8285024155  0.6515513126  0.9668674699  0.8795180723  0.9550898204  0.9285714286  0.9909774436  0.9760479042  1.0000000000  1.0000000000  0.6767371601  0.7228915663  0.9969788520  1.0000000000  0.9925037481  0.9700598802  0.8481927711  0.6135265700  15.951661631  2.3282460690  0.6670284271  330           0.1373937130 
0.7383718923  0.9201807229  0.9216867470  0.7850241546  0.6849642005  0.9894578313  0.8975903614  0.8952095808  0.8809523810  0.9699248120  0.9640718563  0.9969879518  1.0000000000  0.8338368580  0.8072289157  0.9607250755  0.9638554217  1.0000000000  0.9880239521  0.8313253012  0.6521739130  16.435045317  2.1850959778  0.6670284271  340           0.1338374853 
0.6949605659  0.9924698795  0.9578313253  0.7705314010  0.6849642005  0.9262048193  0.8734939759  0.9895209581  0.9821428571  0.9969924812  0.9640718563  0.9924698795  0.9939759036  0.7250755287  0.7590361446  0.9969788520  0.9879518072  0.9925037481  0.9700598802  0.7204819277  0.6038647343  16.918429003  2.3756549954  0.6670284271  350           0.1366311550 
0.7209537017  0.9713855422  0.9337349398  0.8043478261  0.6610978520  0.9834337349  0.8975903614  0.9700598802  0.9404761905  0.9909774436  0.9760479042  1.0000000000  1.0000000000  0.8277945619  0.8192771084  0.9969788520  1.0000000000  0.9985007496  0.9820359281  0.7951807229  0.6231884058  17.401812688  2.1797853708  0.6670284271  360           0.1344634056 
0.7342586805  0.8930722892  0.8734939759  0.8019323671  0.6467780430  0.9894578313  0.9216867470  0.8817365269  0.8750000000  0.9548872180  0.9401197605  0.9939759036  0.9879518072  0.9078549849  0.8614457831  0.9879154079  0.9879518072  1.0000000000  0.9880239521  0.8337349398  0.6545893720  17.885196374  2.1298249960  0.6670284271  370           0.1355821609 
0.6783992808  0.9759036145  0.9518072289  0.7922705314  0.5560859189  0.9186746988  0.8554216867  0.9910179641  0.9761904762  0.9939849624  0.9520958084  0.9984939759  0.9939759036  0.6540785498  0.6807228916  1.0000000000  1.0000000000  0.9835082459  0.9580838323  0.7903614458  0.5748792271  18.368580060  2.0986353636  0.6670284271  380           0.1349746227 
0.6800554610  0.9954819277  0.9939759036  0.7657004831  0.6157517900  0.9201807229  0.8614457831  0.9985029940  0.9821428571  0.9954887218  0.9461077844  0.9894578313  0.9939759036  0.6918429003  0.7228915663  0.9954682779  0.9939759036  0.9745127436  0.9580838323  0.7493975904  0.5893719807  18.851963746  2.1936711311  0.6670284271  390           0.1347681761 
0.7088766154  0.9397590361  0.9156626506  0.7826086957  0.6682577566  0.9879518072  0.8915662651  0.9266467066  0.8869047619  0.9684210526  0.9580838323  0.9984939759  0.9939759036  0.8474320242  0.8373493976  0.9939577039  1.0000000000  1.0000000000  1.0000000000  0.7590361446  0.6256038647  19.335347432  2.2141021729  0.6670284271  400           0.1389403820 
0.6584505432  0.9849397590  0.9638554217  0.7729468599  0.5775656325  0.8930722892  0.8373493976  0.9895209581  0.9702380952  0.9954887218  0.9461077844  0.9954819277  0.9939759036  0.5951661631  0.6265060241  1.0000000000  1.0000000000  0.9805097451  0.9341317365  0.7180722892  0.5652173913  19.818731117  2.0812214375  0.6670284271  410           0.1377113342 
0.6621153043  0.9894578313  0.9698795181  0.7705314010  0.5560859189  0.8900602410  0.8132530120  0.9925149701  0.9702380952  0.9924812030  0.9461077844  0.9954819277  0.9939759036  0.5740181269  0.5903614458  1.0000000000  1.0000000000  0.9625187406  0.9221556886  0.7566265060  0.5652173913  20.302114803  2.2543748617  0.6670284271  420           0.1356625557 
0.6801663227  0.9743975904  0.9337349398  0.7850241546  0.5751789976  0.9066265060  0.8554216867  0.9880239521  0.9642857143  0.9954887218  0.9461077844  1.0000000000  1.0000000000  0.6435045317  0.6987951807  1.0000000000  1.0000000000  0.9730134933  0.9281437126  0.7686746988  0.5917874396  20.785498489  2.2503015518  0.6670284271  430           0.1376853704 
0.6788320033  0.9728915663  0.9277108434  0.7729468599  0.6252983294  0.9442771084  0.8855421687  0.9880239521  0.9642857143  0.9954887218  0.9580838323  1.0000000000  1.0000000000  0.7129909366  0.7590361446  0.9984894260  0.9939759036  0.9940029985  0.9760479042  0.7277108434  0.5893719807  21.268882175  2.2737045884  0.6670284271  440           0.1329318762 
0.6847611747  0.9533132530  0.9096385542  0.7632850242  0.6634844869  0.9668674699  0.9036144578  0.9760479042  0.9404761905  0.9909774436  0.9760479042  1.0000000000  1.0000000000  0.7794561934  0.7831325301  0.9984894260  0.9939759036  0.9985007496  0.9940119760  0.7180722892  0.5942028986  21.752265861  2.1670598984  0.6670284271  450           0.1351367712 
0.7113179188  0.9563253012  0.9216867470  0.7801932367  0.6563245823  0.9789156627  0.9036144578  0.9715568862  0.9345238095  0.9894736842  0.9700598802  0.9969879518  1.0000000000  0.7537764350  0.7891566265  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.7759036145  0.6328502415  22.235649546  2.1490249515  0.6670284271  460           0.1345030308 
0.6883756325  0.9412650602  0.9096385542  0.7560386473  0.6634844869  0.9713855422  0.9096385542  0.9520958084  0.9285714286  0.9879699248  0.9760479042  1.0000000000  1.0000000000  0.7854984894  0.8072289157  0.9984894260  0.9939759036  1.0000000000  1.0000000000  0.7325301205  0.6014492754  22.719033232  2.1203576446  0.6670284271  470           0.1353688955 
0.6566115800  0.9984939759  0.9819277108  0.7512077295  0.5871121718  0.8975903614  0.8433734940  0.9880239521  0.9761904762  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.6570996979  0.7168674699  0.9954682779  0.9939759036  0.9880059970  0.9640718563  0.7156626506  0.5724637681  23.202416918  2.0817711592  0.6670284271  480           0.1338702679 
0.6915417090  0.9894578313  0.9578313253  0.7826086957  0.6062052506  0.9548192771  0.8795180723  0.9850299401  0.9523809524  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.6812688822  0.7530120482  1.0000000000  1.0000000000  0.9955022489  0.9820359281  0.7759036145  0.6014492754  23.685800604  2.2236022711  0.6670284271  490           0.1357397556 
0.7052750450  0.9337349398  0.9036144578  0.7608695652  0.6610978520  0.9864457831  0.9216867470  0.9086826347  0.8750000000  0.9804511278  0.9700598802  0.9969879518  1.0000000000  0.8247734139  0.8313253012  0.9848942598  1.0000000000  1.0000000000  1.0000000000  0.7590361446  0.6400966184  24.169184290  2.2515806556  0.6670284271  500           0.1320593119 
0.6649605756  0.9924698795  0.9819277108  0.7560386473  0.6252983294  0.9397590361  0.8795180723  0.9910179641  0.9761904762  0.9984962406  0.9820359281  0.9909638554  0.9879518072  0.7296072508  0.7650602410  0.9969788520  0.9879518072  0.9865067466  0.9700598802  0.6987951807  0.5797101449  24.652567975  2.2162583232  0.6670284271  510           0.1373519659 
0.6608269924  0.9984939759  0.9939759036  0.7415458937  0.5871121718  0.9171686747  0.8734939759  0.9925149701  0.9583333333  0.9969924812  0.9760479042  0.9939759036  0.9879518072  0.6767371601  0.7349397590  0.9984894260  0.9939759036  0.9970014993  0.9760479042  0.7349397590  0.5797101449  25.135951661  2.1233082771  0.6670284271  520           0.1360875845 
0.6836180911  0.9683734940  0.9337349398  0.7560386473  0.6348448687  0.9774096386  0.9096385542  0.9640718563  0.9404761905  0.9909774436  0.9760479042  1.0000000000  1.0000000000  0.7205438066  0.7530120482  0.9969788520  1.0000000000  0.9985007496  0.9820359281  0.7542168675  0.5893719807  25.619335347  2.2171008706  0.6670284271  530           0.1343560696 
0.6859948181  0.9668674699  0.9277108434  0.7632850242  0.6539379475  0.9713855422  0.8975903614  0.9625748503  0.9345238095  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.7839879154  0.8012048193  0.9984894260  0.9939759036  0.9970014993  0.9760479042  0.7228915663  0.6038647343  26.102719033  2.1819959641  0.6670284271  540           0.1361855507 
0.7094285829  0.9487951807  0.9156626506  0.7487922705  0.6849642005  0.9743975904  0.9096385542  0.9491017964  0.9166666667  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.7960725076  0.8132530120  0.9969788520  1.0000000000  1.0000000000  0.9880239521  0.7614457831  0.6425120773  26.586102719  2.0827202082  0.6670284271  550           0.1362761021 
0.7226214533  0.8825301205  0.8554216867  0.7439613527  0.7183770883  0.9954819277  0.9337349398  0.8667664671  0.8511904762  0.9699248120  0.9520958084  0.9819277108  1.0000000000  0.8897280967  0.8975903614  0.9712990937  0.9819277108  0.9970014993  1.0000000000  0.7469879518  0.6811594203  27.069486404  2.1561057329  0.6670284271  560           0.1391054630 
0.7129669625  0.8945783133  0.8674698795  0.7584541063  0.7207637232  0.9879518072  0.9277108434  0.8967065868  0.8750000000  0.9804511278  0.9700598802  0.9879518072  1.0000000000  0.8580060423  0.8795180723  0.9909365559  0.9879518072  0.9970014993  1.0000000000  0.7228915663  0.6497584541  27.552870090  2.1376999497  0.6670284271  570           0.1379744053 
0.6860062505  0.9894578313  0.9578313253  0.7584541063  0.6467780430  0.9397590361  0.8795180723  0.9910179641  0.9880952381  0.9969924812  0.9760479042  0.9984939759  0.9939759036  0.7160120846  0.7710843373  0.9984894260  0.9939759036  0.9910044978  0.9760479042  0.7397590361  0.5990338164  28.036253776  2.1986769676  0.6670284271  580           0.1371952295 
0.6926026162  0.9834337349  0.9457831325  0.7681159420  0.6610978520  0.9653614458  0.8855421687  0.9730538922  0.9523809524  0.9969924812  0.9760479042  0.9984939759  0.9939759036  0.7386706949  0.7771084337  0.9969788520  0.9879518072  0.9985007496  0.9820359281  0.7445783133  0.5966183575  28.519637462  2.1003177047  0.6670284271  590           0.1388585091 
0.6992004370  0.9713855422  0.9337349398  0.7681159420  0.6754176611  0.9713855422  0.9096385542  0.9640718563  0.9404761905  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.7492447130  0.7710843373  0.9984894260  0.9939759036  0.9985007496  0.9940119760  0.7469879518  0.6062801932  29.003021148  2.0779632330  0.6670284271  600           0.1326724052 
0.6926069815  0.9864457831  0.9578313253  0.7608695652  0.6610978520  0.9653614458  0.8855421687  0.9700598802  0.9523809524  0.9969924812  0.9760479042  0.9984939759  0.9939759036  0.7583081571  0.7951807229  1.0000000000  1.0000000000  0.9985007496  0.9820359281  0.7373493976  0.6111111111  29.486404833  2.0286767721  0.6670284271  610           0.1401014090 
0.7100426333  0.9021084337  0.8614457831  0.7560386473  0.6849642005  0.9969879518  0.9397590361  0.8922155689  0.8690476190  0.9759398496  0.9640718563  0.9909638554  1.0000000000  0.8806646526  0.8975903614  0.9924471299  0.9939759036  1.0000000000  1.0000000000  0.7445783133  0.6545893720  29.969788519  2.0562083006  0.6670284271  620           0.1356630087 
0.6992163736  0.9171686747  0.8734939759  0.7705314010  0.6730310263  0.9894578313  0.9216867470  0.9071856287  0.8928571429  0.9759398496  0.9640718563  0.9969879518  1.0000000000  0.7854984894  0.8192771084  0.9984894260  0.9939759036  0.9985007496  0.9940119760  0.7325301205  0.6207729469  30.453172205  2.0816971660  0.6670284271  630           0.1346756935 
0.6944889725  0.9442771084  0.9216867470  0.7584541063  0.6300715990  0.9834337349  0.9216867470  0.9356287425  0.9107142857  0.9879699248  0.9760479042  1.0000000000  1.0000000000  0.7552870091  0.7951807229  1.0000000000  1.0000000000  0.9985007496  0.9820359281  0.7759036145  0.6135265700  30.936555891  2.1118580341  0.6670284271  640           0.1369020462 
0.6920563302  0.9367469880  0.9036144578  0.7415458937  0.6396181384  0.9894578313  0.9216867470  0.9356287425  0.8988095238  0.9879699248  0.9760479042  0.9939759036  1.0000000000  0.8262839879  0.8373493976  0.9969788520  1.0000000000  1.0000000000  1.0000000000  0.7566265060  0.6304347826  31.419939577  2.1842545033  0.6670284271  650           0.1370706081 
0.6877617210  0.9728915663  0.9277108434  0.7415458937  0.6682577566  0.9834337349  0.9216867470  0.9520958084  0.9166666667  0.9909774436  0.9760479042  1.0000000000  1.0000000000  0.8096676737  0.8313253012  0.9969788520  0.9879518072  1.0000000000  1.0000000000  0.7253012048  0.6159420290  31.903323262  2.1390183687  0.6670284271  660           0.1361147404 
0.6602216726  0.9984939759  0.9819277108  0.7270531401  0.5871121718  0.9126506024  0.8433734940  0.9955089820  0.9821428571  1.0000000000  0.9880239521  0.9969879518  0.9879518072  0.6253776435  0.7108433735  0.9969788520  0.9879518072  0.9850074963  0.9640718563  0.7373493976  0.5893719807  32.386706948  2.2338421464  0.6670284271  670           0.1435232401 
0.6746179071  0.9472891566  0.9216867470  0.7246376812  0.6205250597  0.9789156627  0.9156626506  0.9491017964  0.9166666667  0.9879699248  0.9760479042  1.0000000000  1.0000000000  0.7583081571  0.8072289157  1.0000000000  1.0000000000  0.9985007496  0.9820359281  0.7301204819  0.6231884058  32.870090634  2.1475464582  0.6670284271  680           0.1339193583 
0.6661782129  0.9984939759  0.9819277108  0.7391304348  0.6157517900  0.8870481928  0.8132530120  0.9970059880  0.9880952381  1.0000000000  0.9880239521  0.9984939759  0.9939759036  0.6072507553  0.6867469880  0.9939577039  0.9879518072  0.9865067466  0.9461077844  0.7301204819  0.5797101449  33.353474320  2.0798826218  0.6670284271  690           0.1376988649 
0.6559573426  0.9984939759  0.9819277108  0.7342995169  0.6062052506  0.8659638554  0.7891566265  0.9955089820  0.9821428571  0.9984962406  0.9820359281  0.9984939759  0.9939759036  0.5966767372  0.6686746988  0.9954682779  0.9939759036  0.9805097451  0.9700598802  0.7036144578  0.5797101449  33.836858006  2.2852953076  0.6670284271  700           0.1361682415 
0.6667950346  0.9969879518  0.9759036145  0.7270531401  0.6109785203  0.9382530120  0.8855421687  0.9955089820  0.9821428571  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.7280966767  0.7831325301  0.9984894260  0.9939759036  0.9940029985  0.9760479042  0.7325301205  0.5966183575  34.320241691  2.3031630039  0.6670284271  710           0.1372175217 
0.6674219030  0.9909638554  0.9638554217  0.7318840580  0.6014319809  0.9382530120  0.8855421687  0.9880239521  0.9761904762  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.7311178248  0.7831325301  0.9984894260  0.9939759036  0.9970014993  1.0000000000  0.7421686747  0.5942028986  34.803625377  2.1689063311  0.6670284271  720           0.1376557112 
0.6871592419  0.9201807229  0.8855421687  0.7367149758  0.6658711217  0.9954819277  0.9337349398  0.8937125749  0.8630952381  0.9744360902  0.9700598802  0.9969879518  1.0000000000  0.9003021148  0.9036144578  0.9939577039  0.9879518072  1.0000000000  1.0000000000  0.7349397590  0.6111111111  35.287009063  2.0131018162  0.6670284271  730           0.1357188225 
0.6571836410  0.9984939759  0.9819277108  0.7246376812  0.5942720764  0.9216867470  0.8674698795  0.9955089820  0.9821428571  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.6827794562  0.7590361446  0.9984894260  0.9939759036  0.9970014993  0.9880239521  0.7325301205  0.5772946860  35.770392749  2.0662905693  0.6670284271  740           0.1343630314 
0.6817777422  0.9503012048  0.9096385542  0.7294685990  0.6467780430  0.9939759036  0.9277108434  0.9296407186  0.8869047619  0.9849624060  0.9760479042  1.0000000000  1.0000000000  0.8293051360  0.8493975904  0.9954682779  0.9939759036  1.0000000000  1.0000000000  0.7421686747  0.6086956522  36.253776435  2.1405481815  0.6670284271  750           0.1368917227 
0.6859587879  0.9096385542  0.8915662651  0.7270531401  0.6658711217  0.9939759036  0.9277108434  0.9131736527  0.8809523810  0.9684210526  0.9700598802  1.0000000000  1.0000000000  0.8036253776  0.8313253012  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7228915663  0.6280193237  36.737160120  2.1151143193  0.6670284271  760           0.1356683731 
0.6547725474  0.9984939759  0.9819277108  0.7246376812  0.5942720764  0.9292168675  0.8614457831  0.9970059880  0.9880952381  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.6148036254  0.7168674699  0.9984894260  0.9939759036  0.9895052474  0.9820359281  0.7253012048  0.5748792271  37.220543806  2.0343994975  0.6670284271  770           0.1361463547 
0.6650928469  0.9774096386  0.9457831325  0.7560386473  0.5704057279  0.9487951807  0.8674698795  0.9775449102  0.9702380952  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.6223564955  0.7108433735  0.9984894260  0.9939759036  0.9925037481  0.9820359281  0.7542168675  0.5797101449  37.703927492  2.0228741050  0.6670284271  780           0.1341635704 
0.6554872737  0.9969879518  0.9759036145  0.7512077295  0.5536992840  0.9201807229  0.8493975904  0.9970059880  0.9880952381  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.5921450151  0.6746987952  0.9984894260  0.9939759036  0.9850074963  0.9640718563  0.7445783133  0.5724637681  38.187311178  2.1157302618  0.6670284271  790           0.1347281933 
0.6823484870  0.9397590361  0.8915662651  0.7246376812  0.6587112172  0.9969879518  0.9397590361  0.9191616766  0.8809523810  0.9819548872  0.9760479042  1.0000000000  1.0000000000  0.8776435045  0.8734939759  0.9969788520  0.9879518072  1.0000000000  1.0000000000  0.7373493976  0.6086956522  38.670694864  2.0374404073  0.6670284271  800           0.1366375208 
0.6470389878  0.9984939759  0.9819277108  0.7270531401  0.5536992840  0.9156626506  0.8554216867  0.9985029940  0.9940476190  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.6268882175  0.7168674699  0.9984894260  0.9939759036  0.9895052474  0.9700598802  0.7349397590  0.5724637681  39.154078549  2.0746348381  0.6670284271  810           0.1346516609 
0.6698762328  0.9759036145  0.9397590361  0.7439613527  0.5871121718  0.9879518072  0.9036144578  0.9730538922  0.9642857143  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.7734138973  0.8072289157  0.9984894260  0.9939759036  1.0000000000  0.9880239521  0.7493975904  0.5990338164  39.637462235  1.9464060664  0.6670284271  820           0.1338560343 
0.6980703798  0.9036144578  0.8674698795  0.7415458937  0.6443914081  0.9924698795  0.9216867470  0.9011976048  0.8690476190  0.9804511278  0.9700598802  1.0000000000  1.0000000000  0.8459214502  0.8554216867  0.9969788520  1.0000000000  1.0000000000  1.0000000000  0.7734939759  0.6328502415  40.120845921  2.0431224585  0.6670284271  830           0.1353234291 
0.6927621871  0.9728915663  0.9397590361  0.7536231884  0.5942720764  0.9743975904  0.8975903614  0.9640718563  0.9285714286  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.6827794562  0.7590361446  0.9969788520  1.0000000000  1.0000000000  0.9880239521  0.8144578313  0.6086956522  40.604229607  2.1711315274  0.6670284271  840           0.1391898632 
0.6951979479  0.9412650602  0.9216867470  0.7753623188  0.5918854415  0.9894578313  0.9216867470  0.9296407186  0.9107142857  0.9819548872  0.9760479042  1.0000000000  1.0000000000  0.7386706949  0.7771084337  0.9969788520  1.0000000000  1.0000000000  0.9880239521  0.7927710843  0.6207729469  41.087613293  2.1004362345  0.6670284271  850           0.1331604242 
0.6879401387  0.9698795181  0.9397590361  0.7560386473  0.5990453461  0.9894578313  0.9216867470  0.9550898204  0.9285714286  0.9909774436  0.9760479042  1.0000000000  1.0000000000  0.7477341390  0.8012048193  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.7759036145  0.6207729469  41.570996978  2.0586168885  0.6670284271  860           0.1366491795 
0.7004527883  0.8418674699  0.8373493976  0.7173913043  0.6587112172  0.9954819277  0.9337349398  0.8188622754  0.8273809524  0.9142857143  0.8862275449  0.9774096386  0.9819277108  0.9410876133  0.9096385542  0.9592145015  0.9457831325  1.0000000000  0.9880239521  0.7566265060  0.6690821256  42.054380664  2.0755327463  0.6670284271  870           0.1354138851 
0.6849093132  0.9683734940  0.9337349398  0.7512077295  0.6038186158  0.9864457831  0.9096385542  0.9311377246  0.9047619048  0.9849624060  0.9760479042  1.0000000000  1.0000000000  0.6691842900  0.7409638554  0.9954682779  0.9939759036  1.0000000000  0.9880239521  0.7710843373  0.6135265700  42.537764350  1.9878057361  0.6670284271  880           0.1358593941 
0.6974492623  0.8719879518  0.8373493976  0.7125603865  0.6515513126  0.9954819277  0.9457831325  0.8488023952  0.8392857143  0.9353383459  0.9341317365  0.9894578313  0.9939759036  0.9305135952  0.8915662651  0.9758308157  0.9759036145  1.0000000000  0.9880239521  0.7662650602  0.6594202899  43.021148036  2.0731441021  0.6670284271  890           0.1387113333 
0.6710738461  0.9743975904  0.9337349398  0.7318840580  0.5894988067  0.9819277108  0.9277108434  0.9640718563  0.9523809524  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.7613293051  0.8072289157  0.9984894260  0.9939759036  1.0000000000  0.9880239521  0.7542168675  0.6086956522  43.504531722  2.0908873677  0.6670284271  900           0.1345344067 
0.6607435692  0.8704819277  0.8433734940  0.7077294686  0.6205250597  0.9969879518  0.9397590361  0.8682634731  0.8571428571  0.9458646617  0.9401197605  0.9939759036  1.0000000000  0.9108761329  0.8975903614  0.9924471299  0.9939759036  1.0000000000  1.0000000000  0.7060240964  0.6086956522  43.987915407  1.9218606353  0.6670284271  910           0.1315706730 
0.6470117577  0.9713855422  0.9337349398  0.7222222222  0.5680190931  0.9834337349  0.9216867470  0.9550898204  0.9404761905  0.9849624060  0.9760479042  1.0000000000  1.0000000000  0.7265861027  0.7650602410  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.7084337349  0.5893719807  44.471299093  1.9197058916  0.6670284271  920           0.1348797798 
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.001
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3123273836  0.4442771084  0.4879518072  0.3067632850  0.2935560859  0.4728915663  0.4578313253  0.4431137725  0.4404761905  0.5007518797  0.4970059880  0.4879518072  0.4819277108  0.4637462236  0.4879518072  0.4818731118  0.4759036145  0.4752623688  0.4970059880  0.3180722892  0.3309178744  0.0000000000  9.8018226624  0.2909131050  0             0.4737377167 
0.6127689861  0.7545180723  0.7409638554  0.6376811594  0.5178997613  0.7891566265  0.7228915663  0.8473053892  0.8333333333  0.6947368421  0.6706586826  0.8162650602  0.7710843373  0.5876132931  0.6204819277  0.8036253776  0.7951807229  0.7196401799  0.6946107784  0.6650602410  0.6304347826  0.4833836858  8.4232956886  0.4781041145  10            0.1502562046 
0.7028209236  0.8403614458  0.8433734940  0.7004830918  0.6825775656  0.8253012048  0.7831325301  0.8862275449  0.8690476190  0.8285714286  0.8203592814  0.7831325301  0.7951807229  0.7824773414  0.8072289157  0.7824773414  0.7710843373  0.8560719640  0.8802395210  0.7156626506  0.7125603865  0.9667673716  6.3961913109  0.4781041145  20            0.1406170845 
0.7396243130  0.8524096386  0.8554216867  0.7584541063  0.6706443914  0.8072289157  0.7831325301  0.8952095808  0.8809523810  0.8556390977  0.8682634731  0.9216867470  0.9397590361  0.7824773414  0.8192771084  0.9154078550  0.8795180723  0.9025487256  0.9341317365  0.8289156627  0.7004830918  1.4501510574  5.2713678837  0.4781041145  30            0.1339568138 
0.7906850153  0.8539156627  0.8614457831  0.7584541063  0.7684964200  0.8870481928  0.8493975904  0.8967065868  0.8750000000  0.9022556391  0.9101796407  0.9216867470  0.9156626506  0.8096676737  0.8554216867  0.9154078550  0.8795180723  0.9625187406  0.9580838323  0.7831325301  0.8526570048  1.9335347432  4.3745160103  0.4781041145  40            0.1380671024 
0.7956396823  0.8493975904  0.8433734940  0.7801932367  0.7207637232  0.8734939759  0.8313253012  0.8922155689  0.8690476190  0.8917293233  0.9161676647  0.9728915663  0.9759036145  0.8308157100  0.8674698795  0.9516616314  0.9397590361  0.9745127436  0.9580838323  0.8168674699  0.8647342995  2.4169184290  3.8439564943  0.4781041145  50            0.1344951868 
0.7720690031  0.9066265060  0.8795180723  0.8091787440  0.7350835322  0.8358433735  0.7891566265  0.9610778443  0.9523809524  0.9398496241  0.8802395210  0.9849397590  0.9879518072  0.7507552870  0.7650602410  0.9909365559  0.9879518072  0.9475262369  0.9221556886  0.7783132530  0.7657004831  2.9003021148  3.5697783709  0.4781041145  60            0.1381350279 
0.7796888602  0.8614457831  0.8433734940  0.7657004831  0.8186157518  0.9096385542  0.8674698795  0.9266467066  0.8869047619  0.9458646617  0.9281437126  0.9894578313  0.9698795181  0.8157099698  0.8433734940  0.9697885196  0.9638554217  0.9820089955  0.9640718563  0.7421686747  0.7922705314  3.3836858006  3.5221521616  0.4781041145  70            0.1311470032 
0.7859461123  0.9472891566  0.9096385542  0.8043478261  0.7303102625  0.9081325301  0.8373493976  0.9700598802  0.9761904762  0.9669172932  0.9281437126  0.9834337349  0.9939759036  0.7975830816  0.8192771084  0.9788519637  0.9879518072  0.9655172414  0.9341317365  0.8216867470  0.7874396135  3.8670694864  3.2433509588  0.4781589508  80            0.1381727219 
0.7358818797  0.9638554217  0.9156626506  0.7971014493  0.7231503580  0.8313253012  0.7951807229  0.9880239521  0.9761904762  0.9609022556  0.9041916168  0.9804216867  0.9939759036  0.7175226586  0.7289156627  0.9637462236  0.9759036145  0.9310344828  0.9041916168  0.7638554217  0.6594202899  4.3504531722  3.0918439388  0.4781641960  90            0.1339201212 
0.7613337894  0.8373493976  0.8072289157  0.7536231884  0.7016706444  0.9713855422  0.8734939759  0.8398203593  0.8273809524  0.8766917293  0.8802395210  0.8900602410  0.8975903614  0.9154078550  0.8795180723  0.8912386707  0.8915662651  0.9430284858  0.9281437126  0.7228915663  0.8671497585  4.8338368580  3.4182702303  0.4781641960  100           0.1531508446 
0.7881084610  0.8463855422  0.8192771084  0.7391304348  0.8329355609  0.9472891566  0.8734939759  0.8727544910  0.8630952381  0.9248120301  0.9401197605  0.9653614458  0.9578313253  0.8716012085  0.8975903614  0.9305135952  0.9277108434  0.9925037481  0.9940119760  0.7277108434  0.8526570048  5.3172205438  3.3018404722  0.4781641960  110           0.1414484739 
0.8019670012  0.8569277108  0.8132530120  0.7560386473  0.8400954654  0.9578313253  0.8674698795  0.8817365269  0.8630952381  0.9533834586  0.9580838323  0.9593373494  0.9578313253  0.8927492447  0.8975903614  0.9471299094  0.9457831325  0.9925037481  0.9940119760  0.7421686747  0.8695652174  5.8006042296  3.0277636290  0.4781641960  120           0.1321485281 
0.7936911737  0.8990963855  0.8614457831  0.8115942029  0.7661097852  0.9201807229  0.8734939759  0.9116766467  0.8988095238  0.9729323308  0.9640718563  0.9969879518  1.0000000000  0.7613293051  0.7831325301  0.9637462236  0.9518072289  0.9820089955  0.9520958084  0.8168674699  0.7801932367  6.2839879154  2.8551118374  0.4781641960  130           0.1306635618 
0.7966818122  0.9156626506  0.9036144578  0.8188405797  0.7804295943  0.9503012048  0.8855421687  0.9416167665  0.9107142857  0.9789473684  0.9640718563  0.9909638554  1.0000000000  0.8368580060  0.8433734940  0.9879154079  0.9759036145  0.9925037481  0.9820359281  0.7927710843  0.7946859903  6.7673716012  2.6211789608  0.4781641960  140           0.1349509716 
0.7834068348  0.9307228916  0.9156626506  0.8309178744  0.7756563246  0.9382530120  0.8855421687  0.9535928144  0.9226190476  0.9834586466  0.9700598802  0.9909638554  1.0000000000  0.8489425982  0.8554216867  0.9909365559  0.9879518072  0.9925037481  0.9820359281  0.8000000000  0.7270531401  7.2507552870  2.5929231405  0.4781641960  150           0.1303122520 
0.7635743625  0.9307228916  0.9277108434  0.8333333333  0.7422434368  0.9292168675  0.8734939759  0.9505988024  0.9464285714  0.9909774436  0.9640718563  0.9969879518  1.0000000000  0.7854984894  0.8192771084  0.9954682779  0.9939759036  0.9850074963  0.9520958084  0.8096385542  0.6690821256  7.7341389728  2.4272241831  0.4781641960  160           0.1336387157 
0.7593834089  0.9201807229  0.9096385542  0.8381642512  0.7326968974  0.9412650602  0.8855421687  0.9446107784  0.9107142857  0.9849624060  0.9760479042  0.9969879518  1.0000000000  0.8081570997  0.8132530120  0.9939577039  0.9879518072  0.9895052474  0.9580838323  0.7975903614  0.6690821256  8.2175226586  2.5005839109  0.4781641960  170           0.1347926378 
0.7565424334  0.9518072289  0.9397590361  0.8647342995  0.6610978520  0.9036144578  0.8313253012  0.9550898204  0.9404761905  0.9939849624  0.9640718563  0.9969879518  1.0000000000  0.6948640483  0.7349397590  0.9954682779  0.9939759036  0.9745127436  0.9461077844  0.8602409639  0.6400966184  8.7009063444  2.4055127859  0.4782748222  180           0.1328178644 
0.7739248023  0.9487951807  0.9156626506  0.8550724638  0.7040572792  0.9759036145  0.8915662651  0.9595808383  0.9345238095  0.9879699248  0.9760479042  0.9969879518  1.0000000000  0.8247734139  0.7951807229  0.9924471299  0.9939759036  0.9925037481  0.9580838323  0.8602409639  0.6763285024  9.1842900302  2.3436462641  0.4782795906  190           0.1333659410 
0.7654966794  0.9563253012  0.9216867470  0.8599033816  0.6968973747  0.9382530120  0.8734939759  0.9655688623  0.9464285714  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.7447129909  0.7650602410  0.9969788520  1.0000000000  0.9820089955  0.9520958084  0.8530120482  0.6521739130  9.6676737160  2.3200842619  0.4782795906  200           0.1370256424 
0.7296719513  0.9427710843  0.8915662651  0.7729468599  0.7875894988  0.9186746988  0.8674698795  0.9760479042  0.9761904762  0.9954887218  0.9461077844  0.9969879518  1.0000000000  0.8051359517  0.8253012048  0.9939577039  0.9879518072  0.9880059970  0.9880239521  0.7253012048  0.6328502415  10.151057401  2.2695744276  0.4782795906  210           0.1323666096 
0.7431800149  0.9759036145  0.9277108434  0.8381642512  0.7016706444  0.9412650602  0.8855421687  0.9850299401  0.9761904762  0.9954887218  0.9580838323  0.9954819277  0.9939759036  0.7673716012  0.7590361446  0.9954682779  0.9939759036  0.9865067466  0.9580838323  0.7855421687  0.6473429952  10.634441087  2.3373219728  0.4782795906  220           0.1391947269 
0.7414144281  0.9533132530  0.9216867470  0.8333333333  0.6825775656  0.9503012048  0.8855421687  0.9655688623  0.9345238095  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.7703927492  0.7831325301  0.9954682779  0.9939759036  0.9955022489  0.9820359281  0.8048192771  0.6449275362  11.117824773  2.2599499822  0.4782795906  230           0.1413536549 
0.7667044089  0.9608433735  0.9156626506  0.8502415459  0.6968973747  0.9819277108  0.8915662651  0.9640718563  0.9404761905  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.8368580060  0.8192771084  0.9969788520  1.0000000000  0.9970014993  0.9760479042  0.8530120482  0.6666666667  11.601208459  2.2651321888  0.6689839363  240           0.1346000433 
0.7330192862  0.9472891566  0.9216867470  0.8429951691  0.6587112172  0.9503012048  0.8855421687  0.9565868263  0.9226190476  0.9879699248  0.9760479042  1.0000000000  1.0000000000  0.7280966767  0.7349397590  0.9969788520  1.0000000000  1.0000000000  0.9880239521  0.8265060241  0.6038647343  12.084592145  2.2142609835  0.6689839363  250           0.1328327417 
0.7335225444  0.9472891566  0.9216867470  0.8164251208  0.7016706444  0.9593373494  0.8975903614  0.9580838323  0.9166666667  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.8081570997  0.8012048193  0.9969788520  1.0000000000  0.9985007496  0.9940119760  0.7783132530  0.6376811594  12.567975830  2.1517590046  0.6689839363  260           0.1466356039 
0.7389126358  0.9292168675  0.8975903614  0.8091787440  0.7159904535  0.9819277108  0.9036144578  0.9206586826  0.8869047619  0.9849624060  0.9760479042  1.0000000000  1.0000000000  0.8746223565  0.8614457831  0.9969788520  1.0000000000  1.0000000000  1.0000000000  0.7807228916  0.6497584541  13.051359516  2.0505763769  0.6689839363  270           0.1405022621 
0.7292911260  0.8825301205  0.8433734940  0.7826086957  0.7016706444  0.9939759036  0.9156626506  0.8787425150  0.8750000000  0.9669172932  0.9640718563  0.9924698795  0.9819277108  0.9123867069  0.8674698795  0.9803625378  0.9819277108  1.0000000000  0.9880239521  0.7855421687  0.6473429952  13.534743202  2.1066875696  0.6689839363  280           0.1359277010 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3123273836  0.4442771084  0.4879518072  0.3067632850  0.2935560859  0.4728915663  0.4578313253  0.4431137725  0.4404761905  0.5007518797  0.4970059880  0.4879518072  0.4819277108  0.4637462236  0.4879518072  0.4818731118  0.4759036145  0.4752623688  0.4970059880  0.3180722892  0.3309178744  0.0000000000  9.8018226624  0.2909131050  0             0.7180473804 
0.6000965572  0.7500000000  0.7349397590  0.6231884058  0.5178997613  0.7981927711  0.7469879518  0.8428143713  0.8273809524  0.7022556391  0.6766467066  0.8298192771  0.7891566265  0.5951661631  0.6265060241  0.8111782477  0.7771084337  0.7151424288  0.6886227545  0.6506024096  0.6086956522  0.4833836858  8.4108963013  0.4781041145  10            0.1383363962 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/tokenize.py", line 381, in read_or_stop


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2173, in <module>
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2166, in main
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1984, in handle_keyboard_interrupt
    traceback.print_exception(type(value), value, tb, limit=limit)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/traceback.py", line 104, in print_exception
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    type(value), value, tb, limit=limit).format(chain=chain):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/traceback.py", line 509, in __init__
    capture_locals=capture_locals)
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/traceback.py", line 364, in extract
    f.line  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/traceback.py", line 286, in line
    self._line = linecache.getline(self.filename, self.lineno).strip()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/linecache.py", line 16, in getline
    lines = getlines(filename, module_globals)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/linecache.py", line 47, in getlines
    return updatecache(filename, module_globals)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/linecache.py", line 136, in updatecache
      File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/tokenize.py", line 454, in open
    encoding, lines = detect_encoding(buffer.readline)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/tokenize.py", line 423, in detect_encoding
    first = read_or_stop()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/tokenize.py", line 381, in read_or_stop
    return readline()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1172276) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3123273836  0.4442771084  0.4879518072  0.3067632850  0.2935560859  0.4728915663  0.4578313253  0.4431137725  0.4404761905  0.5007518797  0.4970059880  0.4879518072  0.4819277108  0.4637462236  0.4879518072  0.4818731118  0.4759036145  0.4752623688  0.4970059880  0.3180722892  0.3309178744  0.0000000000  9.8018226624  0.2909131050  0             0.5331239700 
0.6000965572  0.7500000000  0.7349397590  0.6231884058  0.5178997613  0.7981927711  0.7469879518  0.8428143713  0.8273809524  0.7022556391  0.6766467066  0.8298192771  0.7891566265  0.5951661631  0.6265060241  0.8111782477  0.7771084337  0.7151424288  0.6886227545  0.6506024096  0.6086956522  0.4833836858  8.4108963013  0.4781041145  10            0.1413592339 
0.7016404242  0.8539156627  0.8614457831  0.6908212560  0.6682577566  0.8418674699  0.8132530120  0.8832335329  0.8690476190  0.8240601504  0.8383233533  0.7921686747  0.7951807229  0.8006042296  0.8192771084  0.7870090634  0.7771084337  0.8920539730  0.9041916168  0.7421686747  0.7053140097  0.9667673716  6.3454818249  0.4781041145  20            0.1427939892 
0.7553059493  0.8674698795  0.8795180723  0.7922705314  0.6730310263  0.7846385542  0.7650602410  0.9041916168  0.8809523810  0.8736842105  0.8562874251  0.9608433735  0.9397590361  0.7688821752  0.8012048193  0.9425981873  0.9397590361  0.9100449775  0.9161676647  0.8481927711  0.7077294686  1.4501510574  5.1840727329  0.4781041145  30            0.1343556404 
0.8020935216  0.8614457831  0.8554216867  0.7657004831  0.7875894988  0.8810240964  0.8373493976  0.8982035928  0.8809523810  0.8947368421  0.9041916168  0.9623493976  0.9578313253  0.8141993958  0.8493975904  0.9456193353  0.9036144578  0.9730134933  0.9640718563  0.7951807229  0.8599033816  1.9335347432  4.3187926292  0.4781041145  40            0.1439451694 
0.8050812499  0.8509036145  0.8493975904  0.7705314010  0.8019093079  0.8810240964  0.8373493976  0.9011976048  0.8809523810  0.9052631579  0.9221556886  0.9683734940  0.9578313253  0.8429003021  0.8915662651  0.9622356495  0.9578313253  0.9745127436  0.9820359281  0.7759036145  0.8719806763  2.4169184290  3.8804344416  0.4781923294  50            0.1374782562 
0.7785186160  0.8599397590  0.8734939759  0.7777777778  0.8066825776  0.8780120482  0.8614457831  0.9311377246  0.9047619048  0.9323308271  0.9221556886  0.9939759036  1.0000000000  0.7870090634  0.8012048193  0.9803625378  0.9819277108  0.9775112444  0.9580838323  0.7397590361  0.7898550725  2.9003021148  3.7255551338  0.4781923294  60            0.1404941320 
0.7723431237  0.9021084337  0.8734939759  0.7729468599  0.8544152745  0.8855421687  0.8674698795  0.9625748503  0.9702380952  0.9729323308  0.9401197605  0.9834337349  0.9819277108  0.7900302115  0.8132530120  0.9728096677  0.9879518072  0.9700149925  0.9640718563  0.7277108434  0.7342995169  3.3836858006  3.5881847382  0.4781923294  70            0.1330386162 
0.7670477489  0.9472891566  0.9216867470  0.8043478261  0.7947494033  0.8960843373  0.8614457831  0.9865269461  0.9821428571  0.9849624060  0.9401197605  0.9834337349  0.9939759036  0.7854984894  0.8072289157  0.9773413897  0.9939759036  0.9700149925  0.9401197605  0.7951807229  0.6739130435  3.8670694864  3.2876569986  0.4781923294  80            0.1368288755 
0.7407096790  0.9728915663  0.9397590361  0.8164251208  0.7159904535  0.8192771084  0.7590361446  0.9895209581  0.9821428571  0.9714285714  0.9101796407  0.9743975904  0.9939759036  0.6706948640  0.6867469880  0.9531722054  0.9698795181  0.9175412294  0.8862275449  0.8048192771  0.6256038647  4.3504531722  3.1184529305  0.4781923294  90            0.1359803438 
0.8099750807  0.8825301205  0.8795180723  0.8429951691  0.7637231504  0.9051204819  0.8253012048  0.9341317365  0.9047619048  0.9563909774  0.9580838323  0.9939759036  0.9879518072  0.7311178248  0.7469879518  0.9728096677  0.9638554217  0.9775112444  0.9580838323  0.8626506024  0.7705314010  4.8338368580  3.3823405266  0.4782199860  100           0.1346698523 
0.7883526323  0.9231927711  0.9096385542  0.8574879227  0.7231503580  0.9006024096  0.8433734940  0.9550898204  0.9523809524  0.9834586466  0.9700598802  0.9969879518  1.0000000000  0.7129909366  0.7710843373  0.9924471299  0.9939759036  0.9805097451  0.9461077844  0.8722891566  0.7004830918  5.3172205438  3.2998969555  0.4782357216  110           0.1350543022 
0.7907033065  0.9623493976  0.9216867470  0.8502415459  0.7470167064  0.8945783133  0.8192771084  0.9940119760  0.9880952381  0.9954887218  0.9580838323  0.9864457831  0.9939759036  0.7492447130  0.7831325301  0.9894259819  0.9819277108  0.9715142429  0.9461077844  0.8602409639  0.7053140097  5.8006042296  3.0476355791  0.4782357216  120           0.1294619560 
0.7661234784  0.9593373494  0.9457831325  0.8599033816  0.6849642005  0.8358433735  0.7771084337  0.9805389222  0.9821428571  0.9924812030  0.9700598802  0.9954819277  0.9939759036  0.5876132931  0.6204819277  0.9879154079  0.9879518072  0.9595202399  0.9101796407  0.8746987952  0.6449275362  6.2839879154  2.7719374180  0.4784564972  130           0.1333935976 
0.7858004673  0.9503012048  0.9337349398  0.8478260870  0.7756563246  0.9246987952  0.8674698795  0.9850299401  0.9880952381  0.9939849624  0.9640718563  0.9954819277  0.9939759036  0.7280966767  0.7710843373  0.9894259819  0.9939759036  0.9835082459  0.9580838323  0.8361445783  0.6835748792  6.7673716012  2.7121842384  0.4784564972  140           0.1394463062 
0.7555844352  0.9442771084  0.9337349398  0.8043478261  0.7923627685  0.9231927711  0.8855421687  0.9775449102  0.9821428571  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.7673716012  0.8192771084  0.9954682779  0.9939759036  0.9895052474  0.9820359281  0.7903614458  0.6352657005  7.2507552870  2.6442968369  0.4784564972  150           0.1374146223 
0.7549575668  0.9277108434  0.9156626506  0.7995169082  0.8019093079  0.9246987952  0.8795180723  0.9595808383  0.9702380952  0.9894736842  0.9700598802  0.9969879518  1.0000000000  0.7854984894  0.7951807229  0.9939577039  0.9879518072  0.9895052474  0.9820359281  0.7807228916  0.6376811594  7.7341389728  2.5601799965  0.4784564972  160           0.1358213186 
0.7602771919  0.9216867470  0.9156626506  0.7898550725  0.8448687351  0.9427710843  0.8915662651  0.9431137725  0.9166666667  0.9864661654  0.9700598802  0.9969879518  1.0000000000  0.8187311178  0.8192771084  0.9909365559  0.9879518072  0.9940029985  1.0000000000  0.7566265060  0.6497584541  8.2175226586  2.6377226353  0.4784564972  170           0.1350477219 
0.7599784042  0.9231927711  0.9096385542  0.8429951691  0.7279236277  0.9277108434  0.8795180723  0.9491017964  0.9166666667  0.9894736842  0.9700598802  0.9969879518  1.0000000000  0.7280966767  0.7469879518  0.9924471299  0.9939759036  0.9895052474  0.9580838323  0.8361445783  0.6328502415  8.7009063444  2.5473889589  0.4784564972  180           0.1370751858 
0.7839658694  0.9518072289  0.9277108434  0.8381642512  0.7852028640  0.9774096386  0.8975903614  0.9580838323  0.9404761905  0.9939849624  0.9760479042  0.9939759036  0.9879518072  0.8353474320  0.8373493976  0.9924471299  0.9939759036  0.9925037481  0.9820359281  0.8265060241  0.6859903382  9.1842900302  2.3920474529  0.4784693718  190           0.1363765240 
0.7574176475  0.9487951807  0.9156626506  0.8140096618  0.7852028640  0.9307228916  0.8795180723  0.9655688623  0.9583333333  0.9909774436  0.9760479042  0.9969879518  1.0000000000  0.7341389728  0.7590361446  0.9924471299  0.9939759036  0.9910044978  0.9760479042  0.7903614458  0.6400966184  9.6676737160  2.3527035475  0.4784693718  200           0.1370413065 
0.7543408145  0.9563253012  0.9216867470  0.7995169082  0.8090692124  0.9548192771  0.8915662651  0.9835329341  0.9702380952  0.9969924812  0.9640718563  0.9969879518  1.0000000000  0.8157099698  0.8313253012  0.9954682779  0.9939759036  0.9940029985  1.0000000000  0.7662650602  0.6425120773  10.151057401  2.3779385328  0.4784693718  210           0.1385448933 
0.7484345774  0.9653614458  0.9216867470  0.8164251208  0.7589498807  0.9337349398  0.8795180723  0.9895209581  0.9821428571  0.9969924812  0.9760479042  0.9954819277  0.9939759036  0.7054380665  0.7409638554  0.9954682779  0.9939759036  0.9895052474  0.9700598802  0.7975903614  0.6207729469  10.634441087  2.3198907852  0.4784693718  220           0.1341675520 
0.7321966778  0.9623493976  0.9216867470  0.8091787440  0.7422434368  0.9412650602  0.8855421687  0.9895209581  0.9821428571  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.7371601208  0.7590361446  0.9984894260  0.9939759036  0.9955022489  0.9940119760  0.7710843373  0.6062801932  11.117824773  2.3437052250  0.4784693718  230           0.1382469893 
0.7484159390  0.9518072289  0.9036144578  0.8043478261  0.7684964200  0.9849397590  0.9036144578  0.9535928144  0.9345238095  0.9849624060  0.9640718563  0.9939759036  0.9879518072  0.8595166163  0.8493975904  0.9954682779  0.9939759036  1.0000000000  1.0000000000  0.7807228916  0.6400966184  11.601208459  2.3335587502  0.4784693718  240           0.1348232031 
0.7208183813  0.9503012048  0.9216867470  0.8019323671  0.7112171838  0.9397590361  0.8795180723  0.9700598802  0.9642857143  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.7356495468  0.7530120482  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7686746988  0.6014492754  12.084592145  2.3376903534  0.4784693718  250           0.1376019478 
0.7375421474  0.9548192771  0.9156626506  0.7898550725  0.7732696897  0.9608433735  0.9036144578  0.9850299401  0.9642857143  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.7990936556  0.8012048193  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7638554217  0.6231884058  12.567975830  2.2255805969  0.4784693718  260           0.1361487865 
0.7357837667  0.9503012048  0.9216867470  0.7850241546  0.7517899761  0.9638554217  0.9036144578  0.9520958084  0.9047619048  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.8051359517  0.8132530120  0.9969788520  1.0000000000  1.0000000000  1.0000000000  0.7831325301  0.6231884058  13.051359516  2.2384432793  0.4784693718  270           0.1393575430 
0.7298618013  0.9186746988  0.9156626506  0.7971014493  0.7112171838  0.9864457831  0.9096385542  0.9206586826  0.8869047619  0.9849624060  0.9760479042  0.9969879518  0.9879518072  0.8429003021  0.8072289157  0.9924471299  0.9939759036  1.0000000000  0.9760479042  0.7927710843  0.6183574879  13.534743202  2.1556657553  0.4784693718  280           0.1367537498 
0.7190111524  0.9683734940  0.9216867470  0.7898550725  0.7112171838  0.9668674699  0.9036144578  0.9670658683  0.9404761905  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.7673716012  0.7710843373  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7614457831  0.6135265700  14.018126888  2.1666695833  0.4784693718  290           0.1394703150 
0.7261797180  0.9713855422  0.9096385542  0.7801932367  0.7374701671  0.9849397590  0.9036144578  0.9715568862  0.9464285714  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.8157099698  0.8192771084  0.9969788520  1.0000000000  1.0000000000  1.0000000000  0.7590361446  0.6280193237  14.501510574  2.2134197235  0.4784693718  300           0.1351521492 
0.6915459354  0.9954819277  0.9698795181  0.7922705314  0.6014319809  0.9096385542  0.8433734940  0.9970059880  0.9880952381  0.9969924812  0.9640718563  0.9984939759  0.9939759036  0.6299093656  0.6927710843  1.0000000000  1.0000000000  0.9895052474  0.9580838323  0.7927710843  0.5797101449  14.984894259  2.1759342909  0.4784693718  310           0.1324966669 
0.7394474199  0.8840361446  0.8493975904  0.7777777778  0.7422434368  0.9969879518  0.9277108434  0.8937125749  0.8750000000  0.9744360902  0.9700598802  0.9984939759  0.9939759036  0.9003021148  0.8915662651  0.9864048338  0.9939759036  1.0000000000  1.0000000000  0.7638554217  0.6739130435  15.468277945  2.0851653814  0.4784693718  320           0.1371919155 
0.7167353097  0.9774096386  0.9457831325  0.8067632850  0.6587112172  0.9683734940  0.8975903614  0.9835329341  0.9702380952  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.6570996979  0.7168674699  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7927710843  0.6086956522  15.951661631  2.1707051158  0.4784693718  330           0.1347597122 
0.7250568668  0.9277108434  0.9156626506  0.7874396135  0.7040572792  0.9909638554  0.9156626506  0.9191616766  0.8809523810  0.9819548872  0.9760479042  1.0000000000  1.0000000000  0.8232628399  0.8132530120  0.9924471299  0.9939759036  1.0000000000  1.0000000000  0.7855421687  0.6231884058  16.435045317  2.0954875946  0.4784693718  340           0.1359788179 
0.7201483461  0.9548192771  0.9277108434  0.7729468599  0.7374701671  0.9804216867  0.9096385542  0.9520958084  0.9047619048  0.9909774436  0.9760479042  1.0000000000  1.0000000000  0.7885196375  0.8313253012  0.9969788520  1.0000000000  1.0000000000  1.0000000000  0.7469879518  0.6231884058  16.918429003  2.1717819929  0.4784693718  350           0.1368912935 
0.7214697780  0.9728915663  0.9277108434  0.7971014493  0.6945107399  0.9939759036  0.9156626506  0.9820359281  0.9642857143  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.8262839879  0.8373493976  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7734939759  0.6207729469  17.401812688  2.0313097835  0.4784693718  360           0.1340757847 
0.7196897791  0.9246987952  0.9277108434  0.7874396135  0.6801909308  0.9939759036  0.9277108434  0.9266467066  0.8988095238  0.9879699248  0.9760479042  1.0000000000  1.0000000000  0.8232628399  0.8373493976  0.9939577039  1.0000000000  1.0000000000  1.0000000000  0.7927710843  0.6183574879  17.885196374  2.0275402308  0.4784693718  370           0.1366146564 
0.6932092522  0.9984939759  0.9819277108  0.7681159420  0.6563245823  0.9382530120  0.8734939759  0.9985029940  0.9940476190  0.9984962406  0.9700598802  1.0000000000  1.0000000000  0.6631419940  0.7289156627  1.0000000000  1.0000000000  0.9985007496  0.9940119760  0.7638554217  0.5845410628  18.368580060  1.9075352669  0.4784693718  380           0.1398309469 
0.7064135550  0.9969879518  0.9759036145  0.7801932367  0.6825775656  0.9759036145  0.8915662651  0.9970059880  0.9761904762  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.6903323263  0.7409638554  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7662650602  0.5966183575  18.851963746  2.0014049053  0.4784693718  390           0.1353597879 
0.7135721434  0.9337349398  0.8795180723  0.7729468599  0.7159904535  0.9969879518  0.9397590361  0.9311377246  0.9047619048  0.9834586466  0.9700598802  0.9984939759  0.9939759036  0.8776435045  0.8493975904  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7445783133  0.6207729469  19.335347432  2.0114144683  0.4784693718  400           0.1318321943 
0.6853520130  0.9939759036  0.9638554217  0.7632850242  0.6658711217  0.9683734940  0.8975903614  0.9985029940  0.9821428571  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.7190332326  0.7590361446  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7277108434  0.5845410628  19.818731117  2.0027095199  0.4789323807  410           0.1344738960 
0.6956579005  0.9487951807  0.9397590361  0.7753623188  0.6467780430  0.9623493976  0.8975903614  0.9775449102  0.9464285714  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.7054380665  0.7409638554  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7566265060  0.6038647343  20.302114803  1.9667269111  0.4789323807  420           0.1359097481 
0.7077364419  1.0000000000  0.9879518072  0.7922705314  0.6396181384  0.9653614458  0.8855421687  0.9985029940  0.9940476190  0.9984962406  0.9700598802  1.0000000000  1.0000000000  0.6797583082  0.7228915663  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.7903614458  0.6086956522  20.785498489  1.9596961021  0.4814496040  430           0.1369375706 
0.6740152891  1.0000000000  0.9879518072  0.7512077295  0.6133651551  0.9126506024  0.8313253012  0.9985029940  0.9940476190  0.9984962406  0.9580838323  1.0000000000  1.0000000000  0.6057401813  0.6566265060  1.0000000000  1.0000000000  0.9910044978  0.9520958084  0.7638554217  0.5676328502  21.268882175  1.8602461100  0.4814496040  440           0.1461112976 
0.6984757334  0.9984939759  0.9819277108  0.7608695652  0.7231503580  0.9804216867  0.9096385542  1.0000000000  1.0000000000  0.9984962406  0.9700598802  1.0000000000  1.0000000000  0.7613293051  0.8072289157  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7084337349  0.6014492754  21.752265861  1.9554524183  0.4814496040  450           0.1633092403 
Process Process-7504:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 251, in _bootstrap
    util._run_after_forkers()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 128, in _run_after_forkers
    items = list(_afterfork_registry.items())
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/weakref.py", line 215, in items
    yield k, v
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2164, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1476, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1483, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 267, in in_project_roots
    return filename_to_in_scope_cache[filename]
KeyError: '/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "_pydevd_frame_eval/pydevd_frame_evaluator_common.pyx", line 157, in _pydevd_frame_eval.pydevd_frame_evaluator_common.get_func_code_info
KeyError: '/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydev_bundle/pydev_log.py'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 557, in get_abs_path_real_path_and_base_from_file
    return NORM_PATHS_AND_BASE_CONTAINER[f]
KeyError: '/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydev_bundle/pydev_log.py'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 230, in _NormPaths
    return NORM_PATHS_CONTAINER[filename]
KeyError: '/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydev_bundle/pydev_log.py'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1324138) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "_pydevd_frame_eval/pydevd_frame_evaluator_common.pyx", line 159, in _pydevd_frame_eval.pydevd_frame_evaluator_common.get_func_code_info
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 571, in get_abs_path_real_path_and_base_from_file
    abs_path, real_path = _NormPaths(f)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 238, in _NormPaths
    real_path = _NormPath(filename, rPath)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 246, in _NormPath
    r = normpath(filename)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/posixpath.py", line 395, in realpath
    path, ok = _joinrealpath(filename[:0], filename, {})
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/posixpath.py", line 429, in _joinrealpath
    if not islink(newpath):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/posixpath.py", line 171, in islink
    st = os.lstat(path)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 63, in handler
    def handler(signum, frame):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
SystemError: <built-in function _error_if_any_worker_fails> returned a result with an error set
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3724929638  0.4201807229  0.4638554217  0.4154589372  0.3532219570  0.4743975904  0.4518072289  0.4431137725  0.4285714286  0.4195488722  0.4610778443  0.4412650602  0.4397590361  0.3564954683  0.3855421687  0.4244712991  0.4277108434  0.3778110945  0.3353293413  0.3855421687  0.3357487923  0.0000000000  9.8018226624  0.2909131050  0             0.4546930790 
0.6472471464  0.7876506024  0.7891566265  0.6690821256  0.7064439141  0.8765060241  0.8313253012  0.8562874251  0.8452380952  0.8421052632  0.8502994012  0.6837349398  0.6626506024  0.7129909366  0.7108433735  0.7296072508  0.7048192771  0.9085457271  0.8862275449  0.6265060241  0.5869565217  0.4833836858  7.8222578526  0.6668839455  10            0.1423472881 
0.7740443945  0.8870481928  0.9096385542  0.8260869565  0.6634844869  0.7454819277  0.7289156627  0.9041916168  0.8809523810  0.8526315789  0.8323353293  0.9894578313  0.9819277108  0.7296072508  0.7891566265  0.9546827795  0.9518072289  0.8365817091  0.8502994012  0.8650602410  0.7415458937  0.9667673716  5.4200447083  0.6689839363  20            0.1390102386 
0.7977069671  0.8328313253  0.8012048193  0.7681159420  0.8568019093  0.8885542169  0.8433734940  0.9401197605  0.9047619048  0.9233082707  0.8862275449  0.9608433735  0.9638554217  0.8111782477  0.8373493976  0.9697885196  0.9518072289  0.9550224888  0.9520958084  0.7132530120  0.8526570048  1.4501510574  4.4255829811  0.6689839363  30            0.1318437099 
0.8141836343  0.8674698795  0.8795180723  0.7946859903  0.7780429594  0.8509036145  0.8012048193  0.8952095808  0.8809523810  0.8977443609  0.9281437126  0.9683734940  0.9698795181  0.7930513595  0.8132530120  0.9577039275  0.9638554217  0.9685157421  0.9461077844  0.8216867470  0.8623188406  1.9335347432  3.7933122635  0.6689839363  40            0.1329140186 
0.8207469496  0.8569277108  0.8614457831  0.8067632850  0.8066825776  0.9277108434  0.8433734940  0.9146706587  0.8869047619  0.9112781955  0.9341317365  0.9683734940  0.9698795181  0.8368580060  0.8433734940  0.9516616314  0.9518072289  0.9925037481  0.9700598802  0.8096385542  0.8599033816  2.4169184290  3.4424946070  0.6689839363  50            0.1355821371 
0.7405530183  0.9081325301  0.8734939759  0.7801932367  0.7828162291  0.8629518072  0.8373493976  0.9745508982  0.9821428571  0.9578947368  0.9281437126  0.9969879518  1.0000000000  0.7447129909  0.7771084337  0.9909365559  0.9879518072  0.9670164918  0.9401197605  0.7301204819  0.6690821256  2.9003021148  3.2616084099  0.6689839363  60            0.1335514545 
0.7759387347  0.8975903614  0.8915662651  0.7753623188  0.8568019093  0.9186746988  0.8795180723  0.9491017964  0.9285714286  0.9714285714  0.9461077844  0.9969879518  1.0000000000  0.7945619335  0.8192771084  0.9743202417  0.9698795181  0.9865067466  0.9700598802  0.7614457831  0.7101449275  3.3836858006  3.1760495663  0.6689839363  70            0.1367606640 
0.7906154485  0.9322289157  0.9216867470  0.8188405797  0.7780429594  0.9487951807  0.8674698795  0.9865269461  0.9821428571  0.9774436090  0.9580838323  0.9924698795  0.9939759036  0.8081570997  0.8253012048  0.9848942598  0.9879518072  0.9745127436  0.9461077844  0.8506024096  0.7149758454  3.8670694864  2.9358098269  0.6689839363  80            0.1348376274 
0.7358010889  0.9789156627  0.9518072289  0.8115942029  0.7470167064  0.8268072289  0.7530120482  0.9970059880  0.9880952381  0.9909774436  0.9520958084  0.9759036145  0.9879518072  0.6042296073  0.6506024096  0.9652567976  0.9698795181  0.9415292354  0.8862275449  0.7783132530  0.6062801932  4.3504531722  2.8067769289  0.6689839363  90            0.1298896313 
0.7592886228  0.8719879518  0.8734939759  0.7512077295  0.7756563246  0.9698795181  0.8915662651  0.8832335329  0.8690476190  0.8857142857  0.9041916168  0.9352409639  0.9457831325  0.8157099698  0.8072289157  0.9048338369  0.9096385542  0.9940029985  0.9640718563  0.7397590361  0.7705314010  4.8338368580  3.0518605947  0.6689839363  100           0.1354313374 
0.7821287085  0.8734939759  0.8554216867  0.7946859903  0.8066825776  0.9819277108  0.8915662651  0.9206586826  0.8869047619  0.9624060150  0.9700598802  0.9804216867  0.9698795181  0.8731117825  0.8915662651  0.9803625378  0.9819277108  1.0000000000  1.0000000000  0.7614457831  0.7657004831  5.3172205438  3.1123354197  0.6689839363  110           0.1339900494 
0.7566325244  0.8975903614  0.8915662651  0.7342995169  0.8568019093  0.9442771084  0.8855421687  0.9101796407  0.8928571429  0.9744360902  0.9700598802  0.9894578313  0.9939759036  0.7975830816  0.8313253012  0.9531722054  0.9457831325  0.9955022489  0.9940119760  0.7325301205  0.7028985507  5.8006042296  2.7662213802  0.6689839363  120           0.1311503410 
0.7593198014  0.9593373494  0.9337349398  0.8357487923  0.7470167064  0.9006024096  0.8433734940  0.9700598802  0.9761904762  0.9969924812  0.9760479042  0.9969879518  1.0000000000  0.6525679758  0.6867469880  0.9954682779  0.9939759036  0.9850074963  0.9760479042  0.8313253012  0.6231884058  6.2839879154  2.6120816231  0.6689839363  130           0.1352696896 
0.7449626461  0.9518072289  0.9277108434  0.8381642512  0.7064439141  0.9231927711  0.8614457831  0.9745508982  0.9821428571  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.6812688822  0.7409638554  0.9984894260  0.9939759036  0.9910044978  0.9760479042  0.8096385542  0.6256038647  6.7673716012  2.5071934223  0.6689839363  140           0.1347275734 
0.7364322532  0.9728915663  0.9397590361  0.8212560386  0.7350835322  0.9246987952  0.8674698795  0.9880239521  0.9880952381  0.9969924812  0.9760479042  0.9954819277  0.9939759036  0.6933534743  0.7530120482  1.0000000000  1.0000000000  0.9850074963  0.9760479042  0.7927710843  0.5966183575  7.2507552870  2.4063781023  0.6689839363  150           0.1322863102 
0.7171001984  0.9608433735  0.9277108434  0.7922705314  0.7470167064  0.9051204819  0.8734939759  0.9880239521  0.9880952381  0.9969924812  0.9640718563  0.9969879518  1.0000000000  0.6933534743  0.7409638554  0.9984894260  0.9939759036  0.9925037481  0.9700598802  0.7469879518  0.5821256039  7.7341389728  2.3169797421  0.6689839363  160           0.1354099989 
0.7332805887  0.9683734940  0.9216867470  0.7826086957  0.7875894988  0.9427710843  0.8915662651  0.9760479042  0.9642857143  0.9969924812  0.9760479042  0.9969879518  1.0000000000  0.7613293051  0.7951807229  0.9954682779  0.9939759036  1.0000000000  1.0000000000  0.7493975904  0.6135265700  8.2175226586  2.3257538557  0.6689839363  170           0.1366778612 
0.7256391135  0.9518072289  0.9277108434  0.8091787440  0.7112171838  0.9548192771  0.8915662651  0.9595808383  0.9583333333  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.7129909366  0.7710843373  0.9939577039  0.9879518072  0.9985007496  0.9820359281  0.7855421687  0.5966183575  8.7009063444  2.2945631266  0.6689839363  180           0.1353750944 
0.7519398371  0.9262048193  0.9216867470  0.8091787440  0.8066825776  0.9804216867  0.8975903614  0.9326347305  0.8988095238  0.9909774436  0.9760479042  0.9984939759  0.9939759036  0.8338368580  0.8313253012  0.9939577039  0.9879518072  1.0000000000  1.0000000000  0.7542168675  0.6376811594  9.1842900302  2.1206732988  0.6689839363  190           0.1333336115 
0.7376890392  0.9442771084  0.9216867470  0.8164251208  0.7207637232  0.9774096386  0.8975903614  0.9401197605  0.9166666667  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.7764350453  0.7951807229  0.9954682779  0.9939759036  0.9985007496  0.9700598802  0.7831325301  0.6304347826  9.6676737160  2.1704373717  0.6689839363  200           0.1357190609 
0.7214151094  0.9698795181  0.9397590361  0.7946859903  0.7159904535  0.9713855422  0.8975903614  0.9760479042  0.9642857143  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.8157099698  0.8072289157  0.9984894260  0.9939759036  1.0000000000  1.0000000000  0.7566265060  0.6183574879  10.151057401  2.2084044456  0.6689839363  210           0.1433730364 
0.7178422937  0.9894578313  0.9578313253  0.8019323671  0.6968973747  0.9442771084  0.8734939759  0.9910179641  0.9880952381  0.9969924812  0.9760479042  0.9954819277  0.9939759036  0.6495468278  0.6987951807  1.0000000000  1.0000000000  0.9955022489  0.9940119760  0.7686746988  0.6038647343  10.634441087  2.1384335399  0.6689839363  220           0.1415396452 
0.7052520413  0.9819277108  0.9397590361  0.7971014493  0.6706443914  0.9668674699  0.9036144578  0.9880239521  0.9880952381  0.9969924812  0.9760479042  1.0000000000  1.0000000000  0.7265861027  0.7530120482  1.0000000000  1.0000000000  0.9970014993  0.9880239521  0.7493975904  0.6038647343  11.117824773  2.1293893099  0.6689839363  230           0.1692301512 
Process Process-4009:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/threading.py", line 226, in __init__
    self._release_save = lock._release_save
AttributeError: '_thread.lock' object has no attribute '_release_save'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 251, in _bootstrap
    util._run_after_forkers()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 132, in _run_after_forkers
    func(obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 69, in _after_fork
    self._notempty = threading.Condition(threading.Lock())
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/threading.py", line 226, in __init__
    self._release_save = lock._release_save
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 896, in __init__
    index_queue = multiprocessing_context.Queue()  # type: ignore
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 102, in Queue
    return Queue(maxsize, ctx=self.get_context())
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 42, in __init__
    self._rlock = ctx.Lock()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 67, in Lock
    return Lock(ctx=self.get_context())
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/synchronize.py", line 162, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/synchronize.py", line 50, in __init__
    def __init__(self, kind, value, maxvalue, *, ctx):
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0004
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3477880010  0.4804216867  0.4759036145  0.3768115942  0.3412887828  0.5391566265  0.5421686747  0.4955089820  0.5059523810  0.5037593985  0.5449101796  0.5015060241  0.5000000000  0.3655589124  0.4337349398  0.5060422961  0.4759036145  0.4257871064  0.4431137725  0.3566265060  0.3164251208  0.0000000000  9.8018226624  0.2909131050  0             0.4715015888 
0.6485785555  0.8283132530  0.8192771084  0.6594202899  0.6563245823  0.8388554217  0.8132530120  0.8697604790  0.8630952381  0.7819548872  0.7784431138  0.7349397590  0.7228915663  0.6586102719  0.6746987952  0.7613293051  0.7469879518  0.8500749625  0.8203592814  0.6722891566  0.6062801932  0.4833836858  8.0042428970  0.4781041145  10            0.1388942719 
0.7474934014  0.8629518072  0.8734939759  0.7584541063  0.6682577566  0.7695783133  0.7409638554  0.8892215569  0.8809523810  0.8165413534  0.8682634731  0.9141566265  0.9216867470  0.7930513595  0.8373493976  0.8942598187  0.8915662651  0.9085457271  0.9221556886  0.8096385542  0.7536231884  0.9667673716  5.6379752636  0.4781599045  20            0.1352991819 
0.7977429277  0.8403614458  0.8072289157  0.7753623188  0.8424821002  0.8765060241  0.8554216867  0.9356287425  0.9107142857  0.9293233083  0.8862275449  0.9442771084  0.9578313253  0.8006042296  0.8313253012  0.9592145015  0.9216867470  0.9400299850  0.9401197605  0.7253012048  0.8478260870  1.4501510574  4.6264748573  0.4781599045  30            0.1310729980 
0.8025461293  0.8614457831  0.8674698795  0.7391304348  0.8400954654  0.8885542169  0.8554216867  0.8952095808  0.8809523810  0.8947368421  0.9161676647  0.9743975904  0.9819277108  0.8277945619  0.8674698795  0.9577039275  0.9638554217  0.9865067466  0.9700598802  0.7831325301  0.8478260870  1.9335347432  3.9670063734  0.4781727791  40            0.1346240520 
0.8051848361  0.8584337349  0.8433734940  0.7898550725  0.7613365155  0.9021084337  0.8253012048  0.8937125749  0.8869047619  0.9037593985  0.9281437126  0.9653614458  0.9698795181  0.8323262840  0.8493975904  0.9501510574  0.9457831325  0.9820089955  0.9520958084  0.8072289157  0.8623188406  2.4169184290  3.5475884438  0.4781727791  50            0.1340215206 
0.7707632300  0.9066265060  0.8795180723  0.8164251208  0.7661097852  0.8539156627  0.8253012048  0.9550898204  0.9523809524  0.9639097744  0.9401197605  0.9939759036  1.0000000000  0.7114803625  0.7530120482  0.9954682779  0.9939759036  0.9655172414  0.9461077844  0.7855421687  0.7149758454  2.9003021148  3.3492879391  0.4781727791  60            0.1369618654 
0.7614720339  0.9186746988  0.8795180723  0.7777777778  0.8520286396  0.8960843373  0.8614457831  0.9760479042  0.9761904762  0.9789473684  0.9401197605  0.9909638554  1.0000000000  0.7930513595  0.8253012048  0.9864048338  0.9819277108  0.9760119940  0.9520958084  0.7421686747  0.6739130435  3.3836858006  3.3192842960  0.4781727791  70            0.1305644512 
0.7652060145  0.9382530120  0.8975903614  0.7922705314  0.8090692124  0.9352409639  0.8734939759  0.9805389222  0.9702380952  0.9849624060  0.9401197605  0.9834337349  0.9939759036  0.8338368580  0.8554216867  0.9818731118  0.9879518072  0.9775112444  0.9461077844  0.7734939759  0.6859903382  3.8670694864  3.0173078299  0.4781727791  80            0.1347961903 
0.7267418712  0.9789156627  0.9397590361  0.7874396135  0.7541766110  0.8328313253  0.8012048193  0.9970059880  0.9880952381  0.9849624060  0.9520958084  0.9804216867  0.9939759036  0.6570996979  0.7048192771  0.9697885196  0.9759036145  0.9415292354  0.9101796407  0.7445783133  0.6207729469  4.3504531722  2.8508425474  0.4781727791  90            0.1465874195 
0.7610555258  0.8689759036  0.8614457831  0.7318840580  0.7899761337  0.9728915663  0.8915662651  0.8712574850  0.8571428571  0.8932330827  0.9221556886  0.9382530120  0.9698795181  0.8262839879  0.8253012048  0.9063444109  0.9277108434  0.9925037481  0.9580838323  0.7421686747  0.7801932367  4.8338368580  3.0753585577  0.4781727791  100           0.1347177267 
0.7797880811  0.8689759036  0.8373493976  0.7850241546  0.7780429594  0.9819277108  0.8915662651  0.9146706587  0.8869047619  0.9548872180  0.9640718563  0.9834337349  0.9819277108  0.8534743202  0.8734939759  0.9773413897  0.9578313253  1.0000000000  1.0000000000  0.7807228916  0.7753623188  5.3172205438  3.1201739788  0.4781727791  110           0.1326484442 
0.7832211416  0.8945783133  0.8795180723  0.7657004831  0.8448687351  0.9533132530  0.8855421687  0.9161676647  0.8928571429  0.9714285714  0.9700598802  0.9894578313  0.9939759036  0.8217522659  0.8554216867  0.9697885196  0.9518072289  1.0000000000  1.0000000000  0.7614457831  0.7608695652  5.8006042296  2.8191454172  0.4781727791  120           0.1317764521 
0.7671816143  0.9442771084  0.9337349398  0.8309178744  0.7446300716  0.9006024096  0.8674698795  0.9580838323  0.9404761905  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.6495468278  0.7108433735  0.9924471299  0.9939759036  0.9850074963  0.9760479042  0.8240963855  0.6690821256  6.2839879154  2.6710306644  0.4781727791  130           0.1305307150 
0.7455060913  0.9472891566  0.9216867470  0.8164251208  0.7303102625  0.9397590361  0.8795180723  0.9760479042  0.9880952381  0.9939849624  0.9760479042  0.9969879518  1.0000000000  0.7416918429  0.7891566265  0.9984894260  0.9939759036  0.9925037481  0.9940119760  0.7903614458  0.6449275362  6.7673716012  2.5780962706  0.4781727791  140           0.1359438896 
0.7424837880  0.9563253012  0.9337349398  0.8115942029  0.7279236277  0.9186746988  0.8795180723  0.9820359281  0.9880952381  0.9969924812  0.9760479042  0.9969879518  1.0000000000  0.7311178248  0.7831325301  0.9984894260  0.9939759036  0.9865067466  0.9700598802  0.8072289157  0.6231884058  7.2507552870  2.4859736919  0.4781727791  150           0.1343441725 
0.7256520704  0.9608433735  0.9277108434  0.8188405797  0.7064439141  0.9201807229  0.8734939759  0.9880239521  0.9880952381  0.9954887218  0.9700598802  0.9969879518  1.0000000000  0.7175226586  0.7409638554  1.0000000000  1.0000000000  0.9925037481  0.9820359281  0.7879518072  0.5893719807  7.7341389728  2.3300888062  0.4781727791  160           0.1327337503 
0.7363430791  0.9563253012  0.9216867470  0.7946859903  0.7708830549  0.9472891566  0.8975903614  0.9625748503  0.9583333333  0.9969924812  0.9760479042  0.9969879518  1.0000000000  0.7945619335  0.8192771084  0.9939577039  0.9879518072  0.9955022489  0.9940119760  0.7614457831  0.6183574879  8.2175226586  2.3633498907  0.4781727791  170           0.1342647552 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 896, in __init__
    index_queue = multiprocessing_context.Queue()  # type: ignore
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 102, in Queue
    return Queue(maxsize, ctx=self.get_context())
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 42, in __init__
    self._rlock = ctx.Lock()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 67, in Lock
    return Lock(ctx=self.get_context())
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/synchronize.py", line 162, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/synchronize.py", line 58, in __init__
    kind, value, maxvalue, self._make_name(),
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/joblib/externals/loky/backend/__init__.py", line 9, in _make_name
    name = '/loky-%i-%s' % (os.getpid(), next(synchronize.SemLock._rand))
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/tempfile.py", line 157, in __next__
    def __next__(self):
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2173, in <module>
    main()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2166, in main
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1984, in handle_keyboard_interrupt
    traceback.print_exception(type(value), value, tb, limit=limit)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/traceback.py", line 104, in print_exception
    type(value), value, tb, limit=limit).format(chain=chain):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/traceback.py", line 509, in __init__
    capture_locals=capture_locals)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/traceback.py", line 364, in extract
    f.line
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/traceback.py", line 286, in line
    self._line = linecache.getline(self.filename, self.lineno).strip()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/linecache.py", line 16, in getline
    lines = getlines(filename, module_globals)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/linecache.py", line 47, in getlines
    return updatecache(filename, module_globals)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/linecache.py", line 137, in updatecache
    lines = fp.readlines()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/codecs.py", line 318, in decode
    def decode(self, input, final=False):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1401612) is killed by signal: Terminated. 
We've got an error while stopping in post-mortem: <class 'RuntimeError'>

Exception ignored in: Error in atexit._run_exitfuncs:

Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 230, in _run_finalizers
    def _run_finalizers(minpriority=None):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 167, in __call__
    def __call__(self, wr=None,
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 196, in _finalize_close
    @staticmethod
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/threading.py", line 334, in notify
    def notify(self, n=1):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 63, in handler
    def handler(signum, frame):
RuntimeError: DataLoader worker (pid 1400385) is killed by signal: Terminated. 
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 24, in poll
    def poll(self, flag=os.WNOHANG):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 63, in handler
    def handler(signum, frame):
RuntimeError: DataLoader worker (pid 1400436) is killed by signal: Terminated. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 282, in _exit_function
    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 41, in active_children
    def active_children():
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 52, in _cleanup
    def _cleanup():
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 24, in poll
    def poll(self, flag=os.WNOHANG):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 63, in handler
    def handler(signum, frame):
RuntimeError: DataLoader worker (pid 1400058) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0007
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 104, in start
    _cleanup()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 55, in _cleanup
    if p._popen.poll() is not None:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 34, in poll
    break
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0007
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3423849526  0.3493975904  0.3493975904  0.3526570048  0.3317422434  0.3493975904  0.3493975904  0.3607784431  0.3630952381  0.3398496241  0.3353293413  0.3478915663  0.3433734940  0.3338368580  0.3433734940  0.3368580060  0.3313253012  0.3358320840  0.3353293413  0.3518072289  0.3333333333  0.0000000000  9.8018226624  0.2909131050  0             0.5182888508 
0.6411050518  0.7289156627  0.7228915663  0.6304347826  0.7517899761  0.8509036145  0.8132530120  0.8428143713  0.8392857143  0.8451127820  0.8023952096  0.6882530120  0.6686746988  0.6858006042  0.7108433735  0.7462235650  0.7108433735  0.8425787106  0.8143712575  0.5710843373  0.6111111111  0.4833836858  7.8223386288  0.4781041145  10            0.1403927803 
0.7907612325  0.8599397590  0.8734939759  0.7608695652  0.7374701671  0.8117469880  0.7891566265  0.8922155689  0.8809523810  0.9007518797  0.8802395210  0.9713855422  0.9698795181  0.7915407855  0.8313253012  0.9486404834  0.9397590361  0.9160419790  0.9161676647  0.8120481928  0.8526570048  0.9667673716  5.1473948956  0.4781041145  20            0.1320779085 
0.7898610908  0.8795180723  0.8072289157  0.7681159420  0.8568019093  0.8900602410  0.8373493976  0.9491017964  0.9166666667  0.9338345865  0.8682634731  0.9578313253  0.9759036145  0.7975830816  0.8192771084  0.9682779456  0.9819277108  0.9535232384  0.9580838323  0.7060240964  0.8285024155  1.4501510574  4.2390647888  0.4781041145  30            0.1355655670 
0.8202595585  0.8795180723  0.8915662651  0.8188405797  0.7589498807  0.8569277108  0.8012048193  0.9251497006  0.9047619048  0.9037593985  0.9281437126  0.9879518072  0.9879518072  0.7854984894  0.7951807229  0.9682779456  0.9698795181  0.9775112444  0.9341317365  0.8554216867  0.8478260870  1.9335347432  3.6278245449  0.4782195091  40            0.1324278831 
0.8182495921  0.8268072289  0.8132530120  0.7874396135  0.8424821002  0.9442771084  0.8734939759  0.8847305389  0.8630952381  0.9037593985  0.9401197605  0.9623493976  0.9698795181  0.8685800604  0.8855421687  0.9456193353  0.9397590361  0.9805097451  0.9820359281  0.7662650602  0.8768115942  2.4169184290  3.2886632204  0.4782195091  50            0.1321402788 
0.7660546062  0.9412650602  0.9096385542  0.8550724638  0.7183770883  0.8584337349  0.8072289157  0.9865269461  0.9821428571  0.9699248120  0.9281437126  0.9954819277  0.9819277108  0.6283987915  0.6987951807  0.9969788520  0.9879518072  0.9565217391  0.8982035928  0.8216867470  0.6690821256  2.9003021148  3.0640856028  0.4782195091  60            0.1330034733 
0.7778281401  0.9201807229  0.8975903614  0.7971014493  0.8305489260  0.9156626506  0.8674698795  0.9610778443  0.9404761905  0.9774436090  0.9461077844  0.9969879518  1.0000000000  0.7930513595  0.8132530120  0.9894259819  0.9819277108  0.9880059970  0.9640718563  0.7638554217  0.7198067633  3.3836858006  3.0138184309  0.4782195091  70            0.1321096897 
0.7863282538  0.9246987952  0.9036144578  0.8091787440  0.8114558473  0.9623493976  0.8855421687  0.9670658683  0.9642857143  0.9804511278  0.9700598802  0.9849397590  0.9879518072  0.8474320242  0.8614457831  0.9909365559  0.9879518072  0.9925037481  0.9820359281  0.7831325301  0.7415458937  3.8670694864  2.7529748440  0.4782195091  80            0.1319606781 
0.7327056175  0.9789156627  0.9518072289  0.7850241546  0.7804295943  0.8780120482  0.8253012048  0.9970059880  0.9880952381  0.9984962406  0.9580838323  0.9834337349  0.9939759036  0.6555891239  0.7228915663  0.9848942598  0.9759036145  0.9640179910  0.9401197605  0.7373493976  0.6280193237  4.3504531722  2.6688750505  0.4782195091  90            0.1314594269 
0.7572869007  0.8719879518  0.8614457831  0.7053140097  0.8424821002  0.9668674699  0.8915662651  0.8847305389  0.8750000000  0.9187969925  0.9281437126  0.9533132530  0.9698795181  0.8353474320  0.8373493976  0.9093655589  0.9156626506  0.9955022489  0.9820359281  0.7204819277  0.7608695652  4.8338368580  2.7852590084  0.4782195091  100           0.1382539749 
0.7633254786  0.9277108434  0.8795180723  0.7753623188  0.8400954654  0.9713855422  0.8975903614  0.9461077844  0.9404761905  0.9939849624  0.9760479042  0.9864457831  0.9819277108  0.8293051360  0.8493975904  0.9864048338  0.9698795181  0.9970014993  1.0000000000  0.7325301205  0.7053140097  5.3172205438  2.8636785746  0.4782195091  110           0.1375733137 
0.7487332957  0.9277108434  0.8915662651  0.7536231884  0.8735083532  0.9367469880  0.8915662651  0.9191616766  0.9047619048  0.9909774436  0.9760479042  0.9954819277  0.9939759036  0.7688821752  0.8132530120  0.9803625378  0.9578313253  0.9985007496  0.9940119760  0.7301204819  0.6376811594  5.8006042296  2.6195469379  0.4782195091  120           0.1776359558 
0.7587996377  0.9563253012  0.9337349398  0.8502415459  0.7231503580  0.9141566265  0.8493975904  0.9625748503  0.9583333333  0.9969924812  0.9760479042  0.9969879518  1.0000000000  0.6450151057  0.7168674699  0.9984894260  0.9939759036  0.9955022489  0.9940119760  0.8120481928  0.6497584541  6.2839879154  2.4726067543  0.4782195091  130           0.1353855610 
0.7220046316  0.9427710843  0.9277108434  0.8115942029  0.7231503580  0.9442771084  0.8975903614  0.9565868263  0.9464285714  0.9909774436  0.9760479042  1.0000000000  1.0000000000  0.7628398792  0.7891566265  0.9954682779  0.9939759036  0.9985007496  0.9940119760  0.7445783133  0.6086956522  6.7673716012  2.3932292700  0.4782195091  140           0.1371970654 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1171, in _next_data
    self._shutdown_workers()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1297, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 124, in join
    res = self._popen.wait(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 47, in wait
    if not wait([self.sentinel], timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3724929638  0.4201807229  0.4638554217  0.4154589372  0.3532219570  0.4743975904  0.4518072289  0.4431137725  0.4285714286  0.4195488722  0.4610778443  0.4412650602  0.4397590361  0.3564954683  0.3855421687  0.4244712991  0.4277108434  0.3778110945  0.3353293413  0.3855421687  0.3357487923  0.0000000000  9.8018226624  0.2909131050  0             0.4816412926 
0.6472471464  0.7876506024  0.7891566265  0.6690821256  0.7064439141  0.8765060241  0.8313253012  0.8562874251  0.8452380952  0.8421052632  0.8502994012  0.6837349398  0.6626506024  0.7129909366  0.7108433735  0.7296072508  0.7048192771  0.9085457271  0.8862275449  0.6265060241  0.5869565217  0.4833836858  7.8222578526  0.4781041145  10            0.1373590231 
0.7740443945  0.8870481928  0.9096385542  0.8260869565  0.6634844869  0.7454819277  0.7289156627  0.9041916168  0.8809523810  0.8526315789  0.8323353293  0.9894578313  0.9819277108  0.7296072508  0.7891566265  0.9546827795  0.9518072289  0.8365817091  0.8502994012  0.8650602410  0.7415458937  0.9667673716  5.4200447083  0.4811902046  20            0.1368099213 
0.7977069671  0.8328313253  0.8012048193  0.7681159420  0.8568019093  0.8885542169  0.8433734940  0.9401197605  0.9047619048  0.9233082707  0.8862275449  0.9608433735  0.9638554217  0.8111782477  0.8373493976  0.9697885196  0.9518072289  0.9550224888  0.9520958084  0.7132530120  0.8526570048  1.4501510574  4.4255829811  0.4811902046  30            0.1308082819 
0.8141836343  0.8674698795  0.8795180723  0.7946859903  0.7780429594  0.8509036145  0.8012048193  0.8952095808  0.8809523810  0.8977443609  0.9281437126  0.9683734940  0.9698795181  0.7930513595  0.8132530120  0.9577039275  0.9638554217  0.9685157421  0.9461077844  0.8216867470  0.8623188406  1.9335347432  3.7933122635  0.4811902046  40            0.1334763765 
0.8207469496  0.8569277108  0.8614457831  0.8067632850  0.8066825776  0.9277108434  0.8433734940  0.9146706587  0.8869047619  0.9112781955  0.9341317365  0.9683734940  0.9698795181  0.8368580060  0.8433734940  0.9516616314  0.9518072289  0.9925037481  0.9700598802  0.8096385542  0.8599033816  2.4169184290  3.4424946070  0.4811902046  50            0.1290106058 
0.8183831796  0.9126506024  0.9156626506  0.8526570048  0.7828162291  0.8629518072  0.8132530120  0.9610778443  0.9523809524  0.9684210526  0.9341317365  0.9969879518  1.0000000000  0.7643504532  0.7951807229  0.9879154079  0.9879518072  0.9700149925  0.9401197605  0.8433734940  0.7946859903  2.9003021148  3.2478852987  0.4811902046  60            0.1307905912 
0.8176742736  0.8945783133  0.8915662651  0.8140096618  0.8233890215  0.9036144578  0.8313253012  0.9446107784  0.9226190476  0.9563909774  0.9461077844  0.9879518072  1.0000000000  0.8021148036  0.8012048193  0.9697885196  0.9638554217  0.9865067466  0.9580838323  0.8144578313  0.8188405797  3.3836858006  3.3060932875  0.4811902046  70            0.1329796553 
0.8195031206  0.8990963855  0.8975903614  0.8333333333  0.8162291169  0.8765060241  0.8313253012  0.9535928144  0.9464285714  0.9593984962  0.9461077844  0.9969879518  1.0000000000  0.7885196375  0.8072289157  0.9833836858  0.9819277108  0.9760119940  0.9401197605  0.8216867470  0.8067632850  3.8670694864  3.2008004427  0.4811902046  80            0.1338110447 
0.8008812879  0.9246987952  0.9036144578  0.8429951691  0.7828162291  0.8554216867  0.8072289157  0.9730538922  0.9642857143  0.9699248120  0.9281437126  0.9894578313  0.9939759036  0.7401812689  0.7710843373  0.9848942598  1.0000000000  0.9655172414  0.9101796407  0.8265060241  0.7512077295  4.3504531722  3.1679038763  0.4811902046  90            0.1332392454 
0.8158943442  0.9021084337  0.8975903614  0.8333333333  0.8114558473  0.8734939759  0.8313253012  0.9535928144  0.9464285714  0.9654135338  0.9461077844  0.9969879518  1.0000000000  0.7764350453  0.8072289157  0.9864048338  0.9819277108  0.9730134933  0.9401197605  0.8216867470  0.7971014493  4.8338368580  3.3351851940  0.4811902046  100           0.1318218946 
0.8039049074  0.9186746988  0.9036144578  0.8502415459  0.7804295943  0.8614457831  0.8072289157  0.9655688623  0.9583333333  0.9684210526  0.9341317365  0.9939759036  1.0000000000  0.7477341390  0.7771084337  0.9909365559  1.0000000000  0.9685157421  0.9101796407  0.8313253012  0.7536231884  5.3172205438  3.2948006392  0.4811902046  110           0.1346307039 
0.8111541250  0.9186746988  0.9156626506  0.8502415459  0.7780429594  0.8689759036  0.8132530120  0.9625748503  0.9583333333  0.9684210526  0.9341317365  0.9939759036  1.0000000000  0.7552870091  0.7831325301  0.9879154079  0.9879518072  0.9700149925  0.9161676647  0.8385542169  0.7777777778  5.8006042296  3.1641386032  0.4811902046  120           0.1312865257 
0.7997483900  0.9307228916  0.9156626506  0.8574879227  0.7541766110  0.8418674699  0.7891566265  0.9745508982  0.9583333333  0.9729323308  0.9281437126  0.9924698795  0.9939759036  0.7160120846  0.7590361446  0.9909365559  1.0000000000  0.9655172414  0.9101796407  0.8457831325  0.7415458937  6.2839879154  3.0860986233  0.4811902046  130           0.1332734585 
0.8057366646  0.9126506024  0.9156626506  0.8502415459  0.7732696897  0.8689759036  0.8132530120  0.9535928144  0.9464285714  0.9684210526  0.9461077844  0.9969879518  1.0000000000  0.7537764350  0.7771084337  0.9879154079  0.9879518072  0.9700149925  0.9281437126  0.8337349398  0.7657004831  6.7673716012  3.1496174335  0.4811902046  140           0.1357176304 
0.8110677916  0.9036144578  0.8915662651  0.8212560386  0.8114558473  0.8855421687  0.8433734940  0.9491017964  0.9285714286  0.9624060150  0.9461077844  0.9969879518  1.0000000000  0.7764350453  0.8072289157  0.9788519637  0.9638554217  0.9805097451  0.9461077844  0.8144578313  0.7971014493  7.2507552870  3.2865350962  0.4811902046  150           0.1342338324 
0.8038790630  0.9036144578  0.9036144578  0.8333333333  0.7923627685  0.8795180723  0.8433734940  0.9535928144  0.9464285714  0.9684210526  0.9461077844  0.9969879518  1.0000000000  0.7673716012  0.7951807229  0.9864048338  0.9819277108  0.9700149925  0.9401197605  0.8144578313  0.7753623188  7.7341389728  3.0658962011  0.4811902046  160           0.1355859041 
0.8074734273  0.9051204819  0.8975903614  0.8285024155  0.8019093079  0.8825301205  0.8433734940  0.9491017964  0.9285714286  0.9624060150  0.9461077844  0.9969879518  1.0000000000  0.7703927492  0.8072289157  0.9864048338  0.9819277108  0.9760119940  0.9401197605  0.8144578313  0.7850241546  8.2175226586  3.1769888878  0.4811902046  170           0.1345597267 
0.8080858837  0.9021084337  0.8975903614  0.8309178744  0.7971360382  0.8825301205  0.8433734940  0.9491017964  0.9166666667  0.9624060150  0.9461077844  0.9969879518  1.0000000000  0.7673716012  0.8072289157  0.9848942598  0.9759036145  0.9775112444  0.9461077844  0.8240963855  0.7801932367  8.7009063444  3.1958310843  0.4811902046  180           0.1356554985 
0.8110376513  0.8644578313  0.8554216867  0.7898550725  0.8257756563  0.9367469880  0.8674698795  0.9176646707  0.8869047619  0.9263157895  0.9341317365  0.9819277108  0.9879518072  0.8429003021  0.8433734940  0.9561933535  0.9578313253  0.9940029985  0.9880239521  0.7927710843  0.8357487923  9.1842900302  3.1091623545  0.4811902046  190           0.1357605219 
0.8008870389  0.9156626506  0.9156626506  0.8429951691  0.7804295943  0.8674698795  0.8192771084  0.9535928144  0.9464285714  0.9699248120  0.9401197605  0.9969879518  1.0000000000  0.7507552870  0.7771084337  0.9864048338  0.9819277108  0.9700149925  0.9161676647  0.8289156627  0.7512077295  9.6676737160  3.0696945429  0.4811902046  200           0.1355073214 
0.8135077094  0.9006024096  0.8915662651  0.8236714976  0.8019093079  0.9006024096  0.8433734940  0.9491017964  0.9166666667  0.9593984962  0.9461077844  0.9969879518  1.0000000000  0.7900302115  0.8132530120  0.9758308157  0.9638554217  0.9865067466  0.9461077844  0.8216867470  0.8067632850  10.151057401  3.1660525560  0.4811902046  210           0.1358915329 
0.8032780390  0.9096385542  0.9036144578  0.8309178744  0.7899761337  0.8780120482  0.8373493976  0.9520958084  0.9404761905  0.9684210526  0.9461077844  0.9969879518  1.0000000000  0.7613293051  0.7951807229  0.9864048338  0.9819277108  0.9700149925  0.9401197605  0.8216867470  0.7705314010  10.634441087  3.0752607346  0.4811902046  220           0.1385930538 
0.8026856761  0.9156626506  0.9036144578  0.8454106280  0.7852028640  0.8704819277  0.8192771084  0.9565868263  0.9464285714  0.9699248120  0.9520958084  0.9969879518  1.0000000000  0.7522658610  0.7831325301  0.9864048338  0.9819277108  0.9700149925  0.9401197605  0.8265060241  0.7536231884  11.117824773  3.1842146397  0.4811902046  230           0.1361834288 
0.8200868223  0.8765060241  0.8795180723  0.8212560386  0.8233890215  0.9262048193  0.8614457831  0.9341317365  0.9047619048  0.9563909774  0.9461077844  0.9939759036  1.0000000000  0.8111782477  0.8132530120  0.9667673716  0.9638554217  0.9910044978  0.9640718563  0.8192771084  0.8164251208  11.601208459  3.2312124252  0.4811902046  240           0.1357142210 
0.8128779307  0.9006024096  0.8915662651  0.8236714976  0.8114558473  0.9006024096  0.8433734940  0.9491017964  0.9166666667  0.9624060150  0.9461077844  0.9969879518  1.0000000000  0.7900302115  0.8132530120  0.9788519637  0.9638554217  0.9865067466  0.9461077844  0.8168674699  0.7995169082  12.084592145  3.1998140812  0.4811902046  250           0.1356926441 
0.8128779307  0.9006024096  0.8915662651  0.8236714976  0.8114558473  0.9066265060  0.8433734940  0.9491017964  0.9166666667  0.9624060150  0.9461077844  0.9969879518  1.0000000000  0.7930513595  0.8132530120  0.9788519637  0.9638554217  0.9865067466  0.9461077844  0.8168674699  0.7995169082  12.567975830  3.1162128210  0.4811902046  260           0.1384430170 
0.8122611090  0.8915662651  0.8915662651  0.8140096618  0.8162291169  0.9141566265  0.8493975904  0.9416167665  0.9107142857  0.9593984962  0.9461077844  0.9969879518  1.0000000000  0.7990936556  0.8132530120  0.9728096677  0.9638554217  0.9880059970  0.9520958084  0.8144578313  0.8043478261  13.051359516  3.1605082989  0.4811902046  270           0.1362733126 
0.8140971621  0.9006024096  0.9036144578  0.8236714976  0.8066825776  0.9096385542  0.8433734940  0.9416167665  0.9107142857  0.9624060150  0.9461077844  0.9969879518  1.0000000000  0.7930513595  0.8132530120  0.9728096677  0.9638554217  0.9865067466  0.9461077844  0.8216867470  0.8043478261  13.534743202  3.0512455463  0.4811902046  280           0.1363229036 
0.8087415762  0.9126506024  0.9156626506  0.8502415459  0.7780429594  0.8750000000  0.8253012048  0.9520958084  0.9404761905  0.9684210526  0.9461077844  0.9969879518  1.0000000000  0.7598187311  0.7891566265  0.9864048338  0.9819277108  0.9730134933  0.9401197605  0.8337349398  0.7729468599  14.018126888  3.0735663176  0.4811902046  290           0.1322849989 
0.8104898408  0.9021084337  0.9096385542  0.8309178744  0.8019093079  0.8990963855  0.8493975904  0.9491017964  0.9166666667  0.9624060150  0.9461077844  0.9969879518  1.0000000000  0.7839879154  0.8132530120  0.9788519637  0.9759036145  0.9805097451  0.9461077844  0.8192771084  0.7898550725  14.501510574  3.0863292694  0.4811902046  300           0.1387403488 
0.7972941297  0.9156626506  0.9036144578  0.8454106280  0.7708830549  0.8644578313  0.8192771084  0.9610778443  0.9523809524  0.9729323308  0.9520958084  0.9969879518  1.0000000000  0.7386706949  0.7650602410  0.9864048338  0.9819277108  0.9685157421  0.9101796407  0.8265060241  0.7463768116  14.984894259  3.1841247082  0.4811902046  310           0.1338617325 
0.7971619972  0.8478915663  0.8373493976  0.7777777778  0.8305489260  0.9427710843  0.8674698795  0.9071856287  0.8809523810  0.9203007519  0.9341317365  0.9698795181  0.9638554217  0.8564954683  0.8734939759  0.9501510574  0.9457831325  0.9925037481  0.9940119760  0.7469879518  0.8333333333  15.468277945  2.9714712858  0.4811902046  320           0.1339980602 
0.8026511706  0.9081325301  0.8975903614  0.8309178744  0.7995226730  0.8870481928  0.8493975904  0.9505988024  0.9345238095  0.9654135338  0.9461077844  0.9969879518  1.0000000000  0.7703927492  0.8072289157  0.9894259819  0.9819277108  0.9775112444  0.9461077844  0.8120481928  0.7681159420  15.951661631  3.1791712284  0.4811902046  330           0.1358839273 
0.7996779238  0.9111445783  0.9096385542  0.8333333333  0.7828162291  0.8825301205  0.8433734940  0.9505988024  0.9226190476  0.9654135338  0.9461077844  0.9969879518  1.0000000000  0.7598187311  0.7891566265  0.9864048338  0.9819277108  0.9775112444  0.9461077844  0.8192771084  0.7632850242  16.435045317  3.1464073658  0.4811902046  340           0.1317425013 
0.8099147307  0.8930722892  0.8975903614  0.8285024155  0.7899761337  0.8945783133  0.8433734940  0.9386227545  0.9107142857  0.9593984962  0.9461077844  0.9969879518  1.0000000000  0.7703927492  0.8072289157  0.9728096677  0.9638554217  0.9775112444  0.9461077844  0.8313253012  0.7898550725  16.918429003  3.0958252430  0.4811902046  350           0.1346717834 
0.8134673834  0.8765060241  0.8795180723  0.8164251208  0.8162291169  0.9231927711  0.8614457831  0.9341317365  0.9047619048  0.9563909774  0.9461077844  0.9969879518  1.0000000000  0.8021148036  0.8012048193  0.9697885196  0.9638554217  0.9910044978  0.9640718563  0.8168674699  0.8043478261  17.401812688  2.9982457399  0.4811902046  360           0.1365906715 
0.8068508547  0.8900602410  0.8855421687  0.8091787440  0.8090692124  0.9201807229  0.8614457831  0.9416167665  0.9107142857  0.9593984962  0.9461077844  0.9969879518  1.0000000000  0.8051359517  0.8132530120  0.9728096677  0.9638554217  0.9880059970  0.9520958084  0.8096385542  0.7995169082  17.885196374  3.0581707954  0.4811902046  370           0.1330115795 
0.7954724887  0.9186746988  0.9036144578  0.8429951691  0.7756563246  0.8750000000  0.8373493976  0.9610778443  0.9523809524  0.9729323308  0.9520958084  0.9939759036  1.0000000000  0.7432024169  0.7710843373  0.9879154079  0.9879518072  0.9685157421  0.9101796407  0.8192771084  0.7439613527  18.368580060  3.0458069086  0.4811902046  380           0.1348903894 
0.7990740590  0.9126506024  0.9036144578  0.8381642512  0.7828162291  0.8825301205  0.8433734940  0.9520958084  0.9404761905  0.9684210526  0.9461077844  0.9969879518  1.0000000000  0.7583081571  0.7951807229  0.9894259819  0.9819277108  0.9760119940  0.9401197605  0.8192771084  0.7560386473  18.851963746  3.1326892376  0.4811902046  390           0.1327685118 
0.8068250797  0.8689759036  0.8614457831  0.7922705314  0.8233890215  0.9382530120  0.8734939759  0.9176646707  0.8869047619  0.9368421053  0.9401197605  0.9789156627  0.9879518072  0.8459214502  0.8433734940  0.9546827795  0.9518072289  0.9970014993  0.9880239521  0.7807228916  0.8309178744  19.335347432  3.1152426243  0.4811902046  400           0.1317768574 
0.7984946531  0.9186746988  0.9156626506  0.8454106280  0.7732696897  0.8795180723  0.8313253012  0.9580838323  0.9523809524  0.9699248120  0.9520958084  0.9939759036  1.0000000000  0.7477341390  0.7771084337  0.9864048338  0.9819277108  0.9700149925  0.9281437126  0.8265060241  0.7487922705  19.818731117  3.0002638817  0.4811902046  410           0.1312576532 
0.8003047922  0.9186746988  0.9156626506  0.8502415459  0.7732696897  0.8765060241  0.8313253012  0.9580838323  0.9523809524  0.9699248120  0.9520958084  0.9969879518  1.0000000000  0.7477341390  0.7771084337  0.9864048338  0.9819277108  0.9700149925  0.9161676647  0.8289156627  0.7487922705  20.302114803  3.0016463995  0.4811902046  420           0.1305671692 
0.7973042459  0.9186746988  0.9156626506  0.8502415459  0.7684964200  0.8780120482  0.8373493976  0.9580838323  0.9523809524  0.9699248120  0.9520958084  0.9969879518  1.0000000000  0.7432024169  0.7710843373  0.9879154079  0.9879518072  0.9700149925  0.9161676647  0.8216867470  0.7487922705  20.785498489  3.1194921494  0.4811902046  430           0.1349985123 
0.8038891098  0.9126506024  0.9156626506  0.8478260870  0.7875894988  0.8900602410  0.8493975904  0.9520958084  0.9404761905  0.9669172932  0.9520958084  0.9969879518  1.0000000000  0.7613293051  0.7951807229  0.9864048338  0.9819277108  0.9760119940  0.9401197605  0.8216867470  0.7584541063  21.268882175  3.0510902166  0.4811902046  440           0.1366884947 
0.8050709253  0.9081325301  0.8975903614  0.8309178744  0.7971360382  0.8990963855  0.8493975904  0.9491017964  0.9285714286  0.9684210526  0.9461077844  0.9969879518  1.0000000000  0.7764350453  0.8072289157  0.9848942598  0.9759036145  0.9775112444  0.9461077844  0.8168674699  0.7753623188  21.752265861  3.0467182159  0.4811902046  450           0.1354174614 
0.7979052004  0.9186746988  0.9156626506  0.8550724638  0.7684964200  0.8750000000  0.8253012048  0.9685628743  0.9583333333  0.9729323308  0.9401197605  0.9939759036  1.0000000000  0.7432024169  0.7710843373  0.9894259819  0.9939759036  0.9685157421  0.9101796407  0.8265060241  0.7415458937  22.235649546  3.1113043070  0.4811902046  460           0.1324317932 
0.8110922504  0.9006024096  0.9036144578  0.8333333333  0.8019093079  0.9141566265  0.8493975904  0.9461077844  0.9166666667  0.9624060150  0.9461077844  0.9969879518  1.0000000000  0.7870090634  0.8132530120  0.9788519637  0.9638554217  0.9835082459  0.9461077844  0.8216867470  0.7874396135  22.719033232  3.0626965046  0.4811902046  470           0.1375183105 
0.7990870160  0.9186746988  0.9156626506  0.8454106280  0.7780429594  0.8855421687  0.8433734940  0.9535928144  0.9464285714  0.9699248120  0.9520958084  0.9969879518  1.0000000000  0.7552870091  0.7831325301  0.9864048338  0.9819277108  0.9700149925  0.9281437126  0.8216867470  0.7512077295  23.202416918  3.1274797201  0.4811902046  480           0.1345617771 
0.7966946302  0.9186746988  0.9156626506  0.8574879227  0.7708830549  0.8810240964  0.8373493976  0.9685628743  0.9583333333  0.9729323308  0.9520958084  0.9939759036  1.0000000000  0.7401812689  0.7710843373  0.9894259819  0.9939759036  0.9700149925  0.9161676647  0.8192771084  0.7391304348  23.685800604  3.1422964573  0.4811902046  490           0.1348889828 
0.7996707177  0.9186746988  0.9036144578  0.8429951691  0.7852028640  0.8825301205  0.8433734940  0.9535928144  0.9464285714  0.9699248120  0.9520958084  0.9969879518  1.0000000000  0.7552870091  0.7831325301  0.9864048338  0.9819277108  0.9700149925  0.9401197605  0.8192771084  0.7512077295  24.169184290  3.0528077364  0.4811902046  500           0.1347495794 
0.8134990482  0.9081325301  0.9096385542  0.8405797101  0.8042959427  0.9126506024  0.8554216867  0.9491017964  0.9166666667  0.9684210526  0.9461077844  0.9969879518  1.0000000000  0.7764350453  0.8072289157  0.9818731118  0.9759036145  0.9775112444  0.9461077844  0.8240963855  0.7850241546  24.652567975  3.0519456625  0.4811902046  510           0.1338267088 
0.7840480458  0.9337349398  0.9036144578  0.8550724638  0.7589498807  0.8554216867  0.7951807229  0.9760479042  0.9761904762  0.9759398496  0.9281437126  0.9894578313  0.9939759036  0.7039274924  0.7469879518  0.9894259819  0.9939759036  0.9595202399  0.9101796407  0.8216867470  0.7004830918  25.135951661  3.0057995081  0.4811902046  520           0.1324310064 
0.7984701943  0.9216867470  0.9036144578  0.8526570048  0.7828162291  0.8810240964  0.8373493976  0.9655688623  0.9583333333  0.9729323308  0.9520958084  0.9939759036  1.0000000000  0.7462235650  0.7710843373  0.9879154079  0.9879518072  0.9685157421  0.9101796407  0.8192771084  0.7391304348  25.619335347  3.0065500021  0.4811902046  530           0.1348402977 
0.8068493996  0.9051204819  0.8975903614  0.8285024155  0.8090692124  0.9081325301  0.8614457831  0.9520958084  0.9404761905  0.9684210526  0.9461077844  0.9969879518  1.0000000000  0.7870090634  0.8132530120  0.9848942598  0.9759036145  0.9835082459  0.9461077844  0.8120481928  0.7777777778  26.102719033  3.0403290749  0.4811902046  540           0.1335250139 
0.8134932973  0.9036144578  0.9036144578  0.8381642512  0.8066825776  0.9111445783  0.8493975904  0.9505988024  0.9345238095  0.9684210526  0.9461077844  0.9969879518  1.0000000000  0.7839879154  0.8132530120  0.9818731118  0.9759036145  0.9835082459  0.9461077844  0.8216867470  0.7874396135  26.586102719  3.1001124144  0.4811902046  550           0.1437109232 
0.8092260571  0.8810240964  0.8734939759  0.7946859903  0.8257756563  0.9397590361  0.8674698795  0.9266467066  0.8988095238  0.9563909774  0.9461077844  0.9834337349  0.9939759036  0.8459214502  0.8433734940  0.9607250755  0.9638554217  0.9970014993  0.9880239521  0.7927710843  0.8236714976  27.069486404  3.0053405523  0.4811902046  560           0.1348188162 
0.8128822266  0.9036144578  0.9036144578  0.8309178744  0.8090692124  0.9171686747  0.8493975904  0.9446107784  0.9226190476  0.9624060150  0.9461077844  0.9969879518  1.0000000000  0.7900302115  0.8132530120  0.9758308157  0.9638554217  0.9865067466  0.9461077844  0.8216867470  0.7898550725  27.552870090  3.0201165915  0.4811902046  570           0.1368227243 
0.7996635117  0.9156626506  0.9036144578  0.8429951691  0.7875894988  0.8900602410  0.8493975904  0.9535928144  0.9464285714  0.9729323308  0.9520958084  0.9969879518  1.0000000000  0.7567975831  0.7891566265  0.9864048338  0.9819277108  0.9760119940  0.9401197605  0.8192771084  0.7487922705  28.036253776  3.0808693886  0.4811902046  580           0.1388614655 
0.8050406461  0.9111445783  0.8975903614  0.8236714976  0.8066825776  0.9006024096  0.8554216867  0.9461077844  0.9166666667  0.9684210526  0.9461077844  0.9969879518  1.0000000000  0.7703927492  0.8072289157  0.9758308157  0.9638554217  0.9805097451  0.9461077844  0.8192771084  0.7705314010  28.519637462  2.9935198784  0.4811902046  590           0.1366853714 
0.8014606939  0.9111445783  0.8975903614  0.8357487923  0.7923627685  0.8990963855  0.8493975904  0.9505988024  0.9345238095  0.9699248120  0.9520958084  0.9969879518  1.0000000000  0.7598187311  0.7891566265  0.9833836858  0.9819277108  0.9775112444  0.9461077844  0.8192771084  0.7584541063  29.003021148  3.0257658243  0.4811902046  600           0.1338697195 
0.7954508706  0.9156626506  0.9036144578  0.8405797101  0.7828162291  0.8855421687  0.8433734940  0.9535928144  0.9464285714  0.9759398496  0.9520958084  0.9969879518  1.0000000000  0.7537764350  0.7771084337  0.9864048338  0.9819277108  0.9760119940  0.9401197605  0.8192771084  0.7391304348  29.486404833  3.0709197044  0.4811902046  610           0.1332119703 
0.8068034616  0.8674698795  0.8674698795  0.7922705314  0.8305489260  0.9397590361  0.8674698795  0.9221556886  0.8928571429  0.9488721805  0.9401197605  0.9819277108  0.9879518072  0.8459214502  0.8433734940  0.9561933535  0.9578313253  0.9970014993  0.9880239521  0.7807228916  0.8236714976  29.969788519  2.9745190859  0.4811902046  620           0.1349394560 
0.8074058713  0.8780120482  0.8734939759  0.7995169082  0.8305489260  0.9322289157  0.8734939759  0.9326347305  0.8988095238  0.9593984962  0.9461077844  0.9879518072  1.0000000000  0.8368580060  0.8313253012  0.9667673716  0.9638554217  0.9970014993  0.9880239521  0.7831325301  0.8164251208  30.453172205  2.9319166422  0.4811902046  630           0.1346323013 
0.7864145177  0.9186746988  0.9036144578  0.8429951691  0.7756563246  0.8780120482  0.8373493976  0.9610778443  0.9523809524  0.9759398496  0.9520958084  0.9969879518  1.0000000000  0.7401812689  0.7710843373  0.9879154079  0.9879518072  0.9685157421  0.9101796407  0.8192771084  0.7077294686  30.936555891  3.0203967810  0.4811902046  640           0.1335907936 
0.8044123226  0.8960843373  0.8855421687  0.8164251208  0.8162291169  0.9186746988  0.8674698795  0.9431137725  0.9166666667  0.9654135338  0.9461077844  0.9969879518  1.0000000000  0.7960725076  0.8132530120  0.9728096677  0.9638554217  0.9865067466  0.9461077844  0.8120481928  0.7729468599  31.419939577  3.0238435984  0.4811902046  650           0.1426544666 
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
Exception ignored in: <function WeakSet.__init__.<locals>._remove at 0x7f6d0fafa950>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/_weakrefset.py", line 38, in _remove
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
    def _remove(item, selfref=ref(self)):
KeyboardInterrupt: 
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1715519) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3724929638  0.4201807229  0.4638554217  0.4154589372  0.3532219570  0.4743975904  0.4518072289  0.4431137725  0.4285714286  0.4195488722  0.4610778443  0.4412650602  0.4397590361  0.3564954683  0.3855421687  0.4244712991  0.4277108434  0.3778110945  0.3353293413  0.3855421687  0.3357487923  0.0000000000  9.8018226624  0.2909131050  0             0.4707493782 
0.8050033693  0.8780120482  0.8614457831  0.8043478261  0.8257756563  0.9156626506  0.8674698795  0.9281437126  0.9047619048  0.9563909774  0.9461077844  0.9984939759  0.9939759036  0.7975830816  0.7951807229  0.9667673716  0.9518072289  0.9790104948  0.9520958084  0.7855421687  0.8043478261  2.4169184290  4.9947684479  0.6689839363  50            0.1347160864 
0.8032175501  0.9126506024  0.9036144578  0.8309178744  0.8114558473  0.8780120482  0.8373493976  0.9431137725  0.9166666667  0.9714285714  0.9341317365  0.9939759036  1.0000000000  0.7416918429  0.7771084337  0.9864048338  0.9819277108  0.9670164918  0.9401197605  0.8144578313  0.7560386473  4.8338368580  3.2003815413  0.6689839363  100           0.1332170391 
0.8225065076  0.9111445783  0.9096385542  0.8405797101  0.8186157518  0.8885542169  0.8313253012  0.9401197605  0.9166666667  0.9624060150  0.9461077844  0.9969879518  1.0000000000  0.7734138973  0.7951807229  0.9818731118  0.9638554217  0.9715142429  0.9461077844  0.8361445783  0.7946859903  7.2507552870  3.1945009899  0.6689839363  150           0.1330374622 
0.8002673070  0.9277108434  0.9156626506  0.8454106280  0.7852028640  0.8674698795  0.8192771084  0.9595808383  0.9464285714  0.9759398496  0.9401197605  0.9939759036  1.0000000000  0.7401812689  0.7710843373  0.9864048338  0.9819277108  0.9640179910  0.9281437126  0.8313253012  0.7391304348  9.6676737160  3.1909932947  0.6689839363  200           0.1325901508 
0.8104581065  0.9156626506  0.9156626506  0.8357487923  0.8114558473  0.8840361446  0.8373493976  0.9476047904  0.9226190476  0.9714285714  0.9461077844  0.9939759036  1.0000000000  0.7703927492  0.7951807229  0.9864048338  0.9819277108  0.9730134933  0.9401197605  0.8240963855  0.7705314010  12.084592145  3.1708941650  0.6689839363  250           0.1327226400 
0.8182766138  0.9051204819  0.9096385542  0.8260869565  0.8210023866  0.9156626506  0.8554216867  0.9371257485  0.9166666667  0.9593984962  0.9461077844  0.9969879518  1.0000000000  0.7854984894  0.8072289157  0.9728096677  0.9638554217  0.9820089955  0.9520958084  0.8289156627  0.7971014493  14.501510574  3.1236790943  0.6689839363  300           0.1330454350 
0.8188804785  0.9096385542  0.9036144578  0.8260869565  0.8210023866  0.9201807229  0.8614457831  0.9356287425  0.9107142857  0.9593984962  0.9461077844  0.9969879518  1.0000000000  0.7885196375  0.8072289157  0.9697885196  0.9638554217  0.9820089955  0.9520958084  0.8289156627  0.7995169082  16.918429003  3.1301868820  0.6689839363  350           0.1334583139 
0.8031802038  0.8765060241  0.8674698795  0.7898550725  0.8281622912  0.9352409639  0.8734939759  0.9221556886  0.8928571429  0.9518796992  0.9401197605  0.9864457831  0.9939759036  0.8368580060  0.8433734940  0.9577039275  0.9518072289  0.9910044978  0.9760479042  0.7927710843  0.8019323671  19.335347432  3.0670071554  0.6689839363  400           0.1332026529 
0.7948355040  0.9277108434  0.9036144578  0.8309178744  0.7875894988  0.8810240964  0.8373493976  0.9550898204  0.9285714286  0.9729323308  0.9401197605  0.9939759036  1.0000000000  0.7552870091  0.7831325301  0.9864048338  0.9819277108  0.9700149925  0.9401197605  0.8144578313  0.7463768116  21.752265861  3.0534135914  0.6689839363  450           0.1330893803 
0.8032477598  0.9216867470  0.9156626506  0.8454106280  0.7995226730  0.8810240964  0.8373493976  0.9505988024  0.9226190476  0.9729323308  0.9520958084  0.9939759036  1.0000000000  0.7567975831  0.7891566265  0.9864048338  0.9819277108  0.9760119940  0.9401197605  0.8240963855  0.7439613527  24.169184290  3.0835862780  0.6689839363  500           0.1331093788 
0.8134472204  0.8810240964  0.8855421687  0.8115942029  0.8233890215  0.9277108434  0.8795180723  0.9296407186  0.8988095238  0.9578947368  0.9401197605  0.9969879518  1.0000000000  0.8081570997  0.8253012048  0.9667673716  0.9638554217  0.9910044978  0.9640718563  0.8144578313  0.8043478261  26.586102719  3.0557932329  0.6689839363  550           0.1340270853 
0.7930224547  0.9246987952  0.9036144578  0.8405797101  0.7875894988  0.8810240964  0.8373493976  0.9520958084  0.9404761905  0.9759398496  0.9520958084  0.9939759036  1.0000000000  0.7341389728  0.7831325301  0.9864048338  0.9819277108  0.9700149925  0.9401197605  0.8168674699  0.7270531401  29.003021148  3.0037045431  0.6689839363  600           0.1332113695 
0.8128361497  0.9081325301  0.8975903614  0.8212560386  0.8257756563  0.9277108434  0.8795180723  0.9386227545  0.9107142857  0.9654135338  0.9461077844  0.9969879518  1.0000000000  0.8036253776  0.8192771084  0.9758308157  0.9638554217  0.9895052474  0.9580838323  0.8144578313  0.7898550725  31.419939577  3.0549537468  0.6689839363  650           0.1349319458 
0.8044497384  0.9246987952  0.9156626506  0.8526570048  0.8019093079  0.8915662651  0.8433734940  0.9520958084  0.9404761905  0.9789473684  0.9520958084  0.9939759036  1.0000000000  0.7567975831  0.7891566265  0.9864048338  0.9819277108  0.9790104948  0.9401197605  0.8216867470  0.7415458937  33.836858006  2.9828725338  0.6689839363  700           0.1331038523 
0.8086436716  0.9141566265  0.9096385542  0.8285024155  0.8138424821  0.9231927711  0.8734939759  0.9461077844  0.9166666667  0.9684210526  0.9461077844  0.9969879518  1.0000000000  0.7975830816  0.8192771084  0.9788519637  0.9638554217  0.9850074963  0.9520958084  0.8168674699  0.7753623188  36.253776435  3.0029727650  0.6689839363  750           0.1338764524 
0.8025734289  0.8704819277  0.8674698795  0.7874396135  0.8281622912  0.9397590361  0.8674698795  0.9221556886  0.8928571429  0.9488721805  0.9401197605  0.9939759036  1.0000000000  0.8398791541  0.8433734940  0.9577039275  0.9518072289  0.9970014993  0.9760479042  0.7975903614  0.7971014493  38.670694864  2.9203806114  0.6689839363  800           0.1327391863 
0.7936047014  0.9216867470  0.9036144578  0.8333333333  0.7947494033  0.8960843373  0.8493975904  0.9505988024  0.9345238095  0.9789473684  0.9520958084  0.9969879518  1.0000000000  0.7567975831  0.7891566265  0.9894259819  0.9819277108  0.9790104948  0.9401197605  0.8168674699  0.7294685990  41.087613293  2.9524952173  0.6689839363  850           0.1339847803 
0.7948296836  0.9277108434  0.9156626506  0.8454106280  0.7875894988  0.8960843373  0.8493975904  0.9520958084  0.9404761905  0.9849624060  0.9520958084  0.9939759036  1.0000000000  0.7507552870  0.7891566265  0.9879154079  0.9879518072  0.9760119940  0.9401197605  0.8240963855  0.7222222222  43.504531722  2.8965917826  0.6689839363  900           0.1346673775 
0.8001637208  0.8930722892  0.8734939759  0.7971014493  0.8257756563  0.9382530120  0.8855421687  0.9326347305  0.8988095238  0.9624060150  0.9461077844  0.9969879518  1.0000000000  0.8277945619  0.8313253012  0.9697885196  0.9638554217  0.9940029985  0.9760479042  0.8000000000  0.7777777778  45.921450151  2.9041394329  0.6689839363  950           0.1344450665 
0.8098369195  0.9081325301  0.8975903614  0.8260869565  0.8162291169  0.9277108434  0.8674698795  0.9386227545  0.9107142857  0.9654135338  0.9461077844  0.9969879518  1.0000000000  0.7975830816  0.8192771084  0.9712990937  0.9698795181  0.9865067466  0.9580838323  0.8289156627  0.7681159420  48.338368580  2.9108843327  0.6689839363  1000          0.1343432236 
0.8050262341  0.9141566265  0.8975903614  0.8381642512  0.8114558473  0.9201807229  0.8614457831  0.9461077844  0.9166666667  0.9729323308  0.9520958084  0.9969879518  1.0000000000  0.7854984894  0.8192771084  0.9894259819  0.9819277108  0.9835082459  0.9461077844  0.8192771084  0.7512077295  50.755287009  2.9944240570  0.6689839363  1050          0.1352364349 
0.8019839067  0.9096385542  0.8915662651  0.8164251208  0.8210023866  0.9352409639  0.8855421687  0.9401197605  0.9166666667  0.9654135338  0.9461077844  0.9969879518  1.0000000000  0.8111782477  0.8253012048  0.9773413897  0.9698795181  0.9895052474  0.9580838323  0.8096385542  0.7608695652  53.172205438  2.8392395639  0.6689839363  1100          0.1336363792 
0.7996130696  0.9141566265  0.8975903614  0.8429951691  0.8042959427  0.9216867470  0.8674698795  0.9491017964  0.9166666667  0.9759398496  0.9520958084  0.9969879518  1.0000000000  0.7809667674  0.8132530120  0.9894259819  0.9819277108  0.9835082459  0.9461077844  0.8192771084  0.7318840580  55.589123867  2.8777555227  0.6689839363  1150          0.1351010084 
0.7990019988  0.9141566265  0.8975903614  0.8381642512  0.8066825776  0.9231927711  0.8734939759  0.9491017964  0.9166666667  0.9729323308  0.9520958084  0.9969879518  1.0000000000  0.7915407855  0.8192771084  0.9894259819  0.9819277108  0.9865067466  0.9580838323  0.8192771084  0.7318840580  58.006042296  2.8627237415  0.6689839363  1200          0.1356319141 
0.7936132931  0.9307228916  0.9156626506  0.8502415459  0.7899761337  0.9051204819  0.8493975904  0.9520958084  0.9404761905  0.9789473684  0.9520958084  0.9969879518  1.0000000000  0.7522658610  0.7831325301  0.9909365559  0.9879518072  0.9805097451  0.9461077844  0.8265060241  0.7077294686  60.422960725  2.8667560959  0.6689839363  1250          0.1341438580 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1798906) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3664701836  0.4216867470  0.4578313253  0.4033816425  0.3484486874  0.4638554217  0.4819277108  0.4326347305  0.4345238095  0.4601503759  0.5029940120  0.4231927711  0.4518072289  0.3584337349  0.3614457831  0.3838080960  0.3832335329  0.3658170915  0.3473053892  0.3831325301  0.3309178744  0.0000000000  9.9154529572  0.2909131050  0             0.4749476910 
0.5612123879  0.8825301205  0.8554216867  0.5217391304  0.6300715990  0.9503012048  0.8614457831  0.9041916168  0.8809523810  0.9729323308  0.9520958084  0.8795180723  0.8674698795  0.8599397590  0.8855421687  0.8800599700  0.8443113772  0.9730134933  0.9520958084  0.4819277108  0.6111111111  2.4096385542  4.6347940302  0.4781041145  50            0.1337818956 
0.6207137533  0.9924698795  0.9819277108  0.5652173913  0.7159904535  0.8870481928  0.8253012048  0.9970059880  0.9880952381  0.9969924812  0.9760479042  0.9819277108  0.9879518072  0.6837349398  0.7590361446  0.9790104948  0.9640718563  0.9745127436  0.9580838323  0.5180722892  0.6835748792  4.8192771084  2.8217128992  0.4781579971  100           0.1336299181 
0.5708927926  0.9954819277  0.9698795181  0.5483091787  0.6181384248  0.9533132530  0.8855421687  0.9955089820  0.9702380952  0.9969924812  0.9760479042  0.9924698795  0.9819277108  0.8102409639  0.8072289157  0.9910044978  0.9640718563  0.9970014993  0.9760479042  0.5108433735  0.6062801932  7.2289156627  2.6128517056  0.4781579971  150           0.1341446829 
0.5865499701  1.0000000000  1.0000000000  0.5458937198  0.6300715990  0.9548192771  0.8795180723  0.9985029940  0.9940476190  1.0000000000  0.9880239521  0.9969879518  1.0000000000  0.7394578313  0.7891566265  0.9970014993  0.9880239521  0.9940029985  0.9640718563  0.5228915663  0.6473429952  9.6385542169  2.2118300056  0.4781579971  200           0.1336596966 
0.5848204829  1.0000000000  1.0000000000  0.5434782609  0.6014319809  0.9472891566  0.8493975904  0.9985029940  0.9940476190  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.7003012048  0.7530120482  0.9955022489  0.9820359281  0.9970014993  0.9760479042  0.5301204819  0.6642512077  12.048192771  2.1595002317  0.4781579971  250           0.1340424490 
0.5558466858  1.0000000000  0.9759036145  0.5217391304  0.6038186158  0.9819277108  0.8915662651  0.9985029940  0.9821428571  1.0000000000  0.9880239521  1.0000000000  0.9879518072  0.8554216867  0.8795180723  0.9970014993  0.9760479042  1.0000000000  0.9880239521  0.4987951807  0.5990338164  14.457831325  2.0874425149  0.4781579971  300           0.1329940557 
0.5392064818  1.0000000000  1.0000000000  0.5096618357  0.5131264916  0.9894578313  0.9096385542  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.8343373494  0.8433734940  0.9970014993  0.9880239521  1.0000000000  1.0000000000  0.5084337349  0.6256038647  16.867469879  1.9530795670  0.4782962799  350           0.1322457838 
0.5211986995  1.0000000000  0.9879518072  0.5024154589  0.4797136038  0.9984939759  0.9337349398  0.9985029940  0.9821428571  1.0000000000  0.9880239521  1.0000000000  0.9879518072  0.9743975904  0.9578313253  0.9985007496  0.9820359281  1.0000000000  0.9880239521  0.4963855422  0.6062801932  19.277108433  1.9309273505  0.4782962799  400           0.1342863226 
0.5427677262  1.0000000000  0.9879518072  0.5024154589  0.5346062053  0.9472891566  0.8855421687  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  0.9879518072  0.6897590361  0.7469879518  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.5036144578  0.6304347826  21.686746988  1.8702368593  0.4782962799  450           0.1340149307 
0.5492776196  1.0000000000  0.9879518072  0.5314009662  0.5775656325  0.9472891566  0.8734939759  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984939759  0.9939759036  0.7093373494  0.7530120482  0.9970014993  0.9880239521  0.9985007496  0.9940119760  0.5084337349  0.5797101449  24.096385542  1.8390177083  0.4782962799  500           0.1350288296 
0.5121164086  1.0000000000  0.9879518072  0.5000000000  0.4940334129  0.9984939759  0.9337349398  1.0000000000  0.9880952381  1.0000000000  0.9880239521  1.0000000000  0.9879518072  0.8915662651  0.8795180723  1.0000000000  0.9880239521  1.0000000000  0.9880239521  0.4650602410  0.5893719807  26.506024096  1.7775369120  0.4782962799  550           0.1360028839 
0.5300174862  1.0000000000  0.9879518072  0.4927536232  0.5608591885  0.9939759036  0.9156626506  1.0000000000  0.9880952381  1.0000000000  0.9880239521  1.0000000000  0.9879518072  0.8147590361  0.8253012048  1.0000000000  0.9880239521  1.0000000000  0.9880239521  0.4867469880  0.5797101449  28.915662650  1.7600670314  0.4782962799  600           0.1357356071 
0.5138443018  1.0000000000  0.9879518072  0.4782608696  0.5178997613  0.9954819277  0.9096385542  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  0.9879518072  0.8599397590  0.8614457831  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.4843373494  0.5748792271  31.325301204  1.7620376825  0.4782962799  650           0.1336916351 
0.5193165002  1.0000000000  0.9879518072  0.4661835749  0.5035799523  0.9984939759  0.9337349398  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  0.9879518072  0.9126506024  0.9156626506  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.4939759036  0.6135265700  33.734939759  1.7497246122  0.4782962799  700           0.1345896721 
0.5078000425  1.0000000000  0.9879518072  0.4806763285  0.5250596659  0.9939759036  0.9156626506  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  0.9879518072  0.8298192771  0.8614457831  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.4578313253  0.5676328502  36.144578313  1.7564819479  0.4782962799  750           0.1368053055 
0.5097325451  1.0000000000  0.9879518072  0.4613526570  0.4797136038  0.9969879518  0.9156626506  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  0.9879518072  0.9216867470  0.9156626506  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.4843373494  0.6135265700  38.554216867  1.7401888990  0.4782962799  800           0.1332628059 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 514, in __next__
    with torch.autograd.profiler.record_function(self._profile_name):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/profiler.py", line 613, in __init__
    self.handle: torch.Tensor = torch.zeros(1)
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3165009455  0.5180722892  0.5301204819  0.3429951691  0.3054892601  0.5286144578  0.5481927711  0.5059880240  0.5357142857  0.5398496241  0.5688622754  0.5779661017  0.5878378378  0.3792517007  0.4121621622  0.5935374150  0.5743243243  0.5143338954  0.5302013423  0.3469879518  0.2705314010  0.0000000000  9.7419633865  0.2909131050  0             0.4663555622 
0.8220382412  0.8614457831  0.8433734940  0.8164251208  0.7780429594  0.9427710843  0.8554216867  0.8982035928  0.8690476190  0.9082706767  0.9341317365  0.9915254237  0.9797297297  0.8469387755  0.7837837838  0.9591836735  0.9594594595  0.9848229342  0.9932885906  0.8144578313  0.8792270531  2.7210884354  4.9729341888  0.4781041145  50            0.1366132450 
0.7904400105  0.9608433735  0.9156626506  0.8260869565  0.8496420048  0.9066265060  0.8674698795  0.9775449102  0.9702380952  0.9909774436  0.9760479042  0.9949152542  0.9932432432  0.7500000000  0.6689189189  0.9914965986  0.9797297297  0.9898819562  0.9865771812  0.7831325301  0.7028985507  5.4421768707  2.9790149260  0.4781041145  100           0.1353020859 
0.7569376013  0.9879518072  0.9638554217  0.8574879227  0.7398568019  0.9728915663  0.8915662651  0.9730538922  0.9523809524  0.9909774436  0.9760479042  0.9915254237  0.9932432432  0.7125850340  0.6148648649  0.9965986395  0.9729729730  0.9949409781  0.9798657718  0.8120481928  0.6183574879  8.1632653061  2.4696236277  0.4781041145  150           0.1396666670 
0.7057407486  0.9969879518  0.9879518072  0.7705314010  0.7136038186  0.9262048193  0.8734939759  0.9970059880  0.9880952381  0.9984962406  0.9700598802  0.9949152542  0.9932432432  0.7380952381  0.6756756757  0.9982993197  0.9797297297  0.9915682968  0.9798657718  0.7253012048  0.6135265700  10.884353741  2.2805259633  0.4781293869  200           0.1393396187 
0.7196367046  0.9593373494  0.9216867470  0.7922705314  0.7064439141  0.9939759036  0.9277108434  0.9580838323  0.9047619048  0.9939849624  0.9760479042  1.0000000000  1.0000000000  0.8350340136  0.7229729730  0.9965986395  0.9864864865  1.0000000000  1.0000000000  0.7493975904  0.6304347826  13.605442176  2.1151702833  0.4781293869  250           0.1378117228 
0.6758183611  0.9984939759  0.9819277108  0.7560386473  0.6205250597  0.9713855422  0.8855421687  0.9985029940  0.9940476190  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.6530612245  0.5675675676  1.0000000000  0.9864864865  1.0000000000  0.9865771812  0.7421686747  0.5845410628  16.326530612  2.0159626460  0.4781293869  300           0.1367731953 
0.6848820136  0.9984939759  0.9939759036  0.7729468599  0.6157517900  0.9713855422  0.8975903614  0.9985029940  0.9821428571  1.0000000000  0.9880239521  0.9983050847  0.9932432432  0.6955782313  0.6013513514  1.0000000000  0.9864864865  1.0000000000  1.0000000000  0.7566265060  0.5942028986  19.047619047  1.8900452995  0.4781532288  350           0.1361162376 
0.6858724546  0.9487951807  0.8674698795  0.7318840580  0.6992840095  1.0000000000  0.9397590361  0.9700598802  0.9404761905  0.9924812030  0.9700598802  1.0000000000  1.0000000000  0.9353741497  0.8378378378  1.0000000000  0.9864864865  1.0000000000  1.0000000000  0.6987951807  0.6135265700  21.768707483  1.7870537591  0.4781928062  400           0.1369876289 
0.7008666641  0.9894578313  0.9698795181  0.7367149758  0.7303102625  1.0000000000  0.9397590361  0.9955089820  0.9702380952  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.7976190476  0.7094594595  1.0000000000  0.9864864865  1.0000000000  1.0000000000  0.7132530120  0.6231884058  24.489795918  1.8218206787  0.4781928062  450           0.1358798981 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1888323) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.1638084684  0.3424048875  0.3385127636  0.1577142857  0.1734463277  0.3391666667  0.3344444444  0.3469726293  0.3480662983  0.3387902331  0.3425720621  0.2494536372  0.2484394507  0.2500000000  0.2500000000  0.2509375000  0.2487500000  0.2518610422  0.2555831266  0.1743589744  0.1497142857  0.0000000000  9.9078559875  0.2909131050  0             0.4853847027 
0.5418268800  0.7150791447  0.7003329634  0.4685714286  0.5774011299  0.7163888889  0.6766666667  0.7652750899  0.7558011050  0.7669256382  0.7516629712  0.8570090540  0.8651685393  0.6781250000  0.6525000000  0.8496875000  0.8562500000  0.8464640199  0.8449131514  0.5327635328  0.5885714286  0.5000000000  7.2653834343  0.4781041145  50            0.1374835300 
0.6332433518  0.8072757567  0.7824639290  0.5657142857  0.7485875706  0.6336111111  0.6122222222  0.8537461985  0.8220994475  0.8673695893  0.8492239468  0.9787699032  0.9800249688  0.5728125000  0.5637500000  0.9390625000  0.9462500000  0.9103598015  0.9218362283  0.6649572650  0.5537142857  1.0000000000  5.3977219582  0.4781446457  100           0.1345619917 
0.6762004236  0.8336573174  0.8057713651  0.6137142857  0.7740112994  0.7277777778  0.6933333333  0.8805640033  0.8696132597  0.9508879023  0.9323725055  0.8788635654  0.8801498127  0.6996875000  0.6912500000  0.8475000000  0.8700000000  0.9277295285  0.9267990074  0.6233618234  0.6937142857  1.5000000000  4.8541724873  0.4781446457  150           0.1343447733 
0.6685210709  0.8786448209  0.8546059933  0.6005714286  0.8045197740  0.6919444444  0.6722222222  0.9109759469  0.9005524862  0.9644839068  0.9467849224  0.9313143928  0.9375780275  0.6081250000  0.6025000000  0.8856250000  0.9075000000  0.9472704715  0.9379652605  0.6524216524  0.6165714286  2.0000000000  4.3211725235  0.4781446457  200           0.1355813169 
0.6319546436  0.7134129408  0.6892341842  0.5428571429  0.7259887006  0.7830555556  0.7455555556  0.7829693116  0.7558011050  0.8779134295  0.8603104213  0.9094598814  0.9101123596  0.8084375000  0.8212500000  0.8718750000  0.8825000000  0.9416873449  0.9528535980  0.5595441595  0.6994285714  2.5000000000  4.0389665222  0.4781446457  250           0.1355157185 
0.6662170347  0.8925298528  0.8745837958  0.6097142857  0.7542372881  0.6844444444  0.6688888889  0.8943876140  0.8740331492  0.9425638180  0.9301552106  0.9871995005  0.9887640449  0.6568750000  0.6575000000  0.9709375000  0.9737500000  0.9699131514  0.9615384615  0.6792022792  0.6217142857  3.0000000000  3.8391614962  0.4781446457  300           0.1336711836 
0.6486117561  0.8755901139  0.8601553829  0.6080000000  0.7180790960  0.7288888889  0.6933333333  0.8672933370  0.8541436464  0.9286903441  0.9235033259  0.9800187324  0.9800249688  0.7428125000  0.7412500000  0.9618750000  0.9675000000  0.9792183623  0.9714640199  0.6712250712  0.5971428571  3.5000000000  3.5587732172  0.4781446457  350           0.1346279621 
0.6450067534  0.9422382671  0.9378468368  0.5960000000  0.7932203390  0.7213888889  0.7011111111  0.9491291125  0.9381215470  0.9794672586  0.9756097561  0.9044645645  0.9026217228  0.6500000000  0.6450000000  0.8775000000  0.8925000000  0.9615384615  0.9528535980  0.6176638177  0.5731428571  4.0000000000  3.4308411837  0.4781446457  400           0.1357270384 
0.5850022695  0.9358511525  0.9211986681  0.5754285714  0.6429378531  0.7155555556  0.6866666667  0.8924523085  0.8751381215  0.9450610433  0.9379157428  0.9569153918  0.9575530587  0.6540625000  0.6362500000  0.9440625000  0.9462500000  0.9742555831  0.9615384615  0.6250712251  0.4965714286  4.5000000000  3.2303402996  0.4781775475  450           0.1336232948 
0.5816700468  0.9194668148  0.8956714761  0.5525714286  0.6050847458  0.6711111111  0.6555555556  0.8852640310  0.8596685083  0.9162042175  0.9024390244  0.9859506712  0.9812734082  0.6390625000  0.6187500000  0.9696875000  0.9712500000  0.9816997519  0.9789081886  0.6415954416  0.5274285714  5.0000000000  3.1076268244  0.4782409668  500           0.1339878559 
0.5547365742  0.9633435157  0.9511653718  0.5628571429  0.5960451977  0.6941666667  0.6700000000  0.9574232790  0.9403314917  0.9739178690  0.9623059867  0.9085232594  0.9113607990  0.6762500000  0.6625000000  0.9037500000  0.9050000000  0.9692928040  0.9540942928  0.5846153846  0.4754285714  5.5000000000  2.9931649542  0.4782409668  550           0.1364180136 
0.5658935177  0.9164121077  0.8967813541  0.4948571429  0.7062146893  0.8383333333  0.7933333333  0.8899640586  0.8607734807  0.9386792453  0.9323725055  0.9247580393  0.9088639201  0.7546875000  0.7337500000  0.8840625000  0.8937500000  0.9801488834  0.9702233251  0.5242165242  0.5382857143  6.0000000000  2.9760044861  0.4782409668  600           0.1351712799 
0.5849733518  0.9094695918  0.8890122087  0.5491428571  0.6587570621  0.7066666667  0.6888888889  0.8824993088  0.8552486188  0.9303551609  0.9124168514  0.9762722448  0.9800249688  0.6178125000  0.6012500000  0.9687500000  0.9700000000  0.9841811414  0.9689826303  0.6022792023  0.5297142857  6.5000000000  2.9256645918  0.4782409668  650           0.1340568495 
0.5671989956  0.9425159678  0.9322974473  0.5565714286  0.6322033898  0.7444444444  0.7200000000  0.9225877799  0.8917127072  0.9528301887  0.9512195122  0.9313143928  0.9325842697  0.7034375000  0.6862500000  0.9362500000  0.9275000000  0.9770471464  0.9702233251  0.5925925926  0.4874285714  7.0000000000  2.7833385277  0.4782409668  700           0.1332252026 
0.5700425557  0.9372396557  0.9334073252  0.5777142857  0.6926553672  0.7241666667  0.7077777778  0.9361349184  0.9060773481  0.9561598224  0.9490022173  0.9484857946  0.9413233458  0.6184375000  0.6037500000  0.9303125000  0.9362500000  0.9863523573  0.9727047146  0.5698005698  0.4400000000  7.5000000000  2.6235555649  0.4782409668  750           0.1348567867 
0.5633150183  0.9727853374  0.9622641509  0.5560000000  0.7000000000  0.7077777778  0.6822222222  0.9507879458  0.9248618785  0.9714206437  0.9634146341  0.9206993444  0.9275905119  0.6446875000  0.6312500000  0.9215625000  0.9337500000  0.9807692308  0.9702233251  0.5589743590  0.4382857143  8.0000000000  2.7374939299  0.4782409668  800           0.1343429422 
0.5594457206  0.9725076368  0.9589345172  0.5640000000  0.7022598870  0.6916666667  0.6844444444  0.9513408902  0.9248618785  0.9647613762  0.9545454545  0.9391195754  0.9313358302  0.5346875000  0.5287500000  0.9393750000  0.9325000000  0.9696029777  0.9553349876  0.5669515670  0.4045714286  8.5000000000  2.5978856611  0.4782409668  850           0.1350522280 
0.5328413802  0.8555956679  0.8423973363  0.4525714286  0.6378531073  0.9016666667  0.8511111111  0.8451755599  0.8187845304  0.8776359600  0.8725055432  0.9175772713  0.9200998752  0.8859375000  0.8487500000  0.8962500000  0.9000000000  0.9665012407  0.9553349876  0.4706552707  0.5702857143  9.0000000000  2.5290259409  0.4782409668  900           0.1348211956 
0.5037746184  0.9858372674  0.9744728080  0.4994285714  0.6039548023  0.7827777778  0.7622222222  0.9687586398  0.9524861878  0.9844617092  0.9756097561  0.9075866375  0.9026217228  0.6134375000  0.5837500000  0.8793750000  0.8975000000  0.9677419355  0.9578163772  0.4997150997  0.4120000000  9.5000000000  2.4698375773  0.4782409668  950           0.1355194473 
0.5271536392  0.9352957512  0.9256381798  0.4548571429  0.6248587571  0.8813888889  0.8300000000  0.9214818911  0.8983425414  0.9517203108  0.9401330377  0.8988448330  0.8851435705  0.7681250000  0.7600000000  0.8512500000  0.8625000000  0.9609181141  0.9528535980  0.4854700855  0.5434285714  10.000000000  2.3875948071  0.4782409668  1000          0.1335192919 
0.5107397888  0.8972507637  0.8779134295  0.4891428571  0.5610169492  0.8272222222  0.7844444444  0.8664639204  0.8331491713  0.8998335183  0.8946784922  0.9734623790  0.9787765293  0.6971875000  0.6662500000  0.9700000000  0.9625000000  0.9869727047  0.9702233251  0.5202279202  0.4725714286  10.500000000  2.4648461294  0.4782409668  1050          0.1359644556 
0.5326902106  0.9872257706  0.9800221976  0.5137142857  0.6807909605  0.7513888889  0.7411111111  0.9646115565  0.9469613260  0.9891786903  0.9789356984  0.9094598814  0.9176029963  0.5665625000  0.5512500000  0.8965625000  0.9037500000  0.9537841191  0.9317617866  0.5105413105  0.4257142857  11.000000000  2.3816367936  0.4782409668  1100          0.1350775433 
0.5319532639  0.9136351014  0.8990011099  0.5091428571  0.6361581921  0.8250000000  0.7800000000  0.8918993641  0.8662983425  0.9039955605  0.8913525499  0.9606618795  0.9625468165  0.7706250000  0.7450000000  0.9593750000  0.9550000000  0.9931761787  0.9776674938  0.5207977208  0.4617142857  11.500000000  2.2659255147  0.4782409668  1150          0.1341898537 
0.5351175610  0.9211330186  0.9112097669  0.5554285714  0.6141242938  0.7647222222  0.7233333333  0.8910699475  0.8629834254  0.8967813541  0.8847006652  0.9481735873  0.9450686642  0.6431250000  0.6225000000  0.9581250000  0.9500000000  0.9860421836  0.9689826303  0.5789173789  0.3920000000  12.000000000  2.3255625153  0.4782409668  1200          0.1354818583 
0.5603328022  0.9233546237  0.9023307436  0.5354285714  0.6949152542  0.7883333333  0.7466666667  0.8954935029  0.8607734807  0.9228634850  0.9068736142  0.9672182329  0.9612983770  0.7068750000  0.6750000000  0.9718750000  0.9650000000  0.9962779156  0.9875930521  0.5544159544  0.4565714286  12.500000000  2.3129370809  0.4782409668  1250          0.1359845829 
0.5284721274  0.9489030825  0.9267480577  0.5165714286  0.6502824859  0.8005555556  0.7511111111  0.9482996959  0.9193370166  0.9531076582  0.9368070953  0.9525444895  0.9500624220  0.6437500000  0.6175000000  0.9515625000  0.9437500000  0.9888337469  0.9727047146  0.5378917379  0.4091428571  13.000000000  2.1659782171  0.4782409668  1300          0.1344957399 
0.4745965522  0.9902804776  0.9855715871  0.4411428571  0.6288135593  0.8002777778  0.7744444444  0.9646115565  0.9491712707  0.9847391787  0.9700665188  0.9025913206  0.9076154806  0.5709375000  0.5562500000  0.8846875000  0.8937500000  0.9602977667  0.9478908189  0.4495726496  0.3788571429  13.500000000  2.1766482997  0.4782409668  1350          0.1362526798 
0.5303791381  0.9341849486  0.9278579356  0.4674285714  0.6423728814  0.8763888889  0.8255555556  0.9336466685  0.9049723757  0.9317425083  0.9268292683  0.9138307836  0.9176029963  0.7493750000  0.7225000000  0.8871875000  0.8937500000  0.9807692308  0.9702233251  0.4997150997  0.5120000000  14.000000000  2.1850815320  0.4782409668  1400          0.1355328226 
0.5183734539  0.8861427381  0.8756936737  0.4348571429  0.6096045198  0.9336111111  0.8633333333  0.8523638374  0.8386740331  0.8926193119  0.8858093126  0.9010302841  0.8963795256  0.8234375000  0.7937500000  0.8637500000  0.8600000000  0.9754962779  0.9615384615  0.4387464387  0.5902857143  14.500000000  2.1467888880  0.4782409668  1450          0.1363794756 
0.5382232294  0.8669813941  0.8523862375  0.5022857143  0.6412429379  0.8280555556  0.7788888889  0.8706110036  0.8298342541  0.8706992231  0.8647450111  0.9772088667  0.9737827715  0.7634375000  0.7362500000  0.9762500000  0.9625000000  0.9972084367  0.9888337469  0.5225071225  0.4868571429  15.000000000  2.1938038850  0.4782409668  1500          0.1350962305 
0.5837371191  0.8822549292  0.8801331853  0.5337142857  0.7016949153  0.8044444444  0.7533333333  0.8614874205  0.8220994475  0.8951165372  0.8935698448  0.9853262566  0.9837702871  0.7034375000  0.6712500000  0.9893750000  0.9775000000  0.9956575682  0.9900744417  0.5612535613  0.5382857143  15.500000000  2.1283386326  0.4782409668  1550          0.1342267847 
0.5345015487  0.9577895029  0.9556048835  0.4262857143  0.7050847458  0.7505555556  0.7333333333  0.9256289743  0.8972375691  0.9716981132  0.9600886918  0.9119575398  0.9126092385  0.5087500000  0.4900000000  0.8878125000  0.9037500000  0.9274193548  0.9181141439  0.4774928775  0.5291428571  16.000000000  2.3620660973  0.4782409668  1600          0.1328492975 
0.4583944938  0.8891974452  0.8834628191  0.3720000000  0.5045197740  0.9327777778  0.8844444444  0.8678462814  0.8386740331  0.8962264151  0.8980044346  0.8688729316  0.8601747815  0.7812500000  0.7675000000  0.8293750000  0.8350000000  0.9376550868  0.9292803970  0.4296296296  0.5274285714  16.500000000  2.5040370131  0.4783563614  1650          0.1365266418 
0.4912846450  0.9389058595  0.9311875694  0.4182857143  0.6395480226  0.7525000000  0.7366666667  0.9173348079  0.8972375691  0.9506104329  0.9445676275  0.9659694037  0.9662921348  0.6362500000  0.6050000000  0.9784375000  0.9737500000  0.9916253102  0.9813895782  0.4433048433  0.4640000000  17.000000000  2.2565982223  0.4783563614  1700          0.1347653484 
0.5537676442  0.7811718967  0.7647058824  0.5154285714  0.6350282486  0.7752777778  0.7255555556  0.7898811169  0.7613259669  0.7985571587  0.7937915743  0.9984389635  0.9987515605  0.8103125000  0.7837500000  0.9987500000  0.9950000000  0.9978287841  0.9888337469  0.5851851852  0.4794285714  17.500000000  2.1452025604  0.4783563614  1750          0.1367806292 
0.5319663362  0.8653151902  0.8435072142  0.5080000000  0.6214689266  0.7580555556  0.7188888889  0.8556815040  0.8408839779  0.8837402886  0.8835920177  0.9921948174  0.9837702871  0.7246875000  0.7112500000  0.9956250000  0.9875000000  0.9962779156  0.9925558313  0.5612535613  0.4371428571  18.000000000  2.0865284109  0.4785270691  1800          0.1360169554 
0.4905302112  0.8505970564  0.8312985572  0.4474285714  0.5954802260  0.7888888889  0.7377777778  0.8429637821  0.8276243094  0.8598779134  0.8547671840  0.9928192320  0.9862671660  0.7053125000  0.6737500000  0.9912500000  0.9800000000  0.9968982630  0.9925558313  0.4757834758  0.4434285714  18.500000000  2.1070244622  0.4785270691  1850          0.1347804260 
0.5120231783  0.9300194390  0.9112097669  0.4765714286  0.6158192090  0.7477777778  0.7244444444  0.9220348355  0.9027624309  0.9253607103  0.9124168514  0.9581642210  0.9550561798  0.6325000000  0.6100000000  0.9581250000  0.9525000000  0.9947270471  0.9863523573  0.5042735043  0.4514285714  19.000000000  2.1323111367  0.4785270691  1900          0.1337061882 
0.4424690852  0.7525687309  0.7413984462  0.3268571429  0.5423728814  0.8861111111  0.8222222222  0.7461985071  0.7237569061  0.7624861265  0.7827050998  0.9862628786  0.9825218477  0.9484375000  0.9112500000  0.9871875000  0.9787500000  0.9925558313  0.9851116625  0.3737891738  0.5268571429  19.500000000  2.0253733325  0.4785270691  1950          0.1377639484 
0.5208249415  0.9544570953  0.9533851276  0.4434285714  0.6480225989  0.9133333333  0.8488888889  0.9438761404  0.9259668508  0.9581021088  0.9545454545  0.9035279426  0.8938826467  0.7059375000  0.6862500000  0.8615625000  0.8637500000  0.9525434243  0.9416873449  0.4529914530  0.5388571429  20.000000000  2.0405263329  0.4785270691  2000          0.1480646420 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2313, in update
    temp_new_featurizer = networks.CNN(pretrained=False, in_channel=self.input_shape[0], out_channel=self.num_classes).cuda()
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/networks.py", line 276, in __init__
    nn.ReLU(inplace=True))
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/container.py", line 69, in __init__
    self.add_module(str(idx), module)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 361, in add_module
    def add_module(self, name: str, module: Optional['Module']) -> None:
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3351394126  0.3396278811  0.3385127636  0.3374613003  0.3345070423  0.3394444444  0.3400000000  0.3444843793  0.3425414365  0.3371254162  0.3381374723  0.3330557868  0.3329633740  0.3338888889  0.3333333333  0.3364666851  0.3370165746  0.3323204420  0.3322295806  0.3348115299  0.3337777778  0.0000000000  10.136302948  0.2909131050  0             0.4767372608 
0.6616702391  0.7536795335  0.7658157603  0.5900044228  0.7367957746  0.6569444444  0.6766666667  0.7719104230  0.7580110497  0.9045504994  0.9002217295  0.7843463780  0.7991120977  0.5766666667  0.5600000000  0.7340337296  0.7392265193  0.8011049724  0.8035320088  0.6536585366  0.6662222222  0.4444444444  6.2705475426  0.4781041145  50            0.1359748220 
0.6877593673  0.9169675090  0.9056603774  0.6253869969  0.7676056338  0.5800000000  0.5911111111  0.9134641968  0.9060773481  0.9586570477  0.9567627494  0.8787121843  0.8745837958  0.5525000000  0.5322222222  0.8567873929  0.8497237569  0.8593922652  0.8399558499  0.6798226164  0.6782222222  0.8888888889  4.8226269531  0.4781141281  100           0.1504125118 
0.6800193741  0.9255762288  0.9267480577  0.5953118089  0.7698063380  0.6447222222  0.6500000000  0.9314348908  0.9226519337  0.9758601554  0.9700665188  0.8931446017  0.8923418424  0.6194444444  0.6044444444  0.8512579486  0.8563535912  0.9165745856  0.9028697572  0.6682926829  0.6866666667  1.3333333333  4.2204544687  0.4781141281  150           0.1344595194 
0.6628688362  0.8808664260  0.8679245283  0.5400265369  0.7737676056  0.6683333333  0.6577777778  0.8869228643  0.8707182320  0.9586570477  0.9478935698  0.9233971690  0.9156492786  0.6030555556  0.5833333333  0.8918993641  0.8883977901  0.9400552486  0.9260485651  0.6434589800  0.6942222222  1.7777777778  4.0825382710  0.4781775475  200           0.1352310085 
0.6950493852  0.8872535407  0.8890122087  0.5961963733  0.8019366197  0.7122222222  0.7066666667  0.9054465026  0.8850828729  0.9741953385  0.9656319290  0.9300582848  0.9245283019  0.6169444444  0.5855555556  0.8891346420  0.8883977901  0.9593922652  0.9481236203  0.6709534368  0.7111111111  2.2222222222  3.8240292025  0.4801650047  250           0.1353887844 
0.6440629421  0.9178006109  0.9089900111  0.5214506855  0.7513204225  0.7858333333  0.7833333333  0.8841581421  0.8751381215  0.9628190899  0.9423503326  0.9100749376  0.9045504994  0.6475000000  0.6211111111  0.8548520874  0.8530386740  0.9466850829  0.9216335541  0.6337028825  0.6697777778  2.6666666667  3.6680470181  0.4801650047  300           0.1331168556 
0.6945938477  0.9472368786  0.9422863485  0.6068111455  0.8023767606  0.6872222222  0.6822222222  0.9593585845  0.9348066298  0.9900110988  0.9800443459  0.9367194005  0.9356270810  0.6088888889  0.5844444444  0.9187171689  0.9049723757  0.9444751381  0.9282560706  0.6656319290  0.7035555556  3.1111111111  3.5811460829  0.4801650047  350           0.1380574799 
0.6802271261  0.9355734518  0.9334073252  0.6006191950  0.7750880282  0.7622222222  0.7355555556  0.9510644180  0.9259668508  0.9897336293  0.9855875831  0.9134054954  0.9089900111  0.6944444444  0.6666666667  0.9026817805  0.8895027624  0.9546961326  0.9403973510  0.6594235033  0.6857777778  3.5555555556  3.4924465036  0.4801650047  400           0.1390169954 
0.7144851601  0.9464037767  0.9411764706  0.6536930562  0.8186619718  0.6783333333  0.6822222222  0.9651645010  0.9403314917  0.9847391787  0.9789356984  0.9136830419  0.9100998890  0.6038888889  0.5955555556  0.9040641416  0.8883977901  0.9323204420  0.9216335541  0.6864745011  0.6991111111  4.0000000000  3.4584569263  0.4801650047  450           0.1356605816 
0.6858838701  0.9483476812  0.9445061043  0.6368863335  0.7623239437  0.7594444444  0.7488888889  0.9612938900  0.9381215470  0.9900110988  0.9844789357  0.9286705523  0.9189789123  0.6433333333  0.6200000000  0.9073818081  0.8972375691  0.9674033149  0.9536423841  0.6536585366  0.6906666667  4.4444444444  3.2306122017  0.4801650047  500           0.1355136871 
0.6528989153  0.8664259928  0.8501664817  0.5515258735  0.7676056338  0.8633333333  0.8311111111  0.8686756981  0.8508287293  0.9597669256  0.9478935698  0.8973077991  0.8978912320  0.8183333333  0.7733333333  0.8587226984  0.8508287293  0.9508287293  0.9271523179  0.5911308204  0.7013333333  4.8888888889  3.2151052523  0.4801650047  550           0.1378062057 
0.6992172846  0.9594557067  0.9467258602  0.6421937196  0.8058978873  0.7155555556  0.7066666667  0.9693115842  0.9414364641  0.9886237514  0.9833702882  0.9569802942  0.9500554939  0.5980555556  0.5744444444  0.9488526403  0.9215469613  0.9668508287  0.9536423841  0.6501108647  0.6986666667  5.3333333333  3.1069515181  0.4801650047  600           0.1367144489 
0.6936635286  0.9319633435  0.9123196448  0.6134453782  0.8230633803  0.8263888889  0.7922222222  0.9341996129  0.9049723757  0.9841842397  0.9745011086  0.9367194005  0.9245283019  0.7755555556  0.7422222222  0.9176112801  0.8872928177  0.9729281768  0.9514348786  0.6345898004  0.7035555556  5.7777777778  3.1091794252  0.4801650047  650           0.1352495146 
0.6595545701  0.9366842544  0.9200887902  0.6130030960  0.7768485915  0.8002777778  0.7677777778  0.9455349737  0.9060773481  0.9872364040  0.9822616408  0.9650291424  0.9556048835  0.6958333333  0.6722222222  0.9369643351  0.9071823204  0.9803867403  0.9635761589  0.6350332594  0.6133333333  6.2222222222  2.9589452314  0.4801650047  700           0.1448257113 
0.6845564186  0.9641766176  0.9478357381  0.6382131800  0.8085387324  0.7791666667  0.7677777778  0.9632291955  0.9370165746  0.9922308546  0.9844789357  0.9669719678  0.9611542730  0.6402777778  0.6122222222  0.9546585568  0.9270718232  0.9745856354  0.9514348786  0.6368070953  0.6546666667  6.6666666667  2.9805767965  0.4801650047  750           0.1369507217 
0.7052579428  0.9539016940  0.9378468368  0.6585581601  0.8248239437  0.8258333333  0.7877777778  0.9706939453  0.9359116022  0.9950055494  0.9889135255  0.9636414099  0.9500554939  0.7647222222  0.7388888889  0.9563173901  0.9248618785  0.9779005525  0.9602649007  0.6576496674  0.6800000000  7.1111111111  2.9260672951  0.4801650047  800           0.1402804947 
0.6935428699  0.9858372674  0.9744728080  0.6532507740  0.8006161972  0.7850000000  0.7511111111  0.9831351949  0.9546961326  0.9941731410  0.9878048780  0.9600333056  0.9467258602  0.6627777778  0.6400000000  0.9557644457  0.9204419890  0.9773480663  0.9602649007  0.6629711752  0.6573333333  7.5555555556  2.8139370632  0.4801650047  850           0.1380075788 
0.6896877508  0.9836156623  0.9678135405  0.6523662096  0.7830105634  0.7244444444  0.7044444444  0.9828587227  0.9469613260  0.9941731410  0.9922394678  0.9569802942  0.9456159822  0.6041666667  0.5811111111  0.9554879735  0.9193370166  0.9500000000  0.9326710817  0.6815964523  0.6417777778  8.0000000000  2.8965651131  0.4801650047  900           0.1354528809 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2018238) is killed by signal: Terminated. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 124, in join
    res = self._popen.wait(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 50, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2017359) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3327088050  0.3332407665  0.3329633740  0.3333333333  0.3315743184  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3342572062  0.3348115299  0.3330557868  0.3329633740  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3327814570  0.3333333333  0.3325942350  0.3333333333  0.0000000000  9.8745346069  0.2909131050  0             0.5231423378 
0.6181008820  0.8975284643  0.8901220866  0.5631111111  0.6675461741  0.6663888889  0.6366666667  0.9158333333  0.9055555556  0.9074279379  0.9002217295  0.9278379129  0.9178690344  0.6008333333  0.6055555556  0.9055555556  0.8911111111  0.8587196468  0.8719646799  0.6141906874  0.6275555556  0.4444444444  6.9042739677  0.6689839363  50            0.1395066214 
0.7198105224  0.9136351014  0.9145394007  0.6777777778  0.7928759894  0.7122222222  0.6955555556  0.9547222222  0.9455555556  0.9631374723  0.9656319290  0.9839023036  0.9778024417  0.6852777778  0.6744444444  0.9677777778  0.9644444444  0.9288079470  0.9359823400  0.7352549889  0.6733333333  0.8888888889  4.5843635750  0.6689839363  100           0.1420484018 
0.7190603493  0.9166898084  0.9178690344  0.7000000000  0.8240985048  0.6797222222  0.6766666667  0.9541666667  0.9411111111  0.9634146341  0.9578713969  0.9922286983  0.9889012209  0.6355555556  0.6288888889  0.9744444444  0.9755555556  0.9459161148  0.9470198675  0.7356984479  0.6164444444  1.3333333333  4.0488276339  0.6689839363  150           0.1366813326 
0.7290284998  0.9514023882  0.9478357381  0.6880000000  0.8364116095  0.7738888889  0.7488888889  0.9750000000  0.9711111111  0.9722838137  0.9778270510  0.9905634194  0.9866814650  0.6663888889  0.6655555556  0.9791666667  0.9766666667  0.9597130243  0.9646799117  0.7339246120  0.6577777778  1.7777777778  3.6057795048  0.6689839363  200           0.1559059191 
0.7107478290  0.9094695918  0.8956714761  0.7031111111  0.8249780123  0.8200000000  0.7977777778  0.9497222222  0.9411111111  0.9769955654  0.9767184035  0.9927837913  0.9889012209  0.7702777778  0.7411111111  0.9783333333  0.9777777778  0.9793046358  0.9768211921  0.6940133038  0.6208888889  2.2222222222  3.3151553631  0.6689839363  250           0.1474882507 
0.6889124302  0.9661205221  0.9689234184  0.6813333333  0.8087071240  0.8344444444  0.8088888889  0.9858333333  0.9788888889  0.9858647450  0.9855875831  0.9950041632  0.9955604883  0.7255555556  0.7022222222  0.9741666667  0.9766666667  0.9770971302  0.9768211921  0.6758314856  0.5897777778  2.6666666667  3.1623691797  0.6689839363  300           0.1394231462 
0.7064293563  0.9688975285  0.9689234184  0.6742222222  0.8218997361  0.8544444444  0.8266666667  0.9852777778  0.9833333333  0.9850332594  0.9866962306  0.9963918956  0.9966703663  0.7488888889  0.7311111111  0.9847222222  0.9811111111  0.9848233996  0.9834437086  0.6820399113  0.6475555556  3.1111111111  2.9539347601  0.6689839363  350           0.1540284491 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2109619) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.4503667230  0.4568175507  0.4605993341  0.4465631929  0.4088105727  0.4119444444  0.4166666667  0.4693821003  0.4629014396  0.3993353642  0.4097452935  0.3538717735  0.3362930078  0.3358333333  0.3322222222  0.3651981158  0.3543743079  0.3462917011  0.3307607497  0.4580931264  0.4880000000  0.0000000000  10.020523071  0.2909131050  0             0.4684443474 
0.5737516239  0.8125520689  0.8146503885  0.4953436807  0.5973568282  0.6780555556  0.6677777778  0.8877805486  0.8726467331  0.9399058433  0.9368770764  0.8995281710  0.9089900111  0.6177777778  0.6155555556  0.9091160986  0.9025470653  0.9382409705  0.9338478501  0.5623059867  0.6400000000  0.4444444444  6.4348829079  0.4781041145  50            0.1387446499 
0.5858290675  0.9103026937  0.9034406215  0.5170731707  0.6030837004  0.6663888889  0.6477777778  0.9476309227  0.9346622370  0.9556909443  0.9512735327  0.9794615598  0.9822419534  0.6150000000  0.5933333333  0.9612080909  0.9623477298  0.9459608492  0.9448732084  0.5782705100  0.6448888889  0.8888888889  4.5360130501  0.4781041145  100           0.1340046215 
0.6161395376  0.8991946681  0.8923418424  0.5441241685  0.6563876652  0.6905555556  0.6777777778  0.9420892214  0.9302325581  0.9750761562  0.9645625692  0.9802941993  0.9833518313  0.6491666667  0.6277777778  0.9675810474  0.9678848283  0.9779432037  0.9669239250  0.5791574279  0.6848888889  1.3333333333  3.9428883648  0.4781041145  150           0.1345140982 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2164, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1476, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1483, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 104, in start
    _cleanup()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 55, in _cleanup
    if p._popen.poll() is not None:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 267, in in_project_roots
    return filename_to_in_scope_cache[filename]KeyError: '/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 557, in get_abs_path_real_path_and_base_from_file
KeyError: '/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/typeshed/stubs/backports_abc'
Exception ignored in: '_pydevd_frame_eval.pydevd_frame_evaluator.get_bytecode_while_frame_eval'
Traceback (most recent call last):
  File "_pydevd_frame_eval/pydevd_frame_evaluator_common.pyx", line 159, in _pydevd_frame_eval.pydevd_frame_evaluator_common.get_func_code_info
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 555, in get_abs_path_real_path_and_base_from_file
Exception ignored in: <module 'threading' from '/home/yfy/anaconda3/envs/pytorch/lib/python3.6/threading.py'>Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process

Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 296, in _exit_function
    _run_finalizers(0)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 253, in _run_finalizers
    keys = [key for key in list(_finalizer_registry) if f(key)]
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 253, in <listcomp>
    keys = [key for key in list(_finalizer_registry) if f(key)]
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 246, in <lambda>
    f = lambda p : p[0] is not None and p[0] >= minpriority
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.4503667230  0.4568175507  0.4605993341  0.4465631929  0.4088105727  0.4119444444  0.4166666667  0.4693821003  0.4629014396  0.3993353642  0.4097452935  0.3538717735  0.3362930078  0.3358333333  0.3322222222  0.3651981158  0.3543743079  0.3462917011  0.3307607497  0.4580931264  0.4880000000  0.0000000000  10.020523071  0.2909131050  0             0.4924907684 
0.5737516239  0.8125520689  0.8146503885  0.4953436807  0.5973568282  0.6780555556  0.6677777778  0.8877805486  0.8726467331  0.9399058433  0.9368770764  0.8995281710  0.9089900111  0.6177777778  0.6155555556  0.9091160986  0.9025470653  0.9382409705  0.9338478501  0.5623059867  0.6400000000  0.4444444444  6.4348829079  0.4781041145  50            0.1377030039 
0.5858290675  0.9103026937  0.9034406215  0.5170731707  0.6030837004  0.6663888889  0.6477777778  0.9476309227  0.9346622370  0.9556909443  0.9512735327  0.9794615598  0.9822419534  0.6150000000  0.5933333333  0.9612080909  0.9623477298  0.9459608492  0.9448732084  0.5782705100  0.6448888889  0.8888888889  4.5360130501  0.4781041145  100           0.1353297138 
0.6161395376  0.8991946681  0.8923418424  0.5441241685  0.6563876652  0.6905555556  0.6777777778  0.9420892214  0.9302325581  0.9750761562  0.9645625692  0.9802941993  0.9833518313  0.6491666667  0.6277777778  0.9675810474  0.9678848283  0.9779432037  0.9669239250  0.5791574279  0.6848888889  1.3333333333  3.9428883648  0.4781041145  150           0.1385278368 
0.5820461551  0.9522354901  0.9422863485  0.5135254989  0.6057268722  0.6788888889  0.6711111111  0.9717373234  0.9645625692  0.9770146774  0.9700996678  0.9900083264  0.9889012209  0.6105555556  0.6044444444  0.9775561097  0.9767441860  0.9691204852  0.9514884234  0.5804878049  0.6284444444  1.7777777778  3.5820310068  0.4781041145  200           0.1368467808 
0.6047890140  0.9175229103  0.9123196448  0.5600886918  0.6101321586  0.8055555556  0.7733333333  0.9470767526  0.9368770764  0.9808917197  0.9767441860  0.9761310019  0.9755826859  0.7372222222  0.7155555556  0.9534497091  0.9490586932  0.9853873725  0.9724366042  0.5791574279  0.6697777778  2.2222222222  3.3938938332  0.4781041145  250           0.1345385695 
0.6233120731  0.9391835601  0.9367369589  0.5649667406  0.6784140969  0.8197222222  0.7900000000  0.9634247714  0.9424141750  0.9819994461  0.9767441860  0.9791840133  0.9811320755  0.7630555556  0.7344444444  0.9595455805  0.9557032115  0.9845602426  0.9757442117  0.5596452328  0.6902222222  2.6666666667  3.1625023890  0.4781041145  300           0.1329988146 
0.6358837901  0.9486253818  0.9345172031  0.5915742794  0.6550660793  0.7738888889  0.7400000000  0.9692435578  0.9545957918  0.9828302409  0.9800664452  0.9941715237  0.9944506104  0.7347222222  0.7166666667  0.9864228318  0.9745293466  0.9867659222  0.9757442117  0.5973392461  0.6995555556  3.1111111111  3.0386893940  0.4781041145  350           0.1335753345 
0.6212845941  0.9539016940  0.9489456160  0.5698447894  0.6361233480  0.7925000000  0.7700000000  0.9695206428  0.9557032115  0.9908612573  0.9878183832  0.9897307799  0.9900110988  0.6888888889  0.6666666667  0.9794957052  0.9712070875  0.9895230218  0.9845644983  0.5733924612  0.7057777778  3.5555555556  2.9448438597  0.4781141281  400           0.1357232141 
0.6351349728  0.9327964454  0.9289678135  0.5911308204  0.6475770925  0.7650000000  0.7288888889  0.9678581324  0.9512735327  0.9861534201  0.9778516058  0.9977796281  0.9955604883  0.7188888889  0.6955555556  0.9905791078  0.9844961240  0.9909015715  0.9812568908  0.5756097561  0.7262222222  4.0000000000  2.7970606565  0.4781141281  450           0.1359061241 
0.6129371278  0.9833379617  0.9755826859  0.5596452328  0.6466960352  0.7902777778  0.7566666667  0.9869770019  0.9723145072  0.9919689837  0.9878183832  0.9986122676  0.9966703663  0.6586111111  0.6411111111  0.9897478526  0.9811738649  0.9867659222  0.9801543550  0.5667405765  0.6786666667  4.4444444444  2.6566580915  0.4781141281  500           0.1379793215 
0.5958210807  0.8997500694  0.8812430633  0.5445676275  0.6127753304  0.8936111111  0.8633333333  0.9105015240  0.8903654485  0.9576294655  0.9479512735  0.9272828199  0.9223085461  0.8816666667  0.8511111111  0.9157661402  0.9025470653  0.9616763165  0.9459757442  0.5263858093  0.6995555556  4.8888888889  2.6350007176  0.4781141281  550           0.1349334574 
0.5943875241  0.9736184393  0.9567147614  0.5543237251  0.6493392070  0.7330555556  0.7233333333  0.9830978110  0.9811738649  0.9878150097  0.9867109635  0.9997224535  0.9988901221  0.5861111111  0.5644444444  0.9980604045  0.9966777409  0.9704990350  0.9636163175  0.5507760532  0.6231111111  5.3333333333  2.6184370232  0.4781265259  600           0.1374823713 
0.5984078162  0.9822271591  0.9711431743  0.5569844789  0.6387665198  0.8005555556  0.7711111111  0.9833748961  0.9623477298  0.9878150097  0.9822812846  0.9927837913  0.9911209767  0.6927777778  0.6777777778  0.9869770019  0.9700996678  0.9900744417  0.9757442117  0.5538802661  0.6440000000  5.7777777778  2.5689451122  0.4781265259  650           0.1328337955 
0.6239338404  0.9894473757  0.9800221976  0.5756097561  0.6409691630  0.7725000000  0.7433333333  0.9864228318  0.9767441860  0.9922459153  0.9889258029  0.9980571746  0.9944506104  0.6730555556  0.6544444444  0.9894707675  0.9867109635  0.9911772815  0.9845644983  0.5796008869  0.6995555556  6.2222222222  2.5084529114  0.4781265259  700           0.1440397453 
0.5963965941  0.9791724521  0.9655937847  0.5600886918  0.6462555066  0.7919444444  0.7700000000  0.9866999169  0.9756367663  0.9955690944  0.9889258029  0.9986122676  0.9966703663  0.6736111111  0.6500000000  0.9955666390  0.9889258029  0.9933829611  0.9801543550  0.5410199557  0.6382222222  6.6666666667  2.4505460835  0.4781265259  750           0.1368743324 
0.6080342742  0.9811163566  0.9667036626  0.5800443459  0.6519823789  0.8177777778  0.7844444444  0.9850374065  0.9579180509  0.9919689837  0.9811738649  0.9963918956  0.9944506104  0.7252777778  0.7100000000  0.9908561929  0.9745293466  0.9911772815  0.9823594267  0.5503325942  0.6497777778  7.1111111111  2.4856150961  0.4781265259  800           0.1355165720 
0.6086374294  0.9663982227  0.9500554939  0.5525498891  0.6387665198  0.8652777778  0.8433333333  0.9678581324  0.9446290144  0.9944613680  0.9778516058  0.9858451291  0.9766925638  0.7600000000  0.7466666667  0.9745081740  0.9557032115  0.9933829611  0.9779492834  0.5450110865  0.6982222222  7.5555555556  2.4189315414  0.4781265259  850           0.1391851711 
0.5928905179  0.9741738406  0.9611542730  0.5552106430  0.6268722467  0.8594444444  0.8244444444  0.9745081740  0.9557032115  0.9936305732  0.9811738649  0.9919511518  0.9877913430  0.7688888889  0.7400000000  0.9814353006  0.9678848283  0.9931072512  0.9812568908  0.5343680710  0.6551111111  8.0000000000  2.4046453428  0.4781589508  900           0.1368099737 
0.6188980348  0.9869480700  0.9722530522  0.5725055432  0.6687224670  0.8166666667  0.7888888889  0.9916874480  0.9734219269  0.9958460260  0.9878183832  0.9988898140  0.9977802442  0.6838888889  0.6644444444  0.9952895539  0.9856035437  0.9933829611  0.9845644983  0.5361419069  0.6982222222  8.4444444444  2.3472152925  0.4781589508  950           0.1327661562 
0.5809454305  0.9644543182  0.9489456160  0.5463414634  0.6083700441  0.8711111111  0.8333333333  0.9775561097  0.9523809524  0.9941844364  0.9833887043  0.9891756869  0.9900110988  0.7869444444  0.7611111111  0.9861457467  0.9712070875  0.9955886407  0.9801543550  0.5184035477  0.6506666667  8.8888888889  2.4150160098  0.4781589508  1000          0.1361280251 
0.5894501771  0.9902804776  0.9789123196  0.5521064302  0.6211453744  0.8283333333  0.7955555556  0.9911332779  0.9712070875  0.9966768208  0.9933554817  0.9983347211  0.9977802442  0.7094444444  0.6866666667  0.9922416182  0.9822812846  0.9936586711  0.9878721058  0.5529933481  0.6315555556  9.3333333333  2.2770719934  0.4781589508  1050          0.1370970631 
0.5681875106  0.9941682866  0.9877913430  0.5241685144  0.6017621145  0.8127777778  0.7777777778  0.9930728734  0.9745293466  0.9972306840  0.9933554817  0.9980571746  0.9966703663  0.6700000000  0.6444444444  0.9944582987  0.9844961240  0.9933829611  0.9823594267  0.5312638581  0.6155555556  9.7777777778  2.2807249022  0.4781589508  1100          0.1378978729 
0.5823536537  0.9838933630  0.9711431743  0.5436807095  0.6246696035  0.8755555556  0.8311111111  0.9814353006  0.9523809524  0.9952921628  0.9856035437  0.9938939772  0.9911209767  0.7383333333  0.7111111111  0.9844832364  0.9623477298  0.9958643507  0.9834619625  0.5210643016  0.6400000000  10.222222222  2.2246395373  0.4781837463  1150          0.1391668177 
0.6119509373  0.9877811719  0.9755826859  0.5592017738  0.6475770925  0.8586111111  0.8166666667  0.9891936825  0.9700996678  0.9972306840  0.9889258029  0.9977796281  0.9933407325  0.7480555556  0.7255555556  0.9908561929  0.9811738649  0.9980700303  0.9878721058  0.5388026608  0.7022222222  10.666666666  2.1353979993  0.4781837463  1200          0.1366824675 
0.5825773410  0.9950013885  0.9911209767  0.5423503326  0.6237885463  0.7955555556  0.7688888889  0.9933499584  0.9800664452  0.9952921628  0.9900332226  0.9997224535  0.9988901221  0.6508333333  0.6255555556  0.9958437240  0.9922480620  0.9942100910  0.9856670342  0.5232815965  0.6408888889  11.111111111  2.1593190742  0.4781837463  1250          0.1348914099 
0.5906488078  0.9919466815  0.9833518313  0.5490022173  0.6418502203  0.7652777778  0.7455555556  0.9933499584  0.9844961240  0.9939075048  0.9844961240  0.9997224535  0.9988901221  0.6036111111  0.5855555556  0.9983374896  1.0000000000  0.9906258616  0.9801543550  0.5157427938  0.6560000000  11.555555555  2.1637969112  0.4781837463  1300          0.1355712891 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2130641) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3336676816  0.3332407665  0.3329633740  0.3365767360  0.3312775330  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3348103019  0.3355481728  0.3330557868  0.3329633740  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3336090433  0.3329658214  0.3325942350  0.3342222222  0.0000000000  9.9555130005  0.2909131050  0             0.8748981953 
0.5763769956  0.8430991391  0.8412874584  0.5024325520  0.6132158590  0.6950000000  0.6711111111  0.8997222222  0.8944444444  0.9332594849  0.9258028793  0.8992506245  0.9034406215  0.6694444444  0.6555555556  0.8886111111  0.8988888889  0.9032258065  0.9018743109  0.5631929047  0.6266666667  0.4444444444  6.7871391773  0.4781041145  50            0.1342904186 
0.5655831200  0.9241877256  0.9211986681  0.4931446263  0.6229074890  0.6002777778  0.5788888889  0.9400000000  0.9311111111  0.9321517585  0.9302325581  0.9794615598  0.9644839068  0.5730555556  0.5566666667  0.9555555556  0.9600000000  0.8916459884  0.8996692393  0.5738359202  0.5724444444  0.8888888889  4.5959386539  0.4781041145  100           0.1348308134 
0.6398145199  0.9075256873  0.9012208657  0.5568332596  0.7264317181  0.6950000000  0.6777777778  0.9525000000  0.9455555556  0.9775685406  0.9678848283  0.9925062448  0.9900110988  0.6736111111  0.6566666667  0.9838888889  0.9866666667  0.9771160739  0.9680264609  0.6031042129  0.6728888889  1.3333333333  3.9923136330  0.4781041145  150           0.1387715578 
0.5819453684  0.9716745349  0.9711431743  0.5157010172  0.6440528634  0.6927777778  0.6822222222  0.9758333333  0.9700000000  0.9742453614  0.9678848283  0.9952817097  0.9855715871  0.6366666667  0.6288888889  0.9791666667  0.9833333333  0.9556106975  0.9437706725  0.5875831486  0.5804444444  1.7777777778  3.5352691174  0.4781041145  200           0.1368592691 
0.5992806318  0.9078033879  0.9023307436  0.5550641309  0.6792951542  0.7816666667  0.7577777778  0.9580555556  0.9522222222  0.9797839934  0.9656699889  0.9938939772  0.9933407325  0.7347222222  0.7033333333  0.9794444444  0.9822222222  0.9848359526  0.9746416759  0.5565410200  0.6062222222  2.2222222222  3.3086589479  0.4781041145  250           0.1346404791 
0.5405968075  0.9688975285  0.9644839068  0.4953560372  0.6317180617  0.8380555556  0.8100000000  0.9805555556  0.9777777778  0.9783993354  0.9756367663  0.9897307799  0.9811320755  0.7680555556  0.7233333333  0.9680555556  0.9700000000  0.9729804246  0.9625137817  0.5090909091  0.5262222222  2.6666666667  3.0414306974  0.4781041145  300           0.1331065655 
0.5300340485  0.9661205221  0.9600443951  0.4613003096  0.6118942731  0.8541666667  0.8211111111  0.9794444444  0.9711111111  0.9853226253  0.9789590255  0.9919511518  0.9877913430  0.8019444444  0.7677777778  0.9772222222  0.9733333333  0.9831816929  0.9724366042  0.4762749446  0.5706666667  3.1111111111  2.8918694448  0.4781041145  350           0.1333905125 
0.5478845189  0.9791724521  0.9700332963  0.4878372402  0.6070484581  0.7625000000  0.7477777778  0.9908333333  0.9855555556  0.9905843257  0.9844961240  0.9969469886  0.9944506104  0.6644444444  0.6422222222  0.9944444444  0.9955555556  0.9845602426  0.9757442117  0.5064301552  0.5902222222  3.5555555556  2.8392916393  0.4781041145  400           0.1348546410 
0.5265625435  0.9564009997  0.9522752497  0.4873949580  0.5555066079  0.7738888889  0.7488888889  0.9858333333  0.9788888889  0.9886458045  0.9767441860  0.9983347211  0.9977802442  0.7366666667  0.6955555556  0.9988888889  0.9977777778  0.9920044114  0.9856670342  0.4931263858  0.5702222222  4.0000000000  2.6725712919  0.4796552658  450           0.1341516209 
0.5389765204  0.9800055540  0.9755826859  0.4909332154  0.6202643172  0.8069444444  0.7766666667  0.9933333333  0.9866666667  0.9939075048  0.9867109635  0.9991673605  0.9944506104  0.7533333333  0.7044444444  0.9983333333  0.9955555556  0.9892473118  0.9812568908  0.4811529933  0.5635555556  4.4444444444  2.5829557562  0.4796552658  500           0.1307268810 
0.5443817782  0.9491807831  0.9345172031  0.4745687749  0.5995594714  0.8711111111  0.8311111111  0.9763888889  0.9722222222  0.9867072833  0.9756367663  0.9922286983  0.9889012209  0.8791666667  0.8433333333  0.9863888889  0.9788888889  0.9895230218  0.9779492834  0.4705099778  0.6328888889  4.8888888889  2.5454929495  0.4796552658  550           0.1324919462 
0.5222571199  0.9850041655  0.9778024417  0.4705882353  0.6039647577  0.7716666667  0.7600000000  0.9936111111  0.9877777778  0.9914151205  0.9878183832  0.9994449070  0.9955604883  0.6500000000  0.6155555556  0.9963888889  0.9966666667  0.9851116625  0.9779492834  0.4860310421  0.5284444444  5.3333333333  2.5135034370  0.4796552658  600           0.1333370495 
0.5205399610  0.9711191336  0.9644839068  0.4657231314  0.5859030837  0.8872222222  0.8422222222  0.9847222222  0.9722222222  0.9900304625  0.9800664452  0.9941715237  0.9877913430  0.8180555556  0.7744444444  0.9927777778  0.9822222222  0.9895230218  0.9801543550  0.4598669623  0.5706666667  5.7777777778  2.4928802299  0.4796552658  650           0.1343666792 
0.5481610886  0.9775062483  0.9700332963  0.5095090668  0.6255506608  0.8166666667  0.7911111111  0.9880555556  0.9833333333  0.9916920521  0.9867109635  0.9972245351  0.9933407325  0.7836111111  0.7411111111  0.9969444444  0.9944444444  0.9906258616  0.9867695700  0.4869179601  0.5706666667  6.2222222222  2.4821522427  0.4796552658  700           0.1346374035 
0.4963840128  0.9844487642  0.9844617092  0.4453781513  0.5704845815  0.8816666667  0.8466666667  0.9913888889  0.9900000000  0.9944613680  0.9867109635  0.9969469886  0.9944506104  0.7833333333  0.7600000000  0.9947222222  0.9944444444  0.9909015715  0.9812568908  0.4470066519  0.5226666667  6.6666666667  2.3942397165  0.4796552658  750           0.1314111614 
0.5345190253  0.9650097195  0.9622641509  0.4834144184  0.5823788546  0.9136111111  0.8700000000  0.9850000000  0.9733333333  0.9903073941  0.9789590255  0.9938939772  0.9911209767  0.8780555556  0.8277777778  0.9958333333  0.9877777778  0.9903501516  0.9812568908  0.4727272727  0.5995555556  7.1111111111  2.3703684425  0.4810485840  800           0.1334525490 
0.5081802486  0.9833379617  0.9778024417  0.4568774878  0.5533039648  0.8813888889  0.8411111111  0.9913888889  0.9833333333  0.9947382996  0.9878183832  0.9986122676  0.9966703663  0.7933333333  0.7644444444  0.9972222222  0.9888888889  0.9889716019  0.9823594267  0.4572062084  0.5653333333  7.5555555556  2.3528018689  0.4810485840  850           0.1347702694 
0.5009448173  0.9913912802  0.9811320755  0.4551083591  0.5634361233  0.8702777778  0.8277777778  0.9947222222  0.9900000000  0.9961229576  0.9889258029  0.9983347211  0.9955604883  0.7647222222  0.7344444444  0.9991666667  0.9944444444  0.9922801213  0.9823594267  0.4443458980  0.5408888889  8.0000000000  2.2775623512  0.4810485840  900           0.1332578659 
0.5163273603  0.9827825604  0.9778024417  0.4586466165  0.5850220264  0.9016666667  0.8533333333  0.9952777778  0.9900000000  0.9958460260  0.9900332226  0.9972245351  0.9977802442  0.8163888889  0.7900000000  0.9986111111  0.9944444444  0.9942100910  0.9834619625  0.4616407982  0.5600000000  8.4444444444  2.2706915402  0.4810485840  950           0.1343685770 
0.5078168307  0.9669536240  0.9633740289  0.4551083591  0.5718061674  0.9344444444  0.8933333333  0.9802777778  0.9655555556  0.9919689837  0.9811738649  0.9930613378  0.9900110988  0.8872222222  0.8333333333  0.9886111111  0.9811111111  0.9886958919  0.9790518192  0.4412416851  0.5631111111  8.8888888889  2.2257685685  0.4810485840  1000          0.1341938066 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_trace_dispatch.py", line 56, in trace_dispatch
    def trace_dispatch(py_db, frame, event, arg):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2216934) is killed by signal: Terminated. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2173, in <module>
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2164, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1476, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1483, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
      File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 63, in handler
    def handler(signum, frame):
RuntimeError: DataLoader worker (pid 2218196) is killed by signal: Terminated. 
Exception ignored in: <bound method _ConnectionBase.__del__ of <multiprocessing.connection.Connection object at 0x7f21346571d0>>
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 130, in __del__
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
RuntimeError: DataLoader worker (pid 2217422) is killed by signal: Terminated. 
Exception ignored in: Exception ignored in: <function WeakValueDictionary.__init__.<locals>.remove at 0x7f22980a9ae8>
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/weakref.py", line 109, in remove
    def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2216259) is killed by signal: Terminated. 
Error in sys.excepthook:
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 267, in in_project_roots
    return filename_to_in_scope_cache[filename]
KeyError: '/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 557, in get_abs_path_real_path_and_base_from_file
    return NORM_PATHS_AND_BASE_CONTAINER[f]
KeyError: '/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/typeshed/stubs/click'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 230, in _NormPaths
    return NORM_PATHS_CONTAINER[filename]
KeyError: '/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/typeshed/stubs/click'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_breakpoints.py", line 178, in _fallback_excepthook
    stop_on_unhandled_exception(debugger, thread, additional_info, (exctype, value, tb))
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_breakpoints.py", line 165, in stop_on_unhandled_exception
    py_db.stop_on_unhandled_exception(thread, frame, frames_byid, arg)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1253, in stop_on_unhandled_exception
    tb = pydevd_utils.get_top_level_trace_in_project_scope(tb)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 448, in get_top_level_trace_in_project_scope
    if is_top_level_trace_in_project_scope(trace):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 322, in is_top_level_trace_in_project_scope
    return is_exception_trace_in_project_scope(trace) and not is_exception_trace_in_project_scope(trace.tb_next)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 310, in is_exception_trace_in_project_scope
    elif in_project_roots(trace.tb_frame.f_code.co_filename):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 280, in in_project_roots
    library_roots = _get_library_roots()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 261, in _get_library_roots
    return _get_roots(library_roots_cache, 'LIBRARY_ROOTS', set_library_roots, _get_default_library_roots)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 208, in _get_roots
    set_when_not_cached(roots)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 256, in set_library_roots
    roots = _set_roots(roots, _LIBRARY_ROOTS_CACHE)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 188, in _set_roots
    new_roots.append(_normpath(root))
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 27, in _normpath
    return pydevd_file_utils.get_abs_path_real_path_and_base_from_file(filename)[0]
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 571, in get_abs_path_real_path_and_base_from_file
    abs_path, real_path = _NormPaths(f)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 238, in _NormPaths
    real_path = _NormPath(filename, rPath)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 246, in _NormPath
    r = normpath(filename)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/posixpath.py", line 396, in realpath
    return abspath(path)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/posixpath.py", line 385, in abspath
    return normpath(path)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/posixpath.py", line 364, in normpath
    if (comp != dotdot or (not initial_slashes and not new_comps) or
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2215900) is killed by signal: Terminated. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_breakpoints.py", line 178, in _fallback_excepthook
    stop_on_unhandled_exception(debugger, thread, additional_info, (exctype, value, tb))
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2215932) is killed by signal: Terminated. 

Original exception was:
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_trace_dispatch.py", line 56, in trace_dispatch
    def trace_dispatch(py_db, frame, event, arg):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2216934) is killed by signal: Terminated. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2173, in <module>
    main()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2164, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1476, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1483, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 63, in handler
    def handler(signum, frame):
RuntimeError: DataLoader worker (pid 2218196) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3648609228  0.3451818939  0.3496115427  0.3756097561  0.3562005277  0.3333333333  0.3333333333  0.3491271820  0.3455149502  0.3376803552  0.3403547672  0.3336108798  0.3329633740  0.3333333333  0.3333333333  0.3372125242  0.3355481728  0.3323204420  0.3322295806  0.3649667406  0.3626666667  0.0000000000  10.175868988  0.2909131050  0             0.7158071995 
0.5130919576  0.8575395723  0.8412874584  0.4159645233  0.5664028144  0.6916666667  0.6755555556  0.8678304239  0.8571428571  0.9228634850  0.9157427938  0.9378295865  0.9289678135  0.6011111111  0.5933333333  0.8941535051  0.8892580288  0.9038674033  0.8962472406  0.4997782705  0.5702222222  0.4444444444  6.5362708473  0.4781041145  50            0.1393387032 
0.5576503454  0.9258539295  0.9189789123  0.4470066519  0.6332453826  0.6038888889  0.5911111111  0.9487392630  0.9457364341  0.9519977802  0.9368070953  0.9694698862  0.9467258602  0.5188888889  0.5088888889  0.9423663065  0.9490586932  0.8718232044  0.8609271523  0.5427937916  0.6075555556  0.8888888889  4.5272142315  0.4781041145  100           0.1441374350 
0.5526816930  0.9147459039  0.9012208657  0.4164079823  0.6336851363  0.7130555556  0.6944444444  0.9476309227  0.9368770764  0.9775249723  0.9656319290  0.9822370247  0.9778024417  0.6511111111  0.6311111111  0.9564976448  0.9568106312  0.9726519337  0.9724061810  0.5148558758  0.6457777778  1.3333333333  3.8937600899  0.4781041145  150           0.1339262438 
0.5494574505  0.9544570953  0.9489456160  0.4257206208  0.6314863676  0.6886111111  0.6833333333  0.9637018565  0.9568106312  0.9816870144  0.9733924612  0.9877879545  0.9866814650  0.6261111111  0.6022222222  0.9733998337  0.9778516058  0.9662983425  0.9646799117  0.5197339246  0.6208888889  1.7777777778  3.5868318367  0.4781813622  200           0.1357237101 
0.5897161973  0.9289086365  0.9223085461  0.4882483370  0.6868953386  0.7441666667  0.7322222222  0.9581601552  0.9479512735  0.9852941176  0.9811529933  0.9902858729  0.9877913430  0.6547222222  0.6366666667  0.9725685786  0.9745293466  0.9795580110  0.9735099338  0.5254988914  0.6582222222  2.2222222222  3.3542999792  0.4781813622  250           0.1379933310 
0.5685156924  0.9155790058  0.9001109878  0.4616407982  0.6468777485  0.8261111111  0.7888888889  0.9329454142  0.9136212625  0.9747502775  0.9611973392  0.9569802942  0.9478357381  0.7788888889  0.7533333333  0.9332224993  0.9235880399  0.9712707182  0.9536423841  0.5050997783  0.6604444444  2.6666666667  3.1350060368  0.4781813622  300           0.1397470760 
0.5633800239  0.9455706748  0.9267480577  0.4665188470  0.6547933157  0.7683333333  0.7400000000  0.9717373234  0.9623477298  0.9880688124  0.9789356984  0.9950041632  0.9911209767  0.6836111111  0.6588888889  0.9817123857  0.9822812846  0.9872928177  0.9779249448  0.5064301552  0.6257777778  3.1111111111  2.9750453758  0.4781813622  350           0.1404129505 
0.5826731417  0.9180783116  0.9034406215  0.4731707317  0.6763412489  0.7977777778  0.7777777778  0.9509559435  0.9346622370  0.9897336293  0.9767184035  0.9786289203  0.9766925638  0.7288888889  0.7066666667  0.9678581324  0.9557032115  0.9911602210  0.9823399558  0.4687361419  0.7124444444  3.5555555556  2.8877474546  0.4781813622  400           0.1391192055 
0.5817013250  0.9522354901  0.9422863485  0.4749445676  0.6587510994  0.7777777778  0.7511111111  0.9706289831  0.9557032115  0.9880688124  0.9745011086  0.9933388843  0.9933407325  0.7288888889  0.7000000000  0.9828207260  0.9778516058  0.9906077348  0.9801324503  0.5006651885  0.6924444444  4.0000000000  2.7812852716  0.4781813622  450           0.1381481361 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1171, in _next_data
    self._shutdown_workers()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1297, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 124, in join
    res = self._popen.wait(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 47, in wait
    if not wait([self.sentinel], timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3630136206  0.3659638554  0.3674698795  0.3990384615  0.3341288783  0.3960843373  0.4036144578  0.3924812030  0.3772455090  0.3834586466  0.4011976048  0.3915662651  0.3614457831  0.3539156627  0.3674698795  0.3524096386  0.3734939759  0.3583208396  0.3413173653  0.3807228916  0.3381642512  0.0000000000  9.8721332550  0.2909131050  0             0.4807491302 
0.7000730882  0.9337349398  0.9036144578  0.6682692308  0.7446300716  0.9231927711  0.8734939759  0.9563909774  0.9221556886  0.9879699248  0.9640718563  0.9397590361  0.9036144578  0.8433734940  0.8554216867  0.9442771084  0.9096385542  0.9880059970  0.9640718563  0.6192771084  0.7681159420  2.4096385542  4.9429405069  0.4781041145  50            0.1362421417 
0.7095991038  0.9939759036  0.9638554217  0.6875000000  0.7756563246  0.9231927711  0.8493975904  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9924698795  0.9698795181  0.7710843373  0.7831325301  0.9909638554  0.9759036145  0.9895052474  0.9700598802  0.6506024096  0.7246376812  4.8192771084  2.7946293783  0.4781041145  100           0.1337835073 
0.6869604050  1.0000000000  1.0000000000  0.6995192308  0.6634844869  0.9713855422  0.8975903614  0.9984962406  0.9940119760  1.0000000000  0.9880239521  0.9969879518  1.0000000000  0.8102409639  0.7951807229  0.9969879518  0.9879518072  1.0000000000  0.9760479042  0.6771084337  0.7077294686  7.2289156627  2.4511704135  0.4781041145  150           0.1368007374 
0.6930001532  1.0000000000  1.0000000000  0.6971153846  0.6515513126  0.9442771084  0.8734939759  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9864457831  0.9939759036  0.7183734940  0.7409638554  0.9924698795  0.9939759036  0.9880059970  0.9760479042  0.7397590361  0.6835748792  9.6385542169  2.1903628492  0.4781041145  200           0.1301261997 
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
Traceback (most recent call last):
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
KeyboardInterrupt

During handling of the above exception, another exception occurred:


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
KeyboardInterrupt
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
KeyboardInterrupt
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
Traceback (most recent call last):

During handling of the above exception, another exception occurred:

AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
Traceback (most recent call last):
KeyboardInterrupt

During handling of the above exception, another exception occurred:

  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
Traceback (most recent call last):
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
KeyboardInterrupt
Traceback (most recent call last):
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
KeyboardInterrupt

During handling of the above exception, another exception occurred:


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
KeyboardInterrupt

During handling of the above exception, another exception occurred:


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2337800) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3630136206  0.3659638554  0.3674698795  0.3990384615  0.3341288783  0.3960843373  0.4036144578  0.3924812030  0.3772455090  0.3834586466  0.4011976048  0.3915662651  0.3614457831  0.3539156627  0.3674698795  0.3524096386  0.3734939759  0.3583208396  0.3413173653  0.3807228916  0.3381642512  0.0000000000  9.8721332550  0.2909131050  0             0.4832181931 
0.7000730882  0.9337349398  0.9036144578  0.6682692308  0.7446300716  0.9231927711  0.8734939759  0.9563909774  0.9221556886  0.9879699248  0.9640718563  0.9397590361  0.9036144578  0.8433734940  0.8554216867  0.9442771084  0.9096385542  0.9880059970  0.9640718563  0.6192771084  0.7681159420  2.4096385542  4.9429405069  0.4781041145  50            0.1392230463 
0.7095991038  0.9939759036  0.9638554217  0.6875000000  0.7756563246  0.9231927711  0.8493975904  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9924698795  0.9698795181  0.7710843373  0.7831325301  0.9909638554  0.9759036145  0.9895052474  0.9700598802  0.6506024096  0.7246376812  4.8192771084  2.7946293783  0.4781041145  100           0.1381511211 
0.6869604050  1.0000000000  1.0000000000  0.6995192308  0.6634844869  0.9713855422  0.8975903614  0.9984962406  0.9940119760  1.0000000000  0.9880239521  0.9969879518  1.0000000000  0.8102409639  0.7951807229  0.9969879518  0.9879518072  1.0000000000  0.9760479042  0.6771084337  0.7077294686  7.2289156627  2.4511704135  0.6689839363  150           0.1409334993 
0.6930001532  1.0000000000  1.0000000000  0.6971153846  0.6515513126  0.9442771084  0.8734939759  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9864457831  0.9939759036  0.7183734940  0.7409638554  0.9924698795  0.9939759036  0.9880059970  0.9760479042  0.7397590361  0.6835748792  9.6385542169  2.1903628492  0.6689839363  200           0.1354862976 
0.6761415593  1.0000000000  1.0000000000  0.6754807692  0.6587112172  0.9789156627  0.9036144578  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.8283132530  0.8192771084  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.6650602410  0.7053140097  12.048192771  2.0691321015  0.6689839363  250           0.1389551878 
0.6449227539  1.0000000000  0.9879518072  0.6514423077  0.6205250597  0.9984939759  0.9337349398  0.9984962406  0.9940119760  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.9623493976  0.9216867470  0.9984939759  0.9939759036  0.9985007496  0.9820359281  0.6024096386  0.7053140097  14.457831325  1.9609461403  0.6689839363  300           0.1347055435 
0.6449588115  1.0000000000  1.0000000000  0.6370192308  0.6109785203  0.9939759036  0.9277108434  0.9984962406  0.9940119760  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.9518072289  0.9156626506  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6192771084  0.7125603865  16.867469879  1.8266698289  0.6689839363  350           0.1422452688 
0.6461146507  0.9954819277  0.9939759036  0.6394230769  0.6276849642  1.0000000000  0.9397590361  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.9954819277  0.9939759036  0.9924698795  0.9698795181  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.6168674699  0.7004830918  19.277108433  1.7863509870  0.6689839363  400           0.1383443880 
0.6533420073  1.0000000000  1.0000000000  0.6514423077  0.6229116945  0.9984939759  0.9337349398  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.9548192771  0.9156626506  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.6481927711  0.6908212560  21.686746988  1.7667891002  0.6689839363  450           0.1446295261 
0.6580646755  1.0000000000  1.0000000000  0.6682692308  0.6515513126  0.9683734940  0.8975903614  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.6987951807  0.7108433735  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.6506024096  0.6618357488  24.096385542  1.7769924045  0.6689839363  500           0.1451307297 
0.6480168811  1.0000000000  1.0000000000  0.6418269231  0.5918854415  0.9954819277  0.9337349398  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.8569277108  0.8373493976  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.6409638554  0.7173913043  26.506024096  1.7199706554  0.6689839363  550           0.1448960876 
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 113, in get
    return _ForkingPickler.loads(res)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 282, in rebuild_storage_fd
    fd = df.detach()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py", line 57, in detach
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 487, in Client
    c = SocketClient(address)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 614, in SocketClient
    s.connect(address)
FileNotFoundError: [Errno 2] No such file or directory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 999, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 2337852, 2337858, 2337861, 2337864, 2337867, 2337870) exited unexpectedly
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3257061625  0.3298192771  0.3313253012  0.3341346154  0.3317422434  0.3343373494  0.3373493976  0.3323308271  0.3293413174  0.3428571429  0.3353293413  0.4397590361  0.4457831325  0.4262048193  0.4397590361  0.4427710843  0.4457831325  0.4482758621  0.4371257485  0.3036144578  0.3333333333  0.0000000000  9.8721332550  0.2909131050  0             0.4620113373 
0.6989666363  0.9352409639  0.9337349398  0.7091346154  0.6706443914  0.8840361446  0.8373493976  0.9518796992  0.9041916168  0.9368421053  0.9161676647  0.7786144578  0.7891566265  0.8042168675  0.8433734940  0.8012048193  0.7349397590  0.8785607196  0.8862275449  0.7397590361  0.6763285024  2.4096385542  7.0865690327  0.4781041145  50            0.1356271982 
0.7546753621  0.9683734940  0.9457831325  0.6947115385  0.8353221957  0.8734939759  0.8433734940  0.9849624060  0.9760479042  0.9954887218  0.9700598802  0.9337349398  0.9397590361  0.7409638554  0.7831325301  0.9186746988  0.9277108434  0.9445277361  0.9341317365  0.6915662651  0.7971014493  4.8192771084  4.2873548365  0.4781169891  100           0.1354502678 
0.7529887088  0.9819277108  0.9638554217  0.7235576923  0.7756563246  0.9141566265  0.8614457831  0.9849624060  0.9760479042  0.9939849624  0.9760479042  0.9593373494  0.9578313253  0.7725903614  0.7891566265  0.9578313253  0.9638554217  0.9805097451  0.9461077844  0.7253012048  0.7874396135  7.2289156627  3.4595962572  0.4781737328  150           0.1355060482 
0.7469294683  1.0000000000  1.0000000000  0.7427884615  0.7661097852  0.8840361446  0.8132530120  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9789156627  0.9879518072  0.7409638554  0.7831325301  0.9593373494  0.9698795181  0.9775112444  0.9341317365  0.7686746988  0.7101449275  9.6385542169  3.0795736313  0.4781737328  200           0.1364346838 
0.7355096960  1.0000000000  1.0000000000  0.7283653846  0.7613365155  0.9096385542  0.8554216867  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9894578313  0.9939759036  0.7228915663  0.7831325301  0.9879518072  0.9759036145  0.9850074963  0.9401197605  0.7325301205  0.7198067633  12.048192771  2.8527308750  0.4781737328  250           0.1344025135 
0.7270544399  0.9969879518  0.9879518072  0.7139423077  0.7732696897  0.9427710843  0.8795180723  0.9954887218  0.9820359281  0.9969924812  0.9760479042  0.9924698795  0.9819277108  0.7801204819  0.8072289157  0.9939759036  0.9759036145  0.9940029985  0.9760479042  0.7036144578  0.7173913043  14.457831325  2.6282293987  0.4781737328  300           0.1354423094 
0.6977236887  0.9879518072  0.9397590361  0.6778846154  0.7136038186  0.9819277108  0.9156626506  0.9699248120  0.9401197605  0.9969924812  0.9760479042  0.9849397590  0.9397590361  0.9201807229  0.8855421687  0.9698795181  0.9397590361  0.9970014993  0.9760479042  0.6457831325  0.7536231884  16.867469879  2.4481339025  0.4781794548  350           0.1372036314 
0.7150553032  0.9969879518  0.9879518072  0.6899038462  0.7613365155  0.9834337349  0.9096385542  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9924698795  0.9819277108  0.8960843373  0.8734939759  0.9954819277  0.9819277108  0.9985007496  0.9820359281  0.6819277108  0.7270531401  19.277108433  2.3406682634  0.4781794548  400           0.1341226959 
0.7108555221  1.0000000000  1.0000000000  0.7043269231  0.7470167064  0.9713855422  0.8855421687  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.8162650602  0.8313253012  0.9954819277  0.9819277108  0.9985007496  0.9820359281  0.6795180723  0.7125603865  21.686746988  2.2676979113  0.4781794548  450           0.1396355534 
0.7173957572  1.0000000000  1.0000000000  0.7019230769  0.7828162291  0.9683734940  0.8855421687  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.7891566265  0.8072289157  0.9954819277  0.9819277108  0.9985007496  0.9820359281  0.6746987952  0.7101449275  24.096385542  2.1844823813  0.4781794548  500           0.1325634813 
0.6982554308  0.9984939759  0.9939759036  0.6802884615  0.7350835322  0.9969879518  0.9277108434  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9954819277  0.9939759036  0.9337349398  0.8795180723  0.9969879518  0.9879518072  0.9985007496  0.9820359281  0.6530120482  0.7246376812  26.506024096  2.1189160776  0.4781794548  550           0.1337890530 
0.6999815151  1.0000000000  1.0000000000  0.7019230769  0.7446300716  0.9683734940  0.8734939759  0.9984962406  0.9940119760  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.7665662651  0.7771084337  0.9969879518  0.9879518072  0.9985007496  0.9820359281  0.7036144578  0.6497584541  28.915662650  2.0425043869  0.4781794548  600           0.1343560457 
0.6958197464  0.9984939759  0.9939759036  0.6826923077  0.7398568019  0.9774096386  0.8975903614  0.9984962406  0.9940119760  0.9984962406  0.9820359281  0.9984939759  0.9939759036  0.8524096386  0.8433734940  0.9984939759  0.9939759036  0.9985007496  0.9820359281  0.6578313253  0.7028985507  31.325301204  1.9854003692  0.4781794548  650           0.1342694044 
0.7024245929  1.0000000000  1.0000000000  0.6923076923  0.7446300716  0.9864457831  0.8975903614  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.8644578313  0.8433734940  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6771084337  0.6956521739  33.734939759  1.9889207602  0.4781794548  700           0.1351565313 
0.6814509917  1.0000000000  1.0000000000  0.6730769231  0.7016706444  0.9909638554  0.9156626506  0.9984962406  0.9940119760  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.8855421687  0.8554216867  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6626506024  0.6884057971  36.144578313  1.9735912752  0.4781794548  750           0.1342553234 
0.6796340004  0.9984939759  0.9939759036  0.6514423077  0.7183770883  1.0000000000  0.9397590361  0.9984962406  0.9940119760  1.0000000000  0.9880239521  0.9984939759  0.9939759036  0.9834337349  0.9457831325  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6313253012  0.7173913043  38.554216867  1.9113263512  0.4781928062  800           0.1355365849 
0.6819872240  1.0000000000  1.0000000000  0.6706730769  0.7279236277  0.9909638554  0.9156626506  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.8750000000  0.8493975904  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.6481927711  0.6811594203  40.963855421  1.8866779852  0.4786534309  850           0.1364157200 
0.6760709956  1.0000000000  1.0000000000  0.6658653846  0.6849642005  0.9984939759  0.9337349398  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.9171686747  0.8855421687  0.9984939759  0.9939759036  1.0000000000  0.9760479042  0.6698795181  0.6835748792  43.373493975  1.8671450996  0.4786534309  900           0.1339026594 
0.6807868045  1.0000000000  1.0000000000  0.6586538462  0.7303102625  1.0000000000  0.9397590361  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.9849397590  0.9518072289  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6481927711  0.6859903382  45.783132530  1.8452795839  0.4786534309  950           0.1352161312 
0.6765973270  1.0000000000  1.0000000000  0.6658653846  0.7207637232  1.0000000000  0.9397590361  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.9653614458  0.9216867470  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.6192771084  0.7004830918  48.192771084  1.7990436292  0.4786534309  1000          0.1348897457 
0.6849932784  1.0000000000  1.0000000000  0.6826923077  0.7207637232  0.9653614458  0.8855421687  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.8343373494  0.8313253012  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6819277108  0.6545893720  50.602409638  1.8383652759  0.4786534309  1050          0.1354307175 
0.6634146213  1.0000000000  1.0000000000  0.6586538462  0.6873508353  1.0000000000  0.9397590361  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.9804216867  0.9337349398  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.6313253012  0.6763285024  53.012048192  1.8054369044  0.4786534309  1100          0.1330626535 
0.6634491268  1.0000000000  1.0000000000  0.6586538462  0.6730310263  1.0000000000  0.9397590361  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.9352409639  0.8855421687  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.6457831325  0.6763285024  55.421686747  1.7636943221  0.4786534309  1150          0.1354356718 
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 113, in get
    return _ForkingPickler.loads(res)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 282, in rebuild_storage_fd
    fd = df.detach()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 487, in Client
    c = SocketClient(address)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 614, in SocketClient
    s.connect(address)
FileNotFoundError: [Errno 2] No such file or directory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 999, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 2377025, 2377028, 2377034, 2377037, 2377040) exited unexpectedly
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3257061625  0.3298192771  0.3313253012  0.3341346154  0.3317422434  0.3343373494  0.3373493976  0.3323308271  0.3293413174  0.3428571429  0.3353293413  0.4397590361  0.4457831325  0.4262048193  0.4397590361  0.4427710843  0.4457831325  0.4482758621  0.4371257485  0.3036144578  0.3333333333  0.0000000000  9.8721332550  0.2909131050  0             0.4767260551 
0.6971608275  0.9382530120  0.9337349398  0.6971153846  0.6706443914  0.8780120482  0.8373493976  0.9578947368  0.9041916168  0.9308270677  0.9041916168  0.7786144578  0.7891566265  0.7966867470  0.8373493976  0.7921686747  0.7349397590  0.8785607196  0.8862275449  0.7542168675  0.6666666667  2.4096385542  7.0759645271  0.4781041145  50            0.1328635502 
0.7553051408  0.9683734940  0.9457831325  0.6947115385  0.8257756563  0.8704819277  0.8433734940  0.9834586466  0.9820359281  0.9954887218  0.9700598802  0.9337349398  0.9397590361  0.7469879518  0.7831325301  0.9171686747  0.9337349398  0.9430284858  0.9281437126  0.6963855422  0.8043478261  4.8192771084  4.2806668854  0.4781041145  100           0.1352207232 
0.7439712996  0.9819277108  0.9638554217  0.7091346154  0.7708830549  0.9186746988  0.8554216867  0.9834586466  0.9820359281  0.9969924812  0.9760479042  0.9653614458  0.9578313253  0.7771084337  0.8072289157  0.9533132530  0.9578313253  0.9865067466  0.9461077844  0.7108433735  0.7850241546  7.2289156627  3.4567834425  0.4781789780  150           0.1331622124 
0.7475419871  1.0000000000  1.0000000000  0.7403846154  0.7637231504  0.8840361446  0.8132530120  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9789156627  0.9879518072  0.7409638554  0.7831325301  0.9563253012  0.9578313253  0.9775112444  0.9341317365  0.7710843373  0.7149758454  9.6385542169  3.0692852974  0.4781789780  200           0.1332170010 
0.7391212576  1.0000000000  1.0000000000  0.7331730769  0.7613365155  0.9096385542  0.8554216867  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9894578313  0.9939759036  0.7243975904  0.7771084337  0.9879518072  0.9759036145  0.9850074963  0.9401197605  0.7421686747  0.7198067633  12.048192771  2.8560798836  0.4783902168  250           0.1338666201 
0.7222538567  0.9984939759  0.9819277108  0.7091346154  0.7661097852  0.9427710843  0.8795180723  0.9954887218  0.9820359281  0.9969924812  0.9760479042  0.9939759036  0.9879518072  0.7786144578  0.8012048193  0.9939759036  0.9759036145  0.9940029985  0.9760479042  0.6987951807  0.7149758454  14.457831325  2.6670120335  0.4783902168  300           0.1339658642 
0.7007255932  0.9879518072  0.9397590361  0.6923076923  0.7136038186  0.9819277108  0.9156626506  0.9729323308  0.9401197605  0.9969924812  0.9760479042  0.9849397590  0.9397590361  0.9246987952  0.8795180723  0.9728915663  0.9397590361  0.9970014993  0.9760479042  0.6457831325  0.7512077295  16.867469879  2.4972346020  0.4783902168  350           0.1338425446 
0.7180659172  0.9984939759  0.9939759036  0.6971153846  0.7613365155  0.9864457831  0.9096385542  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9939759036  0.9879518072  0.8930722892  0.8734939759  0.9954819277  0.9819277108  0.9985007496  0.9820359281  0.6819277108  0.7318840580  19.277108433  2.3539424658  0.4783902168  400           0.1337842464 
0.7168666200  1.0000000000  1.0000000000  0.7139423077  0.7494033413  0.9713855422  0.8855421687  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.8192771084  0.8313253012  0.9954819277  0.9819277108  0.9985007496  0.9820359281  0.6939759036  0.7101449275  21.686746988  2.2762937379  0.4784297943  450           0.1335487890 
0.7222366664  1.0000000000  1.0000000000  0.7067307692  0.7756563246  0.9668674699  0.8795180723  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.7981927711  0.8072289157  0.9954819277  0.9819277108  0.9985007496  0.9820359281  0.6843373494  0.7222222222  24.096385542  2.1916062927  0.4784426689  500           0.1339838123 
0.7000943455  0.9984939759  0.9939759036  0.6875000000  0.7231503580  0.9969879518  0.9277108434  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9954819277  0.9939759036  0.9442771084  0.8855421687  0.9954819277  0.9819277108  0.9985007496  0.9820359281  0.6530120482  0.7367149758  26.506024096  2.1473647451  0.4784426689  550           0.1353712273 
0.7048080122  1.0000000000  1.0000000000  0.7067307692  0.7422434368  0.9683734940  0.8855421687  0.9984962406  0.9940119760  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.7695783133  0.7891566265  0.9969879518  0.9879518072  0.9985007496  0.9820359281  0.7132530120  0.6570048309  28.915662650  2.0839333034  0.4784426689  600           0.1342398548 
0.7024448183  0.9984939759  0.9939759036  0.6899038462  0.7398568019  0.9789156627  0.9036144578  0.9984962406  0.9940119760  0.9984962406  0.9820359281  0.9984939759  0.9939759036  0.8599397590  0.8373493976  0.9984939759  0.9939759036  0.9985007496  0.9820359281  0.6722891566  0.7077294686  31.325301204  2.0112320995  0.4810328484  650           0.1427711725 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3  normal: 124 -> 356
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3257061625  0.3298192771  0.3313253012  0.3341346154  0.3317422434  0.3343373494  0.3373493976  0.3323308271  0.3293413174  0.3428571429  0.3353293413  0.4397590361  0.4457831325  0.4262048193  0.4397590361  0.4427710843  0.4457831325  0.4482758621  0.4371257485  0.3036144578  0.3333333333  0.0000000000  9.8721332550  0.2909131050  0             0.4816327095 
0.6971608275  0.9382530120  0.9337349398  0.6971153846  0.6706443914  0.8780120482  0.8373493976  0.9578947368  0.9041916168  0.9308270677  0.9041916168  0.7786144578  0.7891566265  0.7966867470  0.8373493976  0.7921686747  0.7349397590  0.8785607196  0.8862275449  0.7542168675  0.6666666667  2.4096385542  7.0759645271  0.4781041145  50            0.1329876661 
0.7553051408  0.9683734940  0.9457831325  0.6947115385  0.8257756563  0.8704819277  0.8433734940  0.9834586466  0.9820359281  0.9954887218  0.9700598802  0.9337349398  0.9397590361  0.7469879518  0.7831325301  0.9171686747  0.9337349398  0.9430284858  0.9281437126  0.6963855422  0.8043478261  4.8192771084  4.2806668854  0.4781041145  100           0.1353259516 
0.7614345077  0.9638554217  0.9397590361  0.7019230769  0.7923627685  0.8900602410  0.8373493976  0.9684210526  0.9341317365  0.9924812030  0.9700598802  0.9397590361  0.9397590361  0.7665662651  0.8012048193  0.9352409639  0.9096385542  0.9565217391  0.9341317365  0.6987951807  0.8526570048  7.2289156627  3.7162278271  0.4781532288  150           0.1350814629 
0.7638152737  0.9638554217  0.9518072289  0.7091346154  0.7995226730  0.8795180723  0.8313253012  0.9714285714  0.9461077844  0.9924812030  0.9700598802  0.9337349398  0.9518072289  0.7500000000  0.7831325301  0.9292168675  0.9216867470  0.9490254873  0.9281437126  0.7060240964  0.8405797101  9.6385542169  3.7177591515  0.4781532288  200           0.1355081558 
0.7626537321  0.9608433735  0.9397590361  0.6995192308  0.7875894988  0.8885542169  0.8433734940  0.9624060150  0.9341317365  0.9924812030  0.9700598802  0.9472891566  0.9457831325  0.7635542169  0.8012048193  0.9292168675  0.9096385542  0.9655172414  0.9341317365  0.7084337349  0.8550724638  12.048192771  3.7479751587  0.4781627655  250           0.1346913242 
0.7596532552  0.9518072289  0.9397590361  0.6995192308  0.7852028640  0.8945783133  0.8433734940  0.9578947368  0.9281437126  0.9894736842  0.9700598802  0.9412650602  0.9457831325  0.7816265060  0.8132530120  0.9307228916  0.9036144578  0.9685157421  0.9461077844  0.6891566265  0.8647342995  14.457831325  3.6884304285  0.4781627655  300           0.1366154385 
0.7578546179  0.9503012048  0.9337349398  0.6995192308  0.7804295943  0.8930722892  0.8373493976  0.9563909774  0.9221556886  0.9894736842  0.9700598802  0.9427710843  0.9397590361  0.7801204819  0.8072289157  0.9307228916  0.9036144578  0.9685157421  0.9461077844  0.6915662651  0.8599033816  16.867469879  3.6298998547  0.4781627655  350           0.1340896177 
0.7422179642  0.9246987952  0.9036144578  0.6802884615  0.7756563246  0.9246987952  0.8554216867  0.9383458647  0.9221556886  0.9864661654  0.9700598802  0.9246987952  0.9036144578  0.8313253012  0.8433734940  0.9216867470  0.9036144578  0.9790104948  0.9640718563  0.6481927711  0.8647342995  19.277108433  3.6578677368  0.4781627655  400           0.1349877834 
0.7589887211  0.9638554217  0.9518072289  0.7091346154  0.7995226730  0.8885542169  0.8433734940  0.9684210526  0.9341317365  0.9924812030  0.9700598802  0.9397590361  0.9518072289  0.7560240964  0.7951807229  0.9322289157  0.9096385542  0.9520239880  0.9281437126  0.6987951807  0.8285024155  21.686746988  3.5773030519  0.4781627655  450           0.1340708065 
0.7602107932  0.9668674699  0.9518072289  0.7091346154  0.7923627685  0.8825301205  0.8313253012  0.9744360902  0.9341317365  0.9924812030  0.9700598802  0.9427710843  0.9518072289  0.7500000000  0.7831325301  0.9382530120  0.9096385542  0.9520239880  0.9281437126  0.7108433735  0.8285024155  24.096385542  3.6183761644  0.4781627655  500           0.1328372955 
0.7578503291  0.9503012048  0.9337349398  0.7019230769  0.7828162291  0.8975903614  0.8554216867  0.9563909774  0.9221556886  0.9894736842  0.9700598802  0.9427710843  0.9397590361  0.7816265060  0.8132530120  0.9397590361  0.9036144578  0.9715142429  0.9461077844  0.6819277108  0.8647342995  26.506024096  3.6256302214  0.4781627655  550           0.1351482010 
0.7608060038  0.9668674699  0.9518072289  0.7115384615  0.7947494033  0.8825301205  0.8313253012  0.9789473684  0.9520958084  0.9924812030  0.9700598802  0.9397590361  0.9518072289  0.7500000000  0.7831325301  0.9427710843  0.9277108434  0.9520239880  0.9281437126  0.7084337349  0.8285024155  28.915662650  3.5845410299  0.4781627655  600           0.1330922461 
0.7566425996  0.9503012048  0.9337349398  0.7019230769  0.7828162291  0.9006024096  0.8554216867  0.9563909774  0.9221556886  0.9894736842  0.9700598802  0.9457831325  0.9397590361  0.7846385542  0.8132530120  0.9397590361  0.9036144578  0.9715142429  0.9461077844  0.6819277108  0.8599033816  31.325301204  3.5911901140  0.4781627655  650           0.1326004219 
0.7590304397  0.9668674699  0.9518072289  0.7115384615  0.7828162291  0.8885542169  0.8313253012  0.9714285714  0.9341317365  0.9924812030  0.9700598802  0.9518072289  0.9518072289  0.7530120482  0.7831325301  0.9382530120  0.9096385542  0.9550224888  0.9281437126  0.7084337349  0.8333333333  33.734939759  3.6293084192  0.4781794548  700           0.1335148811 
0.7572233217  0.9608433735  0.9397590361  0.7019230769  0.7875894988  0.8990963855  0.8493975904  0.9609022556  0.9281437126  0.9924812030  0.9700598802  0.9533132530  0.9457831325  0.7816265060  0.8132530120  0.9322289157  0.9096385542  0.9715142429  0.9461077844  0.6963855422  0.8429951691  36.144578313  3.6155647421  0.4782032967  750           0.1342095327 
0.7422135919  0.9292168675  0.9096385542  0.6778846154  0.7756563246  0.9262048193  0.8614457831  0.9383458647  0.9221556886  0.9894736842  0.9700598802  0.9292168675  0.9096385542  0.8253012048  0.8433734940  0.9216867470  0.9036144578  0.9820089955  0.9640718563  0.6602409639  0.8550724638  38.554216867  3.5722804546  0.4782161713  800           0.1343456221 
0.7578097463  0.9668674699  0.9518072289  0.7091346154  0.7875894988  0.8885542169  0.8313253012  0.9744360902  0.9341317365  0.9924812030  0.9700598802  0.9518072289  0.9518072289  0.7590361446  0.7951807229  0.9412650602  0.9096385542  0.9565217391  0.9341317365  0.7108433735  0.8236714976  40.963855421  3.5964631176  0.4782309532  850           0.1343654203 
0.7590347910  0.9638554217  0.9518072289  0.7067307692  0.7828162291  0.8915662651  0.8433734940  0.9759398496  0.9520958084  0.9924812030  0.9700598802  0.9487951807  0.9518072289  0.7695783133  0.8012048193  0.9427710843  0.9277108434  0.9625187406  0.9341317365  0.7108433735  0.8357487923  43.373493975  3.5355737448  0.4782309532  900           0.1313673258 
0.7494280680  0.9457831325  0.9277108434  0.6923076923  0.7780429594  0.9156626506  0.8554216867  0.9503759398  0.9221556886  0.9894736842  0.9700598802  0.9412650602  0.9337349398  0.7951807229  0.8192771084  0.9337349398  0.9036144578  0.9790104948  0.9520958084  0.6722891566  0.8550724638  45.783132530  3.5413717556  0.4782309532  950           0.1343412066 
0.7548612706  0.9457831325  0.9277108434  0.6971153846  0.7732696897  0.8975903614  0.8433734940  0.9533834586  0.9221556886  0.9894736842  0.9700598802  0.9412650602  0.9337349398  0.7906626506  0.8132530120  0.9367469880  0.9036144578  0.9760119940  0.9520958084  0.6891566265  0.8599033816  48.192771084  3.5218234634  0.4782309532  1000          0.1337513065 
0.7578140491  0.9698795181  0.9518072289  0.7115384615  0.7852028640  0.8855421687  0.8313253012  0.9789473684  0.9520958084  0.9939849624  0.9760479042  0.9518072289  0.9518072289  0.7530120482  0.7831325301  0.9457831325  0.9277108434  0.9610194903  0.9401197605  0.7108433735  0.8236714976  50.602409638  3.6104478693  0.4782309532  1050          0.1316900253 
0.7536117671  0.9548192771  0.9397590361  0.6995192308  0.7875894988  0.9096385542  0.8554216867  0.9609022556  0.9281437126  0.9924812030  0.9700598802  0.9472891566  0.9457831325  0.7846385542  0.8132530120  0.9292168675  0.9096385542  0.9745127436  0.9461077844  0.6819277108  0.8454106280  53.012048192  3.5506528282  0.4782309532  1100          0.1321716213 
0.7530193348  0.9608433735  0.9397590361  0.6995192308  0.7804295943  0.9021084337  0.8493975904  0.9609022556  0.9281437126  0.9924812030  0.9700598802  0.9533132530  0.9457831325  0.7831325301  0.8192771084  0.9352409639  0.9096385542  0.9745127436  0.9461077844  0.6987951807  0.8333333333  55.421686747  3.5622294235  0.4782309532  1150          0.1359405804 
0.7542270642  0.9623493976  0.9337349398  0.6995192308  0.7804295943  0.9036144578  0.8433734940  0.9609022556  0.9281437126  0.9924812030  0.9700598802  0.9518072289  0.9397590361  0.7846385542  0.8132530120  0.9352409639  0.9096385542  0.9745127436  0.9461077844  0.6987951807  0.8381642512  57.831325301  3.5155715036  0.4782309532  1200          0.1347856045 
0.7542067833  0.9668674699  0.9397590361  0.7067307692  0.7828162291  0.8960843373  0.8493975904  0.9744360902  0.9341317365  0.9924812030  0.9700598802  0.9563253012  0.9457831325  0.7710843373  0.8072289157  0.9442771084  0.9096385542  0.9700149925  0.9401197605  0.7060240964  0.8212560386  60.240963855  3.5296766758  0.4782309532  1250          0.1348946762 
0.7542198722  0.9578313253  0.9397590361  0.7043269231  0.7828162291  0.9111445783  0.8493975904  0.9609022556  0.9281437126  0.9924812030  0.9700598802  0.9503012048  0.9457831325  0.7906626506  0.8132530120  0.9352409639  0.9096385542  0.9775112444  0.9461077844  0.6891566265  0.8405797101  62.650602409  3.4828328514  0.4782309532  1300          0.1340153122 
0.7602294456  0.9759036145  0.9518072289  0.7139423077  0.7828162291  0.8945783133  0.8313253012  0.9819548872  0.9640718563  0.9939849624  0.9760479042  0.9548192771  0.9518072289  0.7590361446  0.7951807229  0.9457831325  0.9397590361  0.9610194903  0.9401197605  0.7180722892  0.8260869565  65.060240963  3.4584815598  0.4782309532  1350          0.1336836386 
0.7524140844  0.9563253012  0.9337349398  0.6995192308  0.7828162291  0.9111445783  0.8493975904  0.9609022556  0.9281437126  0.9924812030  0.9700598802  0.9487951807  0.9397590361  0.7936746988  0.8132530120  0.9352409639  0.9096385542  0.9775112444  0.9461077844  0.6891566265  0.8381642512  67.469879518  3.5159337854  0.4782309532  1400          0.1342408943 
0.7554275947  0.9593373494  0.9337349398  0.7019230769  0.7828162291  0.9036144578  0.8433734940  0.9609022556  0.9281437126  0.9924812030  0.9700598802  0.9487951807  0.9397590361  0.7876506024  0.8132530120  0.9442771084  0.9096385542  0.9790104948  0.9520958084  0.6939759036  0.8429951691  69.879518072  3.4794860506  0.4782314301  1450          0.1345780754 
0.7602078276  0.9759036145  0.9518072289  0.7139423077  0.7899761337  0.8885542169  0.8313253012  0.9819548872  0.9640718563  0.9939849624  0.9760479042  0.9548192771  0.9518072289  0.7560240964  0.7831325301  0.9457831325  0.9397590361  0.9580209895  0.9401197605  0.7180722892  0.8188405797  72.289156626  3.4874990463  0.4782314301  1500          0.1340107822 
0.7463929951  0.9442771084  0.9216867470  0.6850961538  0.7875894988  0.9277108434  0.8674698795  0.9503759398  0.9221556886  0.9894736842  0.9700598802  0.9412650602  0.9337349398  0.8162650602  0.8433734940  0.9292168675  0.8975903614  0.9835082459  0.9700598802  0.6650602410  0.8478260870  74.698795180  3.4962508154  0.4782314301  1550          0.1335729742 
0.7553942874  0.9759036145  0.9518072289  0.7091346154  0.7875894988  0.8990963855  0.8373493976  0.9819548872  0.9520958084  0.9939849624  0.9760479042  0.9578313253  0.9518072289  0.7665662651  0.7891566265  0.9487951807  0.9277108434  0.9685157421  0.9461077844  0.7108433735  0.8140096618  77.108433734  3.5086897898  0.4782314301  1600          0.1352164412 
0.7566423983  0.9668674699  0.9397590361  0.7043269231  0.7756563246  0.9021084337  0.8493975904  0.9729323308  0.9281437126  0.9924812030  0.9700598802  0.9533132530  0.9457831325  0.7771084337  0.8072289157  0.9472891566  0.9096385542  0.9745127436  0.9461077844  0.7132530120  0.8333333333  79.518072289  3.4681307316  0.4782314301  1650          0.1331914473 
0.7565558775  0.9728915663  0.9518072289  0.7115384615  0.8019093079  0.8930722892  0.8373493976  0.9819548872  0.9520958084  0.9939849624  0.9760479042  0.9578313253  0.9518072289  0.7620481928  0.7831325301  0.9487951807  0.9277108434  0.9640179910  0.9401197605  0.7108433735  0.8019323671  81.927710843  3.4087010717  0.4782314301  1700          0.1328715038 
0.7566178215  0.9759036145  0.9518072289  0.7115384615  0.7804295943  0.8975903614  0.8313253012  0.9819548872  0.9640718563  0.9939849624  0.9760479042  0.9578313253  0.9518072289  0.7605421687  0.7891566265  0.9503012048  0.9457831325  0.9715142429  0.9461077844  0.7156626506  0.8188405797  84.337349397  3.4292429543  0.4782314301  1750          0.1356956768 
0.7614183423  0.9789156627  0.9518072289  0.7187500000  0.7852028640  0.8870481928  0.8253012048  0.9879699248  0.9760479042  0.9939849624  0.9760479042  0.9548192771  0.9518072289  0.7500000000  0.7831325301  0.9533132530  0.9578313253  0.9640179910  0.9401197605  0.7277108434  0.8140096618  86.746987951  3.4826696873  0.4782314301  1800          0.1337158680 
0.7542184101  0.9578313253  0.9277108434  0.7019230769  0.7828162291  0.9066265060  0.8433734940  0.9563909774  0.9221556886  0.9924812030  0.9700598802  0.9472891566  0.9337349398  0.7876506024  0.8132530120  0.9427710843  0.9036144578  0.9790104948  0.9520958084  0.6963855422  0.8357487923  89.156626506  3.4641225719  0.4782314301  1850          0.1360124731 
0.7332220691  0.9201807229  0.8734939759  0.6778846154  0.7589498807  0.9457831325  0.8674698795  0.9278195489  0.9161676647  0.9819548872  0.9640718563  0.9231927711  0.8855421687  0.8448795181  0.8614457831  0.9141566265  0.8975903614  0.9760119940  0.9640718563  0.6337349398  0.8623188406  91.566265060  3.4337242842  0.4782314301  1900          0.1339064789 
0.7416053829  0.9307228916  0.9036144578  0.6850961538  0.7756563246  0.9322289157  0.8734939759  0.9413533835  0.9221556886  0.9894736842  0.9700598802  0.9337349398  0.9156626506  0.8388554217  0.8493975904  0.9277108434  0.9036144578  0.9835082459  0.9700598802  0.6530120482  0.8526570048  93.975903614  3.4825673056  0.4782314301  1950          0.1344162273 
0.7578254955  0.9789156627  0.9518072289  0.7163461538  0.7780429594  0.8975903614  0.8433734940  0.9849624060  0.9760479042  0.9939849624  0.9760479042  0.9578313253  0.9518072289  0.7665662651  0.8012048193  0.9503012048  0.9578313253  0.9715142429  0.9461077844  0.7180722892  0.8188405797  96.385542168  3.4066784143  0.4782314301  2000          0.1332100296 
0.7595996045  0.9789156627  0.9518072289  0.7163461538  0.7899761337  0.8930722892  0.8253012048  0.9879699248  0.9760479042  0.9969924812  0.9760479042  0.9548192771  0.9518072289  0.7530120482  0.7831325301  0.9533132530  0.9578313253  0.9640179910  0.9401197605  0.7204819277  0.8115942029  98.795180722  3.3579610205  0.4782314301  2050          0.1350860119 
0.7572345878  0.9789156627  0.9518072289  0.7163461538  0.7732696897  0.8975903614  0.8433734940  0.9849624060  0.9640718563  0.9939849624  0.9760479042  0.9593373494  0.9578313253  0.7665662651  0.8012048193  0.9533132530  0.9457831325  0.9745127436  0.9461077844  0.7204819277  0.8188405797  101.20481927  3.4203499460  0.4782314301  2100          0.1333092070 
0.7529831383  0.9759036145  0.9518072289  0.7139423077  0.7852028640  0.9051204819  0.8493975904  0.9819548872  0.9520958084  0.9939849624  0.9760479042  0.9623493976  0.9578313253  0.7680722892  0.8072289157  0.9563253012  0.9337349398  0.9745127436  0.9461077844  0.7060240964  0.8067632850  103.61445783  3.4189663839  0.4782314301  2150          0.1348527098 
0.7542038177  0.9789156627  0.9518072289  0.7115384615  0.7804295943  0.9021084337  0.8493975904  0.9849624060  0.9640718563  0.9939849624  0.9760479042  0.9623493976  0.9578313253  0.7635542169  0.7891566265  0.9563253012  0.9457831325  0.9745127436  0.9461077844  0.7132530120  0.8115942029  106.02409638  3.3970355892  0.4782314301  2200          0.1335907793 
0.7595764759  0.9804216867  0.9578313253  0.7211538462  0.7947494033  0.8840361446  0.8253012048  0.9894736842  0.9820359281  0.9969924812  0.9760479042  0.9563253012  0.9578313253  0.7424698795  0.7771084337  0.9487951807  0.9638554217  0.9610194903  0.9401197605  0.7253012048  0.7971014493  108.43373493  3.4115730333  0.4782314301  2250          0.1330539846 
0.7596082032  0.9804216867  0.9578313253  0.7187500000  0.7852028640  0.8900602410  0.8253012048  0.9894736842  0.9820359281  0.9969924812  0.9760479042  0.9563253012  0.9578313253  0.7500000000  0.7831325301  0.9472891566  0.9578313253  0.9685157421  0.9461077844  0.7253012048  0.8091787440  110.84337349  3.3879354239  0.4782314301  2300          0.1326950502 
0.7583816479  0.9864457831  0.9578313253  0.7259615385  0.7875894988  0.8780120482  0.8253012048  0.9894736842  0.9820359281  0.9969924812  0.9760479042  0.9563253012  0.9578313253  0.7334337349  0.7771084337  0.9457831325  0.9518072289  0.9595202399  0.9341317365  0.7301204819  0.7898550725  113.25301204  3.3899499369  0.4782314301  2350          0.1336969709 
0.7536014705  0.9759036145  0.9518072289  0.7091346154  0.7828162291  0.9081325301  0.8493975904  0.9789473684  0.9520958084  0.9939849624  0.9760479042  0.9593373494  0.9578313253  0.7786144578  0.8132530120  0.9548192771  0.9397590361  0.9775112444  0.9580838323  0.7036144578  0.8188405797  115.66265060  3.3524483252  0.4782314301  2400          0.1320825863 
0.7397793766  0.9367469880  0.9156626506  0.6850961538  0.7804295943  0.9352409639  0.8734939759  0.9443609023  0.9221556886  0.9924812030  0.9700598802  0.9367469880  0.9277108434  0.8388554217  0.8493975904  0.9277108434  0.9036144578  0.9865067466  0.9700598802  0.6530120482  0.8405797101  118.07228915  3.3370334387  0.4782314301  2450          0.1352171326 
0.7493888294  0.9789156627  0.9518072289  0.7091346154  0.7780429594  0.9051204819  0.8493975904  0.9819548872  0.9640718563  0.9939849624  0.9760479042  0.9623493976  0.9578313253  0.7756024096  0.8132530120  0.9578313253  0.9518072289  0.9775112444  0.9580838323  0.7036144578  0.8067632850  120.48192771  3.2826770210  0.4782600403  2500          0.1341323757 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3614197531  0.3395061728  0.3333333333  0.3209876543  0.3765432099  0.3333333333  0.3580246914  0.3472222222  0.3827160494  0.3469135802  0.3666666667  0.3814814815  0.3506172840  0.0000000000  4.4635930061  0.1364407539  0             0.3923821449 
0.6978395062  0.8364197531  0.8703703704  0.7206790123  0.7469135802  0.7546296296  0.7839506173  0.7793209877  0.8086419753  0.6913580247  0.6925925926  0.7049382716  0.7024691358  2.4691358025  3.0883390617  0.3011631966  50            0.0761919594 
0.7919753086  0.8796296296  0.8518518519  0.7777777778  0.7469135802  0.8348765432  0.8703703704  0.8487654321  0.8703703704  0.7876543210  0.7913580247  0.7901234568  0.7987654321  4.9382716049  2.0962218475  0.3011631966  100           0.0750472546 
0.7719135802  0.8904320988  0.8580246914  0.7839506173  0.7654320988  0.8503086420  0.8765432099  0.8595679012  0.8888888889  0.7530864198  0.7777777778  0.7716049383  0.7851851852  7.4074074074  1.9075517082  0.3011631966  150           0.0757959795 
0.7756172840  0.8904320988  0.8580246914  0.7901234568  0.7654320988  0.8657407407  0.8765432099  0.8611111111  0.8888888889  0.7580246914  0.7777777778  0.7728395062  0.7938271605  9.8765432099  1.8981406617  0.3011846542  200           0.0746321106 
0.7456790123  0.8966049383  0.8580246914  0.7962962963  0.7716049383  0.8518518519  0.8827160494  0.8672839506  0.8888888889  0.7419753086  0.7481481481  0.7395061728  0.7530864198  12.345679012  1.9236002874  0.3011846542  250           0.0761716890 
0.7330246914  0.9043209877  0.8580246914  0.8040123457  0.7592592593  0.8395061728  0.8703703704  0.8672839506  0.8765432099  0.7222222222  0.7333333333  0.7283950617  0.7481481481  14.814814814  1.9230098176  0.3011846542  300           0.0764351416 
0.7783950617  0.8904320988  0.8518518519  0.7993827160  0.7839506173  0.8750000000  0.8888888889  0.8780864198  0.8950617284  0.7654320988  0.7802469136  0.7679012346  0.8000000000  17.283950617  1.9196118283  0.3011846542  350           0.0752557516 
0.7401234568  0.9089506173  0.8703703704  0.8101851852  0.7654320988  0.8641975309  0.8703703704  0.8703703704  0.8827160494  0.7296296296  0.7419753086  0.7296296296  0.7592592593  19.753086419  1.9411513472  0.3012022972  400           0.0759936428 
0.7685185185  0.8919753086  0.8580246914  0.8132716049  0.7901234568  0.8827160494  0.9012345679  0.8842592593  0.9012345679  0.7629629630  0.7716049383  0.7493827160  0.7901234568  22.222222222  1.9284670162  0.3012022972  450           0.0768510437 
0.7456790123  0.9058641975  0.8703703704  0.8148148148  0.7716049383  0.8765432099  0.8765432099  0.8796296296  0.8827160494  0.7358024691  0.7506172840  0.7320987654  0.7641975309  24.691358024  1.8763621140  0.3012022972  500           0.0758864069 
0.7277777778  0.9058641975  0.8641975309  0.8132716049  0.7530864198  0.8472222222  0.8580246914  0.8672839506  0.8765432099  0.7172839506  0.7271604938  0.7271604938  0.7395061728  27.160493827  1.9003958464  0.3012022972  550           0.0773713255 
0.7345679012  0.9089506173  0.8641975309  0.8209876543  0.7592592593  0.8595679012  0.8703703704  0.8672839506  0.8765432099  0.7259259259  0.7345679012  0.7283950617  0.7493827160  29.629629629  1.9371938634  0.3012022972  600           0.0775367165 
0.7580246914  0.8966049383  0.8641975309  0.8101851852  0.7839506173  0.8827160494  0.9012345679  0.8858024691  0.8950617284  0.7543209877  0.7580246914  0.7432098765  0.7765432099  32.098765432  1.8895581055  0.3012022972  650           0.0798203754 
0.7570987654  0.8981481481  0.8641975309  0.8101851852  0.7839506173  0.8780864198  0.9012345679  0.8858024691  0.8950617284  0.7518518519  0.7580246914  0.7432098765  0.7753086420  34.567901234  1.8762087393  0.3012022972  700           0.0790781498 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
Traceback (most recent call last):
KeyboardInterrupt
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2683055) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3614197531  0.3395061728  0.3333333333  0.3209876543  0.3765432099  0.3333333333  0.3580246914  0.3472222222  0.3827160494  0.3469135802  0.3666666667  0.3814814815  0.3506172840  0.0000000000  4.4635930061  0.1364407539  0             0.4894378185 
0.6978395062  0.8364197531  0.8703703704  0.7206790123  0.7469135802  0.7546296296  0.7839506173  0.7793209877  0.8086419753  0.6913580247  0.6925925926  0.7049382716  0.7024691358  2.4691358025  3.0883390617  0.3012309074  50            0.0783401489 
0.7919753086  0.8796296296  0.8518518519  0.7777777778  0.7469135802  0.8348765432  0.8703703704  0.8487654321  0.8703703704  0.7876543210  0.7913580247  0.7901234568  0.7987654321  4.9382716049  2.0962218475  0.3012371063  100           0.0778252506 
0.7787037037  0.9290123457  0.9012345679  0.8456790123  0.8086419753  0.9166666667  0.9074074074  0.9305555556  0.9259259259  0.7716049383  0.7851851852  0.7629629630  0.7950617284  7.4074074074  1.8059664106  0.3012371063  150           0.0771238613 
0.8308641975  0.9783950617  0.9567901235  0.8919753086  0.8456790123  0.9351851852  0.9074074074  0.9351851852  0.9197530864  0.8197530864  0.8259259259  0.8160493827  0.8617283951  9.8765432099  1.6274700379  0.3012433052  200           0.0760371590 
0.8570987654  0.9861111111  0.9753086420  0.9506172840  0.8827160494  0.9706790123  0.9320987654  0.9444444444  0.9382716049  0.8530864198  0.8518518519  0.8456790123  0.8777777778  12.345679012  1.4722266364  0.3012647629  250           0.0757125759 
0.8290123457  0.9969135802  0.9876543210  0.9598765432  0.8950617284  0.9660493827  0.9197530864  0.9598765432  0.9320987654  0.8197530864  0.8296296296  0.8234567901  0.8432098765  14.814814814  1.3322521305  0.3012647629  300           0.0747616768 
0.8089506173  0.9938271605  0.9814814815  0.9953703704  0.9506172840  0.9861111111  0.9506172840  0.9891975309  0.9382716049  0.7728395062  0.8148148148  0.8160493827  0.8320987654  17.283950617  1.2219561207  0.3012862206  350           0.0758212042 
0.8376543210  0.9984567901  0.9938271605  0.9814814815  0.9259259259  0.9814814815  0.9259259259  0.9753086420  0.9382716049  0.8234567901  0.8419753086  0.8271604938  0.8580246914  19.753086419  1.1308991921  0.3012862206  400           0.0762105799 
0.8435185185  0.9984567901  0.9938271605  0.9953703704  0.9691358025  0.9891975309  0.9506172840  0.9938271605  0.9382716049  0.8246913580  0.8518518519  0.8370370370  0.8604938272  22.222222222  1.0885524404  0.3012862206  450           0.0776162148 
0.8151234568  0.9938271605  0.9814814815  0.9984567901  0.9691358025  0.9969135802  0.9506172840  0.9953703704  0.9629629630  0.8086419753  0.8185185185  0.8074074074  0.8259259259  24.691358024  1.0083309042  0.3012862206  500           0.0764074087 
0.8537037037  1.0000000000  0.9938271605  0.9922839506  0.9444444444  0.9938271605  0.9382716049  0.9876543210  0.9629629630  0.8555555556  0.8469135802  0.8382716049  0.8740740741  27.160493827  0.9967939913  0.3012862206  550           0.0763620329 
0.8197530864  0.9984567901  0.9814814815  1.0000000000  0.9753086420  0.9984567901  0.9567901235  1.0000000000  0.9814814815  0.8123456790  0.8234567901  0.8086419753  0.8345679012  29.629629629  0.9799810910  0.3012862206  600           0.0752507162 
0.8527777778  1.0000000000  0.9876543210  1.0000000000  0.9629629630  0.9984567901  0.9506172840  0.9969135802  0.9567901235  0.8469135802  0.8543209877  0.8444444444  0.8654320988  32.098765432  0.9321907818  0.3012862206  650           0.0760498810 
0.8635802469  1.0000000000  0.9938271605  0.9984567901  0.9691358025  0.9953703704  0.9567901235  0.9984567901  0.9567901235  0.8592592593  0.8617283951  0.8506172840  0.8827160494  34.567901234  0.9183243203  0.3012862206  700           0.0767173386 
0.8441358025  1.0000000000  0.9876543210  1.0000000000  0.9691358025  1.0000000000  0.9506172840  1.0000000000  0.9691358025  0.8308641975  0.8481481481  0.8370370370  0.8604938272  37.037037037  0.9415852404  0.3012862206  750           0.0764195776 
0.8351851852  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9691358025  0.8234567901  0.8358024691  0.8308641975  0.8506172840  39.506172839  0.8987065518  0.3012862206  800           0.0763785124 
0.8385802469  1.0000000000  0.9876543210  1.0000000000  0.9691358025  1.0000000000  0.9506172840  1.0000000000  0.9629629630  0.8259259259  0.8419753086  0.8259259259  0.8604938272  41.975308642  0.8881556404  0.3012862206  850           0.0759760380 
0.8148148148  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.7987654321  0.8160493827  0.8123456790  0.8320987654  44.444444444  0.8701032805  0.3012862206  900           0.0762981319 
0.8533950617  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9567901235  1.0000000000  0.9691358025  0.8333333333  0.8567901235  0.8432098765  0.8802469136  46.913580246  0.8488856733  0.3012862206  950           0.0764241552 
0.8209876543  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8012345679  0.8209876543  0.8172839506  0.8444444444  49.382716049  0.8476286089  0.3012862206  1000          0.0758697605 
0.8419753086  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9567901235  1.0000000000  0.9691358025  0.8271604938  0.8444444444  0.8333333333  0.8629629630  51.851851851  0.8419999278  0.3012862206  1050          0.0776947260 
0.8382716049  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9506172840  1.0000000000  0.9753086420  0.8234567901  0.8395061728  0.8333333333  0.8567901235  54.320987654  0.8433032465  0.3012862206  1100          0.0766410923 
0.8438271605  1.0000000000  0.9876543210  1.0000000000  0.9691358025  1.0000000000  0.9506172840  1.0000000000  0.9753086420  0.8308641975  0.8469135802  0.8370370370  0.8604938272  56.790123456  0.8270936275  0.3012862206  1150          0.0760133505 
0.8271604938  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8086419753  0.8283950617  0.8234567901  0.8481481481  59.259259259  0.8110461676  0.3012862206  1200          0.0767718220 
0.8447530864  1.0000000000  0.9876543210  1.0000000000  0.9691358025  1.0000000000  0.9444444444  1.0000000000  0.9753086420  0.8320987654  0.8481481481  0.8345679012  0.8641975309  61.728395061  0.8089165890  0.3862781525  1250          0.0769097233 
0.8342592593  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8135802469  0.8370370370  0.8308641975  0.8555555556  64.197530864  0.7838160145  0.3862781525  1300          0.0765607119 
0.8515432099  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9567901235  1.0000000000  0.9753086420  0.8395061728  0.8506172840  0.8432098765  0.8728395062  66.666666666  0.7958446717  0.3862781525  1350          0.0763773203 
0.8305555556  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8123456790  0.8320987654  0.8246913580  0.8530864198  69.135802469  0.7593321097  0.3862781525  1400          0.0761959076 
0.8327160494  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8123456790  0.8370370370  0.8271604938  0.8543209877  71.604938271  0.7735586429  0.3862781525  1450          0.0760881424 
0.8225308642  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8049382716  0.8234567901  0.8209876543  0.8407407407  74.074074074  0.7606322372  0.3862781525  1500          0.0758800554 
0.8444444444  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9506172840  1.0000000000  0.9814814815  0.8345679012  0.8345679012  0.8358024691  0.8728395062  76.543209876  0.7830214608  0.3862781525  1550          0.0767610931 
0.8280864198  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8061728395  0.8320987654  0.8271604938  0.8469135802  79.012345679  0.7462381935  0.3862781525  1600          0.0740760136 
0.8299382716  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8160493827  0.8345679012  0.8185185185  0.8506172840  81.481481481  0.7470043898  0.3862781525  1650          0.0758255053 
0.8404320988  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8209876543  0.8419753086  0.8382716049  0.8604938272  83.950617284  0.7507064867  0.3862781525  1700          0.0765642881 
0.8293209877  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9506172840  1.0000000000  0.9814814815  0.8185185185  0.8320987654  0.8172839506  0.8493827160  86.419753086  0.7290104997  0.3862781525  1750          0.0754585695 
0.8345679012  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8185185185  0.8333333333  0.8320987654  0.8543209877  88.888888888  0.7033142531  0.3862781525  1800          0.0753452730 
0.8268518519  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8000000000  0.8283950617  0.8271604938  0.8518518519  91.358024691  0.7285627997  0.3862781525  1850          0.0750947285 
0.8398148148  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8271604938  0.8370370370  0.8308641975  0.8641975309  93.827160493  0.7200093353  0.3862781525  1900          0.0756733561 
0.8163580247  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.7938271605  0.8197530864  0.8172839506  0.8345679012  96.296296296  0.7077641201  0.3862781525  1950          0.0762112379 
0.8351851852  1.0000000000  0.9938271605  1.0000000000  0.9814814815  1.0000000000  0.9506172840  1.0000000000  0.9876543210  0.8197530864  0.8345679012  0.8283950617  0.8580246914  98.765432098  0.6930775774  0.3862781525  2000          0.0760411453 
0.8246913580  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8000000000  0.8320987654  0.8222222222  0.8444444444  101.23456790  0.7145988286  0.3862781525  2050          0.0759036636 
0.8351851852  1.0000000000  0.9938271605  1.0000000000  0.9691358025  1.0000000000  0.9444444444  1.0000000000  0.9753086420  0.8209876543  0.8370370370  0.8246913580  0.8580246914  103.70370370  0.6917402518  0.3862781525  2100          0.0760780144 
0.8598765432  1.0000000000  0.9938271605  1.0000000000  0.9753086420  1.0000000000  0.9506172840  1.0000000000  0.9691358025  0.8382716049  0.8617283951  0.8506172840  0.8888888889  106.17283950  0.6788501942  0.3862781525  2150          0.0760193586 
0.8305555556  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8049382716  0.8333333333  0.8345679012  0.8493827160  108.64197530  0.6946079338  0.3862781525  2200          0.0773757219 
0.8416666667  1.0000000000  0.9938271605  1.0000000000  0.9753086420  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8222222222  0.8444444444  0.8320987654  0.8679012346  111.11111111  0.6686321580  0.3862781525  2250          0.0764583111 
0.8250000000  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8000000000  0.8296296296  0.8271604938  0.8432098765  113.58024691  0.6907561040  0.3862781525  2300          0.0771140814 
0.8320987654  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8160493827  0.8296296296  0.8271604938  0.8555555556  116.04938271  0.6713706589  0.3862781525  2350          0.0764217997 
0.8265432099  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.7925925926  0.8296296296  0.8358024691  0.8481481481  118.51851851  0.6782000792  0.3862781525  2400          0.0759693098 
0.8225308642  1.0000000000  0.9938271605  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8074074074  0.8271604938  0.8135802469  0.8419753086  120.98765432  0.6695310354  0.3862781525  2450          0.0759787416 
0.8191358025  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9506172840  1.0000000000  0.9938271605  0.7913580247  0.8271604938  0.8197530864  0.8382716049  123.45679012  0.6863940167  0.3862781525  2500          0.0751929760 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
C->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3391975309  0.3556618820  0.3503184713  0.3518518519  0.3765432099  0.3472222222  0.3271604938  0.3657407407  0.3209876543  0.3296296296  0.3419753086  0.3259259259  0.3592592593  0.0000000000  4.3085784912  0.1364407539  0             0.3985698223 
0.6709876543  0.8532695375  0.7961783439  0.7685185185  0.7283950617  0.8132716049  0.7716049383  0.8040123457  0.7716049383  0.6864197531  0.6444444444  0.6802469136  0.6728395062  2.5518341308  3.0757764101  0.3011631966  50            0.0757462645 
0.6984567901  0.9505582137  0.9299363057  0.8580246914  0.8271604938  0.8672839506  0.8148148148  0.8441358025  0.8209876543  0.7123456790  0.6629629630  0.7086419753  0.7098765432  5.1036682616  2.0209615564  0.3011631966  100           0.0793897486 
0.6731481481  0.9505582137  0.9235668790  0.9444444444  0.9259259259  0.9567901235  0.9012345679  0.9074074074  0.9074074074  0.6740740741  0.6580246914  0.6703703704  0.6901234568  7.6555023923  1.5228408456  0.3011631966  150           0.0863972902 
0.6206790123  0.9505582137  0.9235668790  0.9521604938  0.9320987654  0.9722222222  0.9259259259  0.9398148148  0.9320987654  0.6148148148  0.6283950617  0.6234567901  0.6160493827  10.207336523  1.2927295947  0.3011631966  200           0.0768755960 
0.6348765432  0.9601275917  0.9426751592  0.9722222222  0.9444444444  0.9830246914  0.9506172840  0.9614197531  0.9444444444  0.6320987654  0.6370370370  0.6358024691  0.6345679012  12.759170653  1.1665917242  0.3011631966  250           0.0771922493 
0.6135802469  0.9728867624  0.9426751592  0.9799382716  0.9506172840  0.9891975309  0.9506172840  0.9567901235  0.9382716049  0.6037037037  0.6160493827  0.6135802469  0.6209876543  15.311004784  1.1162633514  0.3011631966  300           0.0775638008 
0.5861111111  0.9649122807  0.9490445860  0.9938271605  0.9629629630  0.9953703704  0.9320987654  0.9876543210  0.9753086420  0.5802469136  0.5876543210  0.5790123457  0.5975308642  17.862838915  1.0791345012  0.3011631966  350           0.0783989382 
0.5867283951  0.9808612440  0.9490445860  0.9969135802  0.9753086420  0.9969135802  0.9444444444  0.9922839506  0.9691358025  0.5839506173  0.5839506173  0.5802469136  0.5987654321  20.414673046  1.0506717730  0.3832688332  400           0.0770641661 
0.6296296296  0.9760765550  0.9554140127  0.9984567901  0.9691358025  1.0000000000  0.9629629630  0.9969135802  0.9753086420  0.6234567901  0.6333333333  0.6271604938  0.6345679012  22.966507177  1.0041277993  0.3832688332  450           0.0795516825 
0.6083333333  0.9824561404  0.9554140127  1.0000000000  0.9876543210  0.9984567901  0.9691358025  0.9953703704  0.9691358025  0.6024691358  0.6148148148  0.5987654321  0.6172839506  25.518341307  0.9874303973  0.3832688332  500           0.0798102713 
0.5750000000  0.9936204147  0.9490445860  0.9969135802  0.9753086420  1.0000000000  0.9506172840  0.9953703704  0.9691358025  0.5666666667  0.5790123457  0.5703703704  0.5839506173  28.070175438  0.9482454026  0.3832688332  550           0.0842195415 
0.6151234568  0.9856459330  0.9490445860  1.0000000000  0.9814814815  1.0000000000  0.9629629630  0.9984567901  0.9814814815  0.6222222222  0.6135802469  0.6012345679  0.6234567901  30.622009569  0.9249270642  0.3832688332  600           0.0819368410 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2317, in update
    temp_old_featurizer = networks.CNN(pretrained=False, in_channel=self.input_shape[0], out_channel=self.num_classes).cuda()
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/networks.py", line 263, in __init__
    nn.Conv1d(64, 64, kernel_size=3),  # 248
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 252, in __init__
    False, _single(0), groups, bias, padding_mode)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 109, in __init__
    self.bias = Parameter(torch.Tensor(out_channels))
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 965, in __setattr__
    self.register_parameter(name, value)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 343, in register_parameter
    elif hasattr(self, name) and name not in self._parameters:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 934, in __getattr__
    def __getattr__(self, name: str) -> Union[Tensor, 'Module']:
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
C->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3407407407  0.3460925040  0.3439490446  0.3456790123  0.3518518519  0.3379629630  0.3333333333  0.3364197531  0.3333333333  0.3444444444  0.3395061728  0.3407407407  0.3382716049  0.0000000000  4.3085784912  0.1364407539  0             0.3938219547 
0.6157407407  0.9170653907  0.9044585987  0.9475308642  0.9444444444  0.9691358025  0.9135802469  0.9429012346  0.9506172840  0.6160493827  0.6259259259  0.6123456790  0.6086419753  2.5518341308  2.1080401754  0.3011693954  50            0.0763056707 
0.5827160494  0.9553429027  0.9299363057  0.9552469136  0.9444444444  0.9691358025  0.9197530864  0.9259259259  0.9567901235  0.5802469136  0.5987654321  0.5740740741  0.5777777778  5.1036682616  1.2663736248  0.3012032509  100           0.0763683271 
0.6614197531  0.9665071770  0.9617834395  0.9876543210  0.9814814815  0.9969135802  0.9567901235  0.9876543210  0.9814814815  0.6592592593  0.6530864198  0.6666666667  0.6666666667  7.6555023923  1.1195592785  0.3012032509  150           0.0755598450 
0.6027777778  0.9904306220  0.9426751592  0.9753086420  0.9259259259  0.9814814815  0.9259259259  0.9537037037  0.9567901235  0.6086419753  0.6148148148  0.5975308642  0.5901234568  10.207336523  0.9991814184  0.3012032509  200           0.0761029959 
0.6336419753  0.9952153110  0.9617834395  0.9799382716  0.9320987654  0.9861111111  0.9506172840  0.9691358025  0.9444444444  0.6320987654  0.6407407407  0.6320987654  0.6296296296  12.759170653  0.8836308217  0.3012032509  250           0.0767612123 
0.5962962963  0.9856459330  0.9554140127  1.0000000000  0.9814814815  1.0000000000  0.9876543210  0.9984567901  0.9814814815  0.5962962963  0.5938271605  0.5950617284  0.6000000000  15.311004784  0.8658234346  0.3012032509  300           0.0764052725 
0.6043209877  0.9760765550  0.9554140127  0.9984567901  0.9753086420  0.9984567901  0.9876543210  1.0000000000  0.9876543210  0.6037037037  0.6074074074  0.6024691358  0.6037037037  17.862838915  0.8221483326  0.3012032509  350           0.0777253103 
0.6373456790  1.0000000000  0.9681528662  1.0000000000  0.9691358025  0.9969135802  0.9753086420  0.9984567901  0.9814814815  0.6407407407  0.6370370370  0.6358024691  0.6358024691  20.414673046  0.8197991920  0.3012032509  400           0.0842754984 
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 113, in get
    return _ForkingPickler.loads(res)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 282, in rebuild_storage_fd
    fd = df.detach()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 487, in Client
    c = SocketClient(address)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 614, in SocketClient
    s.connect(address)
FileNotFoundError: [Errno 2] No such file or directory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 999, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
C->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3416666667  0.3460925040  0.3439490446  0.3456790123  0.3518518519  0.3379629630  0.3333333333  0.3364197531  0.3333333333  0.3456790123  0.3407407407  0.3407407407  0.3395061728  0.0000000000  4.3085784912  0.1364407539  0             0.3959915638 
0.6327160494  0.9170653907  0.9044585987  0.9475308642  0.9444444444  0.9691358025  0.9135802469  0.9429012346  0.9506172840  0.6345679012  0.6419753086  0.6320987654  0.6222222222  2.5518341308  2.1080401754  0.3012309074  50            0.0767024803 
0.6120370370  0.9553429027  0.9299363057  0.9552469136  0.9444444444  0.9691358025  0.9197530864  0.9259259259  0.9567901235  0.6222222222  0.6271604938  0.6037037037  0.5950617284  5.1036682616  1.2663736248  0.3012309074  100           0.0775752974 
0.6925925926  0.9665071770  0.9617834395  0.9876543210  0.9814814815  0.9969135802  0.9567901235  0.9876543210  0.9814814815  0.6925925926  0.7074074074  0.6814814815  0.6888888889  7.6555023923  1.1195592785  0.3012309074  150           0.0788935232 
0.6302469136  0.9904306220  0.9426751592  0.9753086420  0.9259259259  0.9814814815  0.9259259259  0.9537037037  0.9567901235  0.6333333333  0.6419753086  0.6283950617  0.6172839506  10.207336523  0.9991814184  0.3012309074  200           0.0818243408 
0.6672839506  0.9952153110  0.9617834395  0.9799382716  0.9320987654  0.9861111111  0.9506172840  0.9691358025  0.9444444444  0.6679012346  0.6802469136  0.6666666667  0.6543209877  12.759170653  0.8836308217  0.3012309074  250           0.0850355434 
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 113, in get
    return _ForkingPickler.loads(res)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 282, in rebuild_storage_fd
    fd = df.detach()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 487, in Client
    c = SocketClient(address)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 614, in SocketClient
    s.connect(address)
FileNotFoundError: [Errno 2] No such file or directory
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
C->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3438271605  0.3942652330  0.3928571429  0.3641975309  0.3456790123  0.3549382716  0.3456790123  0.3518518519  0.3641975309  0.3432098765  0.3469135802  0.3419753086  0.3432098765  0.0000000000  4.3712167740  0.1364407539  0             0.3878140450 
0.7382716049  0.9767025090  0.9571428571  0.9938271605  0.9753086420  0.9907407407  1.0000000000  0.9706790123  0.9259259259  0.7395061728  0.7296296296  0.7407407407  0.7432098765  2.8673835125  2.2376879334  0.3011631966  50            0.0766324568 
0.7358024691  0.9928315412  0.9714285714  0.9984567901  0.9938271605  0.9984567901  0.9876543210  0.9922839506  0.9938271605  0.7407407407  0.7296296296  0.7382716049  0.7345679012  5.7347670251  1.0775258505  0.3011631966  100           0.0767063093 
0.7700617284  0.9982078853  0.9857142857  0.9984567901  0.9876543210  0.9953703704  1.0000000000  0.9938271605  0.9567901235  0.8308641975  0.7358024691  0.7271604938  0.7864197531  8.6021505376  0.9618386543  0.3011846542  150           0.0760425138 
0.7648148148  0.9982078853  0.9857142857  1.0000000000  0.9753086420  1.0000000000  1.0000000000  0.9953703704  0.9691358025  0.8283950617  0.7432098765  0.7185185185  0.7691358025  11.469534050  0.8753604114  0.3011846542  200           0.0765670204 
0.7750000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  1.0000000000  0.9953703704  0.9567901235  0.8580246914  0.7333333333  0.7209876543  0.7876543210  14.336917562  0.8588654625  0.3011846542  250           0.0759951162 
0.7759259259  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8469135802  0.7444444444  0.7246913580  0.7876543210  17.204301075  0.7822983921  0.3011846542  300           0.0761993408 
0.7635802469  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8074074074  0.7345679012  0.7382716049  0.7740740741  20.071684587  0.7739704704  0.3011846542  350           0.0847530985 
0.7253086420  0.9964157706  0.9642857143  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9814814815  0.7308641975  0.7234567901  0.7308641975  0.7160493827  22.939068100  0.7781579041  0.3011846542  400           0.0764829350 
0.7422839506  1.0000000000  0.9785714286  0.9984567901  0.9876543210  0.9984567901  0.9876543210  1.0000000000  0.9814814815  0.7802469136  0.7111111111  0.7271604938  0.7506172840  25.806451612  0.7629600263  0.3011846542  450           0.0771766376 
0.7956790123  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  1.0000000000  0.9969135802  0.9506172840  0.8876543210  0.7530864198  0.7456790123  0.7962962963  28.673835125  0.7614406919  0.3011846542  500           0.0767269087 
0.7722222222  1.0000000000  0.9928571429  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8246913580  0.7358024691  0.7296296296  0.7987654321  31.541218638  0.7315087080  0.3011846542  550           0.0770883703 
0.7836419753  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8432098765  0.7506172840  0.7481481481  0.7925925926  34.408602150  0.7500083518  0.3832688332  600           0.0772186565 
0.7515432099  1.0000000000  0.9928571429  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.7950617284  0.7209876543  0.7185185185  0.7716049383  37.275985663  0.7275730169  0.3832688332  650           0.0815838575 
0.7675925926  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8086419753  0.7395061728  0.7382716049  0.7839506173  40.143369175  0.7066392517  0.3832688332  700           0.0826277113 
0.7716049383  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8530864198  0.7246913580  0.7234567901  0.7851851852  43.010752688  0.7399837506  0.3832688332  750           0.0878889799 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2346, in update
    meta_loss_held_out.backward()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
C->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3342592593  0.3118279570  0.3285714286  0.3379629630  0.3333333333  0.3194444444  0.3086419753  0.3395061728  0.3209876543  0.3271604938  0.3395061728  0.3172839506  0.3530864198  0.0000000000  4.3712167740  0.1364407539  0             0.4259793758 
0.7388888889  0.8207885305  0.7428571429  0.7793209877  0.7345679012  0.7700617284  0.7222222222  0.7546296296  0.6543209877  0.7888888889  0.6925925926  0.7333333333  0.7407407407  2.8673835125  3.0668240404  0.3011631966  50            0.0817146015 
0.7891975309  0.9946236559  0.9785714286  0.8904320988  0.8456790123  0.8811728395  0.8395061728  0.8811728395  0.8024691358  0.7962962963  0.7493827160  0.7901234568  0.8209876543  5.7347670251  1.9734485245  0.3011693954  100           0.0784411669 
0.7716049383  0.9892473118  0.9571428571  0.9922839506  0.9753086420  0.9861111111  0.9938271605  0.9675925926  0.9074074074  0.7827160494  0.7419753086  0.7604938272  0.8012345679  8.6021505376  1.4839604044  0.3011908531  150           0.0798602057 
0.7824074074  0.9964157706  0.9714285714  0.9969135802  0.9753086420  0.9922839506  1.0000000000  0.9614197531  0.9135802469  0.8160493827  0.7506172840  0.7456790123  0.8172839506  11.469534050  1.1169268560  0.3012371063  200           0.0771108294 
0.7716049383  0.9946236559  0.9642857143  0.9984567901  0.9691358025  0.9984567901  0.9938271605  0.9969135802  0.9567901235  0.8197530864  0.7382716049  0.7407407407  0.7876543210  14.336917562  1.0413183749  0.3012371063  250           0.0771198606 
0.7719135802  0.9982078853  0.9785714286  0.9984567901  0.9814814815  0.9984567901  0.9938271605  0.9969135802  0.9691358025  0.8308641975  0.7308641975  0.7395061728  0.7864197531  17.204301075  0.9703332520  0.3012371063  300           0.0753883600 
0.7574074074  0.9964157706  0.9714285714  1.0000000000  0.9876543210  0.9953703704  0.9938271605  1.0000000000  0.9691358025  0.7987654321  0.7234567901  0.7345679012  0.7728395062  20.071684587  0.9529709506  0.3012371063  350           0.0758855820 
0.7694444444  0.9982078853  0.9857142857  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9691358025  0.8234567901  0.7308641975  0.7370370370  0.7864197531  22.939068100  0.9375775254  0.3012371063  400           0.0756448650 
0.7740740741  0.9982078853  0.9857142857  1.0000000000  0.9876543210  0.9984567901  0.9938271605  1.0000000000  0.9753086420  0.8296296296  0.7395061728  0.7407407407  0.7864197531  25.806451612  0.9197600150  0.3012547493  450           0.0783298397 
0.7740740741  1.0000000000  1.0000000000  0.9984567901  0.9876543210  1.0000000000  1.0000000000  0.9984567901  0.9691358025  0.8456790123  0.7271604938  0.7382716049  0.7851851852  28.673835125  0.8934447205  0.3012790680  500           0.0770489264 
0.7802469136  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8530864198  0.7456790123  0.7382716049  0.7839506173  31.541218638  0.8715231419  0.3012790680  550           0.0824250793 
0.7629629630  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  1.0000000000  0.9984567901  0.9691358025  0.8320987654  0.7172839506  0.7283950617  0.7740740741  34.408602150  0.8893146372  0.3013834953  600           0.0763737059 
0.7768518519  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  1.0000000000  0.9984567901  0.9753086420  0.8481481481  0.7259259259  0.7370370370  0.7962962963  37.275985663  0.8752001083  0.3017015457  650           0.0766000128 
0.7638888889  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  0.9938271605  0.9984567901  0.9814814815  0.8320987654  0.7308641975  0.7271604938  0.7654320988  40.143369175  0.8395757985  0.3017015457  700           0.0759290695 
0.7790123457  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8518518519  0.7419753086  0.7419753086  0.7802469136  43.010752688  0.8398889434  0.3017015457  750           0.0760755730 
0.7753086420  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8469135802  0.7271604938  0.7345679012  0.7925925926  45.878136200  0.8039513743  0.3017015457  800           0.0761540842 
0.7716049383  1.0000000000  0.9785714286  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9876543210  0.8382716049  0.7333333333  0.7358024691  0.7790123457  48.745519713  0.7903388894  0.3017015457  850           0.0858122873 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2276, in update
    meta_train_loss_main.backward(retain_graph=True)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
C->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3342592593  0.3118279570  0.3285714286  0.3379629630  0.3333333333  0.3194444444  0.3086419753  0.3395061728  0.3209876543  0.3271604938  0.3395061728  0.3172839506  0.3530864198  0.0000000000  4.3712167740  0.1364407539  0             0.4056999683 
0.7407407407  0.8243727599  0.7428571429  0.7762345679  0.7407407407  0.7716049383  0.7222222222  0.7592592593  0.6666666667  0.7888888889  0.7012345679  0.7395061728  0.7333333333  2.8673835125  3.0597019911  0.3011631966  50            0.0761326170 
0.7925925926  0.9946236559  0.9785714286  0.9012345679  0.8580246914  0.8858024691  0.8456790123  0.8827160494  0.8024691358  0.7987654321  0.7518518519  0.7987654321  0.8209876543  5.7347670251  1.9699133277  0.3011631966  100           0.0764451170 
0.7691358025  0.9892473118  0.9571428571  0.9891975309  0.9753086420  0.9876543210  0.9938271605  0.9706790123  0.9074074074  0.7753086420  0.7456790123  0.7580246914  0.7975308642  8.6021505376  1.4832920051  0.3011837006  150           0.0812906885 
0.7864197531  0.9964157706  0.9642857143  0.9969135802  0.9753086420  0.9922839506  1.0000000000  0.9598765432  0.9074074074  0.8185185185  0.7469135802  0.7580246914  0.8222222222  11.469534050  1.1241569901  0.3011837006  200           0.0790681458 
0.7716049383  0.9946236559  0.9642857143  0.9984567901  0.9753086420  0.9984567901  0.9938271605  0.9953703704  0.9567901235  0.8148148148  0.7432098765  0.7419753086  0.7864197531  14.336917562  1.0435216033  0.3011837006  250           0.0775463724 
0.7762345679  0.9982078853  0.9785714286  0.9984567901  0.9876543210  0.9969135802  0.9938271605  0.9953703704  0.9691358025  0.8271604938  0.7382716049  0.7432098765  0.7962962963  17.204301075  0.9688316822  0.3011837006  300           0.0791181517 
0.7592592593  0.9964157706  0.9714285714  1.0000000000  0.9814814815  0.9953703704  0.9938271605  1.0000000000  0.9753086420  0.8037037037  0.7296296296  0.7308641975  0.7728395062  20.071684587  0.9535037673  0.3011913300  350           0.0797189999 
0.7712962963  0.9982078853  0.9857142857  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8246913580  0.7320987654  0.7419753086  0.7864197531  22.939068100  0.9354897606  0.3011913300  400           0.0776393223 
0.7768518519  0.9982078853  0.9857142857  1.0000000000  0.9876543210  0.9984567901  0.9938271605  1.0000000000  0.9753086420  0.8320987654  0.7481481481  0.7382716049  0.7888888889  25.806451612  0.9226907003  0.3011913300  450           0.0790710497 
0.7777777778  1.0000000000  1.0000000000  0.9984567901  0.9876543210  1.0000000000  1.0000000000  0.9984567901  0.9691358025  0.8493827160  0.7283950617  0.7444444444  0.7888888889  28.673835125  0.8995033538  0.3012089729  500           0.0793196535 
0.7811728395  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8543209877  0.7481481481  0.7333333333  0.7888888889  31.541218638  0.8720399237  0.3012089729  550           0.0891642570 
0.7679012346  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  1.0000000000  0.9984567901  0.9691358025  0.8370370370  0.7271604938  0.7271604938  0.7802469136  34.408602150  0.8931146431  0.3012304306  600           0.0772665787 
0.7836419753  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  1.0000000000  0.9984567901  0.9753086420  0.8567901235  0.7320987654  0.7506172840  0.7950617284  37.275985663  0.8781125402  0.3012304306  650           0.0757903814 
0.7688271605  1.0000000000  0.9928571429  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9814814815  0.8395061728  0.7358024691  0.7283950617  0.7716049383  40.143369175  0.8344656241  0.3012304306  700           0.0763781786 
0.7808641975  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8580246914  0.7432098765  0.7444444444  0.7777777778  43.010752688  0.8364038706  0.3012304306  750           0.0775632429 
0.7762345679  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8456790123  0.7308641975  0.7370370370  0.7913580247  45.878136200  0.8099400151  0.3012304306  800           0.0808857584 
0.7728395062  1.0000000000  0.9785714286  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9876543210  0.8419753086  0.7345679012  0.7320987654  0.7827160494  48.745519713  0.7874105275  0.3012304306  850           0.0782308292 
0.7780864198  1.0000000000  0.9928571429  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8444444444  0.7419753086  0.7456790123  0.7802469136  51.612903225  0.7906363451  0.3012304306  900           0.0885397911 
0.7669753086  1.0000000000  0.9928571429  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8333333333  0.7333333333  0.7296296296  0.7716049383  54.480286738  0.7835920596  0.3012533188  950           0.0765067101 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 104, in start
    _cleanup()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 55, in _cleanup
    if p._popen.poll() is not None:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Exception ignored in: <function WeakValueDictionary.__init__.<locals>.remove at 0x7ffb42da0ae8>
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/weakref.py", line 117, in remove
    _atomic_removal(d, wr.key)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2895857) is killed by signal: Terminated. 
Exception ignored in: <function WeakSet.__init__.<locals>._remove at 0x7ffb51a59378>
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/_weakrefset.py", line 44, in _remove
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2895559) is killed by signal: Terminated. 
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/_pylab_helpers.py", line 89, in destroy_all
    gc.collect(1)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2896227) is killed by signal: Terminated. 
Error in atexit._run_exitfuncs:
Traceback (most recent call last):

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
C->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3438271605  0.3942652330  0.3928571429  0.3641975309  0.3456790123  0.3549382716  0.3456790123  0.3518518519  0.3641975309  0.3432098765  0.3469135802  0.3419753086  0.3432098765  0.0000000000  4.3712167740  0.1364407539  0             0.6109595299 
0.7308641975  0.9731182796  0.9571428571  0.9922839506  0.9753086420  0.9907407407  0.9876543210  0.9768518519  0.9444444444  0.7259259259  0.7333333333  0.7333333333  0.7308641975  2.8673835125  2.2372634363  0.3011631966  50            0.0889467096 
0.7447530864  0.9928315412  0.9714285714  0.9984567901  0.9938271605  0.9969135802  0.9876543210  0.9922839506  0.9876543210  0.7641975309  0.7358024691  0.7395061728  0.7395061728  5.7347670251  1.0845397139  0.3011631966  100           0.0780975819 
0.7790123457  0.9982078853  1.0000000000  0.9984567901  0.9753086420  0.9953703704  1.0000000000  0.9938271605  0.9691358025  0.8481481481  0.7370370370  0.7382716049  0.7925925926  8.6021505376  0.9813997412  0.3011631966  150           0.0797789383 
0.7743827160  0.9982078853  0.9928571429  1.0000000000  0.9814814815  1.0000000000  1.0000000000  0.9953703704  0.9691358025  0.8358024691  0.7419753086  0.7271604938  0.7925925926  11.469534050  0.8716032350  0.3011631966  200           0.0786571121 
0.7712962963  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.9984567901  1.0000000000  0.9969135802  0.9629629630  0.8518518519  0.7370370370  0.7123456790  0.7839506173  14.336917562  0.8429629219  0.3011631966  250           0.0784667444 
0.7648148148  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8061728395  0.7432098765  0.7320987654  0.7777777778  17.204301075  0.7951891947  0.3011631966  300           0.0783041620 
0.7592592593  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8000000000  0.7345679012  0.7320987654  0.7703703704  20.071684587  0.7797911370  0.3011631966  350           0.0785761023 
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 896, in __init__
    index_queue = multiprocessing_context.Queue()  # type: ignore
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 102, in Queue
    return Queue(maxsize, ctx=self.get_context())
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 42, in __init__
    self._rlock = ctx.Lock()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 67, in Lock
    return Lock(ctx=self.get_context())
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/synchronize.py", line 162, in __init__
    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/synchronize.py", line 58, in __init__
    kind, value, maxvalue, self._make_name(),
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/joblib/externals/loky/backend/__init__.py", line 9, in _make_name
    name = '/loky-%i-%s' % (os.getpid(), next(synchronize.SemLock._rand))
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/tempfile.py", line 160, in __next__
    letters = [choose(c) for dummy in range(8)]
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/tempfile.py", line 160, in <listcomp>
    letters = [choose(c) for dummy in range(8)]
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/random.py", line 258, in choice
    i = self._randbelow(len(seq))
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/random.py", line 228, in _randbelow
    getrandbits = self.getrandbits
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
C->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3342592593  0.3118279570  0.3285714286  0.3379629630  0.3333333333  0.3194444444  0.3086419753  0.3395061728  0.3209876543  0.3271604938  0.3395061728  0.3172839506  0.3530864198  0.0000000000  4.3712167740  0.1364407539  0             0.4869236946 
0.7388888889  0.8207885305  0.7428571429  0.7793209877  0.7345679012  0.7700617284  0.7222222222  0.7546296296  0.6543209877  0.7888888889  0.6925925926  0.7333333333  0.7407407407  2.8673835125  3.0668240404  0.3011631966  50            0.0764268160 
0.7891975309  0.9946236559  0.9785714286  0.8904320988  0.8456790123  0.8811728395  0.8395061728  0.8811728395  0.8024691358  0.7962962963  0.7493827160  0.7901234568  0.8209876543  5.7347670251  1.9734485245  0.3011684418  100           0.0780286884 
0.7716049383  0.9892473118  0.9571428571  0.9922839506  0.9753086420  0.9861111111  0.9938271605  0.9675925926  0.9074074074  0.7827160494  0.7419753086  0.7604938272  0.8012345679  8.6021505376  1.4839604044  0.3011898994  150           0.0772853565 
0.7824074074  0.9964157706  0.9714285714  0.9969135802  0.9753086420  0.9922839506  1.0000000000  0.9614197531  0.9135802469  0.8160493827  0.7506172840  0.7456790123  0.8172839506  11.469534050  1.1169268560  0.3011898994  200           0.0793017721 
0.7716049383  0.9946236559  0.9642857143  0.9984567901  0.9691358025  0.9984567901  0.9938271605  0.9969135802  0.9567901235  0.8197530864  0.7382716049  0.7407407407  0.7876543210  14.336917562  1.0413183749  0.3011898994  250           0.0826409769 
0.7719135802  0.9982078853  0.9785714286  0.9984567901  0.9814814815  0.9984567901  0.9938271605  0.9969135802  0.9691358025  0.8308641975  0.7308641975  0.7395061728  0.7864197531  17.204301075  0.9703332520  0.3011898994  300           0.0814332438 
0.7574074074  0.9964157706  0.9714285714  1.0000000000  0.9876543210  0.9953703704  0.9938271605  1.0000000000  0.9691358025  0.7987654321  0.7234567901  0.7345679012  0.7728395062  20.071684587  0.9529709506  0.3011898994  350           0.0815783310 
0.7694444444  0.9982078853  0.9857142857  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9691358025  0.8234567901  0.7308641975  0.7370370370  0.7864197531  22.939068100  0.9375775254  0.3011898994  400           0.0823126650 
0.7740740741  0.9982078853  0.9857142857  1.0000000000  0.9876543210  0.9984567901  0.9938271605  1.0000000000  0.9753086420  0.8296296296  0.7395061728  0.7407407407  0.7864197531  25.806451612  0.9197600150  0.3011898994  450           0.0862079096 
0.7740740741  1.0000000000  1.0000000000  0.9984567901  0.9876543210  1.0000000000  1.0000000000  0.9984567901  0.9691358025  0.8456790123  0.7271604938  0.7382716049  0.7851851852  28.673835125  0.8934447205  0.3011898994  500           0.0894983959 
0.7802469136  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8530864198  0.7456790123  0.7382716049  0.7839506173  31.541218638  0.8715231419  0.3011898994  550           0.0772092485 
0.7629629630  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  1.0000000000  0.9984567901  0.9691358025  0.8320987654  0.7172839506  0.7283950617  0.7740740741  34.408602150  0.8893146372  0.3011898994  600           0.0861390162 
0.7768518519  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  1.0000000000  0.9984567901  0.9753086420  0.8481481481  0.7259259259  0.7370370370  0.7962962963  37.275985663  0.8752001083  0.3012061119  650           0.0789492941 
0.7638888889  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  0.9938271605  0.9984567901  0.9814814815  0.8320987654  0.7308641975  0.7271604938  0.7654320988  40.143369175  0.8395757985  0.3012061119  700           0.0769721270 
0.7790123457  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8518518519  0.7419753086  0.7419753086  0.7802469136  43.010752688  0.8398889434  0.3012061119  750           0.0800124884 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 899, in __init__


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/tokenize.py", line 381, in read_or_stop
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
RuntimeError: DataLoader worker (pid 2951494) is killed by signal: Terminated. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2173, in <module>
    main()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2166, in main
    handle_keyboard_interrupt()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1984, in handle_keyboard_interrupt
    traceback.print_exception(type(value), value, tb, limit=limit)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/traceback.py", line 104, in print_exception
    type(value), value, tb, limit=limit).format(chain=chain):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/traceback.py", line 509, in __init__
    capture_locals=capture_locals)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/traceback.py", line 364, in extract
    f.line
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/traceback.py", line 286, in line
    self._line = linecache.getline(self.filename, self.lineno).strip()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/linecache.py", line 16, in getline
    lines = getlines(filename, module_globals)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/linecache.py", line 47, in getlines
    return updatecache(filename, module_globals)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/linecache.py", line 136, in updatecache
    with tokenize.open(fullname) as fp:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/tokenize.py", line 454, in open
      File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/tokenize.py", line 423, in detect_encoding
    first = read_or_stop()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/tokenize.py", line 381, in read_or_stop
    return readline()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2951258) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
C->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3342592593  0.3118279570  0.3285714286  0.3379629630  0.3333333333  0.3194444444  0.3086419753  0.3395061728  0.3209876543  0.3271604938  0.3395061728  0.3172839506  0.3530864198  0.0000000000  4.3712167740  0.1364407539  0             0.3930671215 
0.7388888889  0.8207885305  0.7428571429  0.7793209877  0.7345679012  0.7700617284  0.7222222222  0.7546296296  0.6543209877  0.7888888889  0.6925925926  0.7333333333  0.7407407407  2.8673835125  3.0668240404  0.3011631966  50            0.0759938908 
0.7891975309  0.9946236559  0.9785714286  0.8904320988  0.8456790123  0.8811728395  0.8395061728  0.8811728395  0.8024691358  0.7962962963  0.7493827160  0.7901234568  0.8209876543  5.7347670251  1.9734485245  0.3011693954  100           0.0789764118 
0.8064814815  0.9946236559  0.9785714286  0.9043209877  0.8518518519  0.8858024691  0.8395061728  0.8811728395  0.8086419753  0.8086419753  0.7666666667  0.8061728395  0.8444444444  8.6021505376  1.7374474430  0.3832902908  150           0.0787129879 
0.8058641975  0.9964157706  0.9642857143  0.9120370370  0.8580246914  0.8919753086  0.8518518519  0.8858024691  0.8148148148  0.8049382716  0.7641975309  0.8111111111  0.8432098765  11.469534050  1.6364483190  0.3832902908  200           0.0795446348 
0.8225308642  0.9964157706  0.9642857143  0.9043209877  0.8456790123  0.8811728395  0.8395061728  0.8750000000  0.8086419753  0.8185185185  0.7753086420  0.8296296296  0.8666666667  14.336917562  1.6583545899  0.3832902908  250           0.0799304581 
0.8311728395  0.9964157706  0.9642857143  0.9089506173  0.8518518519  0.8935185185  0.8518518519  0.8858024691  0.8086419753  0.8246913580  0.7864197531  0.8345679012  0.8790123457  17.204301075  1.6062639308  0.3832902908  300           0.0787707424 
0.8311728395  0.9964157706  0.9642857143  0.9135802469  0.8641975309  0.8981481481  0.8518518519  0.8950617284  0.8148148148  0.8259259259  0.7913580247  0.8296296296  0.8777777778  20.071684587  1.6402235723  0.3832902908  350           0.0794757938 
0.8265432099  0.9982078853  0.9642857143  0.9212962963  0.8641975309  0.9012345679  0.8641975309  0.8858024691  0.8086419753  0.8222222222  0.7790123457  0.8308641975  0.8740740741  22.939068100  1.6252530718  0.3832902908  400           0.0775064754 
0.8280864198  0.9964157706  0.9642857143  0.9151234568  0.8641975309  0.8966049383  0.8518518519  0.8919753086  0.8148148148  0.8246913580  0.7864197531  0.8222222222  0.8790123457  25.806451612  1.6269215941  0.3832902908  450           0.0763789606 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A,C->B
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3342592593  0.3611111111  0.3827160494  0.3370370370  0.3358024691  0.3657407407  0.2777777778  0.3641975309  0.3209876543  0.3425925926  0.3765432099  0.3835125448  0.3785714286  0.3533950617  0.3333333333  0.3302469136  0.3703703704  0.3580246914  0.3456790123  0.3370370370  0.3271604938  0.0000000000  10.427348136  0.2909131050  0             0.4808220863 
0.8123456790  0.8750000000  0.8209876543  0.8209876543  0.8382716049  0.8765432099  0.8271604938  0.8503086420  0.8024691358  0.8765432099  0.7901234568  0.9318996416  0.8857142857  0.8395061728  0.7901234568  0.8503086420  0.8209876543  0.8425925926  0.7777777778  0.8320987654  0.7580246914  2.8673835125  6.8808958530  0.6690006256  50            0.1336043072 
0.7669753086  0.9675925926  0.9567901235  0.7703703704  0.8172839506  0.9891975309  0.9691358025  0.9799382716  0.9753086420  0.9706790123  0.9320987654  0.9767025090  0.9857142857  0.9274691358  0.9012345679  0.9166666667  0.9320987654  0.9228395062  0.8888888889  0.7456790123  0.7345679012  5.7347670251  4.2878446436  0.6690006256  100           0.1315326500 
0.7833333333  0.9938271605  0.9814814815  0.7802469136  0.8419753086  0.9969135802  0.9876543210  0.9861111111  0.9938271605  0.9861111111  0.9382716049  0.9856630824  0.9785714286  0.9552469136  0.9444444444  0.9475308642  0.9691358025  0.9475308642  0.9012345679  0.7753086420  0.7358024691  8.6021505376  3.2825759315  0.6690006256  150           0.1321300030 
0.7759259259  0.9907407407  0.9876543210  0.7790123457  0.8185185185  1.0000000000  0.9938271605  0.9861111111  0.9938271605  0.9861111111  0.9629629630  0.9892473118  0.9785714286  0.9737654321  0.9382716049  0.9706790123  0.9814814815  0.9675925926  0.9320987654  0.7666666667  0.7395061728  11.469534050  2.7027330780  0.6690006256  200           0.1333311176 
0.7570987654  0.9845679012  0.9753086420  0.7617283951  0.8197530864  1.0000000000  1.0000000000  0.9891975309  0.9876543210  0.9876543210  0.9567901235  0.9910394265  0.9785714286  0.9861111111  0.9444444444  0.9783950617  0.9814814815  0.9675925926  0.9259259259  0.7567901235  0.6901234568  14.336917562  2.4881786203  0.6690006256  250           0.1340183258 
0.8015432099  1.0000000000  0.9938271605  0.8123456790  0.8580246914  1.0000000000  0.9938271605  0.9953703704  0.9938271605  0.9969135802  0.9629629630  1.0000000000  0.9785714286  0.9876543210  0.9691358025  0.9891975309  0.9876543210  0.9907407407  0.9135802469  0.8197530864  0.7160493827  17.204301075  2.3676642752  0.6690006256  300           0.1311082315 
0.7879629630  0.9984567901  0.9876543210  0.7913580247  0.8432098765  1.0000000000  1.0000000000  0.9953703704  1.0000000000  0.9969135802  0.9753086420  0.9982078853  0.9857142857  0.9938271605  0.9876543210  0.9984567901  0.9938271605  0.9938271605  0.9382716049  0.7987654321  0.7185185185  20.071684587  2.2701844740  0.6690006256  350           0.1331535721 
0.7873456790  0.9984567901  0.9876543210  0.7987654321  0.8444444444  1.0000000000  1.0000000000  0.9953703704  0.9876543210  0.9984567901  0.9691358025  1.0000000000  0.9857142857  0.9953703704  0.9876543210  0.9969135802  0.9938271605  0.9953703704  0.9444444444  0.7950617284  0.7111111111  22.939068100  2.1809357357  0.6690006256  400           0.1308758307 
0.7966049383  0.9984567901  0.9876543210  0.8049382716  0.8555555556  1.0000000000  1.0000000000  0.9984567901  1.0000000000  0.9984567901  0.9753086420  1.0000000000  0.9785714286  0.9969135802  0.9876543210  1.0000000000  0.9938271605  0.9953703704  0.9444444444  0.8111111111  0.7148148148  25.806451612  2.0326404309  0.6690006256  450           0.1334842682 
0.7972222222  1.0000000000  0.9876543210  0.8098765432  0.8518518519  1.0000000000  1.0000000000  0.9984567901  1.0000000000  0.9984567901  0.9753086420  1.0000000000  0.9785714286  0.9984567901  0.9876543210  1.0000000000  0.9938271605  0.9984567901  0.9506172840  0.8086419753  0.7185185185  28.673835125  1.9993065548  0.6690006256  500           0.1315213299 
0.8024691358  0.9984567901  0.9938271605  0.8135802469  0.8518518519  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9984567901  0.9814814815  1.0000000000  0.9785714286  0.9984567901  0.9876543210  0.9984567901  0.9938271605  0.9938271605  0.9320987654  0.8222222222  0.7222222222  31.541218638  1.9453654861  0.6690006256  550           0.1329944706 
0.7827160494  0.9984567901  0.9876543210  0.7975308642  0.8308641975  1.0000000000  1.0000000000  0.9984567901  0.9938271605  0.9984567901  0.9753086420  0.9982078853  0.9857142857  0.9984567901  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9691358025  0.7888888889  0.7135802469  34.408602150  1.8859781814  0.6690006256  600           0.1315041876 
0.7916666667  1.0000000000  0.9876543210  0.8012345679  0.8345679012  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9984567901  0.9876543210  1.0000000000  0.9785714286  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9691358025  0.8123456790  0.7185185185  37.275985663  1.9038450789  0.6690006256  650           0.1304998159 
0.7774691358  0.9984567901  0.9814814815  0.7888888889  0.8358024691  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9984567901  0.9876543210  1.0000000000  0.9857142857  0.9984567901  0.9876543210  1.0000000000  0.9938271605  0.9984567901  0.9629629630  0.7839506173  0.7012345679  40.143369175  1.8671808910  0.6690006256  700           0.1309348822 
0.7975308642  1.0000000000  0.9938271605  0.8271604938  0.8333333333  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.9982078853  0.9785714286  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  0.8172839506  0.7123456790  43.010752688  1.8156340265  0.6690006256  750           0.1311827946 
0.7808641975  1.0000000000  0.9814814815  0.7975308642  0.8382716049  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.9984567901  0.9876543210  1.0000000000  0.9857142857  0.9984567901  0.9876543210  1.0000000000  0.9938271605  0.9984567901  0.9506172840  0.7901234568  0.6975308642  45.878136200  1.7750763822  0.6690006256  800           0.1286117792 
0.7953703704  1.0000000000  0.9938271605  0.8160493827  0.8419753086  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9785714286  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8135802469  0.7098765432  48.745519713  1.7611474156  0.6690006256  850           0.1320097542 
0.7922839506  1.0000000000  0.9938271605  0.8172839506  0.8259259259  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.9982078853  0.9785714286  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  0.8049382716  0.7209876543  51.612903225  1.7539930201  0.6690006256  900           0.1322137356 
0.7577160494  0.9969135802  0.9814814815  0.7617283951  0.8234567901  0.9984567901  0.9938271605  0.9984567901  0.9691358025  0.9953703704  0.9567901235  1.0000000000  0.9857142857  0.9984567901  0.9814814815  0.9984567901  0.9876543210  0.9922839506  0.9444444444  0.7592592593  0.6864197531  54.480286738  1.7454215479  0.6690006256  950           0.1318178511 
0.7780864198  1.0000000000  0.9876543210  0.7950617284  0.8172839506  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9814814815  1.0000000000  0.9785714286  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9691358025  0.7851851852  0.7148148148  57.347670250  1.7124675703  0.6690006256  1000          0.1309937811 
0.7898148148  1.0000000000  0.9938271605  0.8098765432  0.8209876543  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9785714286  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  0.8037037037  0.7246913580  60.215053763  1.7093372631  0.6690006256  1050          0.1317098522 
0.7913580247  1.0000000000  0.9876543210  0.8074074074  0.8407407407  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.9984567901  0.9876543210  1.0000000000  0.9785714286  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8061728395  0.7111111111  63.082437276  1.6805813193  0.6690006256  1100          0.1301879358 
0.7972222222  1.0000000000  0.9876543210  0.8160493827  0.8395061728  1.0000000000  1.0000000000  1.0000000000  0.9814814815  1.0000000000  0.9876543210  1.0000000000  0.9785714286  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9691358025  0.8185185185  0.7148148148  65.949820788  1.7331183386  0.6690006256  1150          0.1312251806 
0.7972222222  1.0000000000  0.9938271605  0.8222222222  0.8333333333  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  0.9857142857  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9567901235  0.8148148148  0.7185185185  68.817204301  1.6851275253  0.6690006256  1200          0.1309607601 
0.7972222222  1.0000000000  0.9938271605  0.8271604938  0.8271604938  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  0.9857142857  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9629629630  0.8197530864  0.7148148148  71.684587813  1.6381295824  0.6690006256  1250          0.1324743748 
0.7802469136  1.0000000000  0.9938271605  0.8086419753  0.8061728395  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9785714286  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  0.7913580247  0.7148148148  74.551971326  1.6984846735  0.6690006256  1300          0.1313577986 
0.7882716049  1.0000000000  0.9938271605  0.8135802469  0.8222222222  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9785714286  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9691358025  0.7987654321  0.7185185185  77.419354838  1.6495617628  0.6690006256  1350          0.1314833736 
0.7947530864  1.0000000000  0.9938271605  0.8160493827  0.8246913580  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  0.9857142857  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9691358025  0.8160493827  0.7222222222  80.286738351  1.6229967141  0.6690006256  1400          0.1307341766 
0.7885802469  1.0000000000  0.9876543210  0.8135802469  0.8259259259  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9928571429  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8049382716  0.7098765432  83.154121863  1.6518979597  0.6690006256  1450          0.1309769297 
0.7935185185  1.0000000000  0.9938271605  0.8049382716  0.8481481481  1.0000000000  0.9938271605  1.0000000000  0.9691358025  1.0000000000  0.9629629630  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9506172840  0.8172839506  0.7037037037  86.021505376  1.6294252920  0.6690006256  1500          0.1303175306 
0.7814814815  1.0000000000  0.9938271605  0.7950617284  0.8148148148  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9753086420  0.7950617284  0.7209876543  88.888888888  1.6262568736  0.6690006256  1550          0.1309673452 
0.7885802469  1.0000000000  0.9938271605  0.8086419753  0.8234567901  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9785714286  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8086419753  0.7135802469  91.756272401  1.6043668246  0.6690006256  1600          0.1295070124 
0.7910493827  1.0000000000  0.9938271605  0.8074074074  0.8197530864  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9928571429  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9691358025  0.8074074074  0.7296296296  94.623655914  1.5860588360  0.6690006256  1650          0.1316604471 
0.7827160494  1.0000000000  1.0000000000  0.8037037037  0.8074074074  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9785714286  1.0000000000  0.9814814815  1.0000000000  0.9876543210  1.0000000000  0.9691358025  0.8012345679  0.7185185185  97.491039426  1.6138489246  0.6690006256  1700          0.1320031738 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3314814815  0.3179012346  0.3148148148  0.3888888889  0.3827160494  0.3780864198  0.3333333333  0.3518518519  0.3395061728  0.3259259259  0.3345679012  0.3197530864  0.3456790123  0.0000000000  4.5427684784  0.1364407539  0             0.4128651619 
0.7728395062  0.8842592593  0.8395061728  0.8765432099  0.8703703704  0.8750000000  0.8148148148  0.8734567901  0.8148148148  0.7802469136  0.7209876543  0.7827160494  0.8074074074  2.4691358025  3.1544543648  0.3011631966  50            0.0737962294 
0.7672839506  0.9783950617  0.9259259259  0.9691358025  0.9567901235  0.9429012346  0.9259259259  0.9598765432  0.9197530864  0.7469135802  0.7320987654  0.7851851852  0.8049382716  4.9382716049  2.0345851874  0.3011846542  100           0.0734486246 
0.8055555556  0.9922839506  0.9691358025  0.9814814815  0.9629629630  0.9660493827  0.9567901235  0.9783950617  0.9567901235  0.7950617284  0.7493827160  0.8296296296  0.8481481481  7.4074074074  1.5206844187  0.3011846542  150           0.0726857090 
0.7879629630  0.9953703704  0.9876543210  0.9953703704  0.9938271605  0.9922839506  0.9938271605  0.9922839506  0.9506172840  0.7641975309  0.7407407407  0.8098765432  0.8370370370  9.8765432099  1.2168315578  0.3011846542  200           0.0750039339 
0.8191358025  0.9969135802  0.9876543210  1.0000000000  1.0000000000  0.9969135802  1.0000000000  0.9938271605  0.9567901235  0.8123456790  0.7543209877  0.8395061728  0.8703703704  12.345679012  1.0678243732  0.3011846542  250           0.0752363777 
0.7950617284  0.9969135802  0.9938271605  1.0000000000  1.0000000000  0.9953703704  1.0000000000  0.9969135802  0.9567901235  0.7604938272  0.7617283951  0.8160493827  0.8419753086  14.814814814  0.9739327884  0.3011846542  300           0.0734717131 
0.8135802469  0.9984567901  1.0000000000  1.0000000000  1.0000000000  0.9922839506  0.9938271605  0.9969135802  0.9753086420  0.8024691358  0.7839506173  0.8296296296  0.8382716049  17.283950617  0.9520863783  0.3011846542  350           0.0766982985 
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
Traceback (most recent call last):
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)

During handling of the above exception, another exception occurred:

KeyboardInterrupt
Traceback (most recent call last):

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 3237440) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0001
	mldg_beta: 0.2733254999483027
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3246913580  0.3225308642  0.3395061728  0.3456790123  0.3209876543  0.3472222222  0.3333333333  0.3395061728  0.3456790123  0.3283950617  0.3160493827  0.3197530864  0.3345679012  0.0000000000  1.5158015490  0.0312581062  0             0.3867125511 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  2.4691358025  0.8948939420  0.0354824066  50            0.0858720541 
0.3333333333  0.3348765432  0.3333333333  0.3333333333  0.3395061728  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  4.9382716049  0.3577653930  0.0354824066  100           0.0842313910 
0.3348765432  0.3765432099  0.4135802469  0.3966049383  0.3888888889  0.4012345679  0.3827160494  0.3873456790  0.3641975309  0.3345679012  0.3345679012  0.3358024691  0.3345679012  7.4074074074  0.1587821515  0.0354886055  150           0.0870957375 
0.3379629630  0.3966049383  0.4197530864  0.4043209877  0.3827160494  0.4043209877  0.3765432099  0.3858024691  0.3703703704  0.3395061728  0.3345679012  0.3382716049  0.3395061728  9.8765432099  0.0651434759  0.0354886055  200           0.0968871355 
0.3962962963  0.4984567901  0.4938271605  0.5123456790  0.4814814815  0.5030864198  0.4876543210  0.5092592593  0.4876543210  0.4123456790  0.3604938272  0.4012345679  0.4111111111  12.345679012  0.0358067517  0.0354886055  250           0.0846164370 
0.5432098765  0.6172839506  0.6049382716  0.6280864198  0.6172839506  0.6296296296  0.6172839506  0.6126543210  0.5925925926  0.6024691358  0.4975308642  0.5358024691  0.5370370370  14.814814814  0.0255853918  0.0354886055  300           0.0904109335 
0.5851851852  0.6543209877  0.6419753086  0.6558641975  0.6481481481  0.6574074074  0.6481481481  0.6527777778  0.6358024691  0.6283950617  0.5370370370  0.5827160494  0.5925925926  17.283950617  0.0142201134  0.0354886055  350           0.0903863239 
0.4274691358  0.5432098765  0.5432098765  0.5478395062  0.5185185185  0.5447530864  0.5246913580  0.5462962963  0.5246913580  0.4555555556  0.3802469136  0.4333333333  0.4407407407  19.753086419  0.0111030305  0.0356264114  400           0.0913438320 
0.4811728395  0.5895061728  0.5679012346  0.6095679012  0.5925925926  0.5987654321  0.5802469136  0.5956790123  0.5740740741  0.5209876543  0.4308641975  0.4790123457  0.4938271605  22.222222222  0.0104411597  0.0356264114  450           0.0852179432 
0.6077160494  0.6635802469  0.6604938272  0.6651234568  0.6604938272  0.6635802469  0.6543209877  0.6589506173  0.6543209877  0.6358024691  0.5654320988  0.6123456790  0.6172839506  24.691358024  0.0082605805  0.0367226601  500           0.0835410357 
0.6049382716  0.6651234568  0.6604938272  0.6651234568  0.6604938272  0.6620370370  0.6604938272  0.6589506173  0.6604938272  0.6333333333  0.5555555556  0.6148148148  0.6160493827  27.160493827  0.0051710455  0.0367226601  550           0.0851559210 
0.5716049383  0.6512345679  0.6543209877  0.6604938272  0.6604938272  0.6543209877  0.6481481481  0.6543209877  0.6481481481  0.6037037037  0.5111111111  0.5827160494  0.5888888889  29.629629629  0.0048271021  0.0367226601  600           0.0840776920 
0.6521604938  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6641975309  0.6333333333  0.6518518519  0.6592592593  32.098765432  0.0036767706  0.0367226601  650           0.0854295731 
0.6490740741  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6629629630  0.6259259259  0.6506172840  0.6567901235  34.567901234  0.0035821035  0.0367226601  700           0.0847175503 
0.6030864198  0.6635802469  0.6666666667  0.6635802469  0.6604938272  0.6620370370  0.6666666667  0.6574074074  0.6543209877  0.6345679012  0.5493827160  0.6148148148  0.6135802469  37.037037037  0.0034361658  0.0367226601  750           0.0833052540 
0.6533950617  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6629629630  0.6370370370  0.6530864198  0.6604938272  39.506172839  0.0029314329  0.0367226601  800           0.0854634619 
0.6537037037  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6629629630  0.6358024691  0.6555555556  0.6604938272  41.975308642  0.0016653834  0.0367226601  850           0.0886446953 
0.6342592593  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6635802469  0.6666666667  0.6620370370  0.6666666667  0.6530864198  0.5975308642  0.6419753086  0.6444444444  44.444444444  0.0017553959  0.0367226601  900           0.0850261641 
0.6179012346  0.6635802469  0.6666666667  0.6651234568  0.6666666667  0.6635802469  0.6666666667  0.6604938272  0.6666666667  0.6456790123  0.5703703704  0.6296296296  0.6259259259  46.913580246  0.0024092283  0.0367226601  950           0.0908175755 
0.6416666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6635802469  0.6666666667  0.6635802469  0.6666666667  0.6592592593  0.6148148148  0.6432098765  0.6493827160  49.382716049  0.0021855736  0.0367226601  1000          0.0849431086 
0.6382716049  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6555555556  0.6074074074  0.6432098765  0.6469135802  51.851851851  0.0019147741  0.0367226601  1050          0.0870628881 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 301, in <module>
    for x, y in next(train_minibatches_iterator)]
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/fast_data_loader.py", line 46, in __iter__
    yield next(self._infinite_iterator)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 994, in _try_get_data
    if self._workers_status[worker_id] and not w.is_alive():
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 139, in is_alive
    returncode = self._popen.poll()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0005
	mldg_beta: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.0000000000  2.3050104082  0.0312581062  0             0.4777834415 
0.3719135802  0.5092592593  0.5000000000  0.5277777778  0.5000000000  0.5231481481  0.5123456790  0.5092592593  0.5000000000  0.3888888889  0.3604938272  0.3728395062  0.3654320988  2.4691358025  0.7053346103  0.0354671478  50            0.0840347862 
0.3608024691  0.4969135802  0.4814814815  0.5169753086  0.5000000000  0.5046296296  0.5000000000  0.4938271605  0.4691358025  0.3740740741  0.3493827160  0.3617283951  0.3580246914  4.9382716049  0.0817513240  0.0354824066  100           0.0839296055 
0.5083333333  0.5663580247  0.5432098765  0.5694444444  0.5493827160  0.5679012346  0.5432098765  0.5648148148  0.5493827160  0.5604938272  0.4814814815  0.5086419753  0.4827160494  7.4074074074  0.0286621723  0.0355000496  150           0.0840254641 
0.3608024691  0.4768518519  0.4629629630  0.4861111111  0.4691358025  0.4737654321  0.4753086420  0.4490740741  0.4629629630  0.3691358025  0.3530864198  0.3641975309  0.3567901235  9.8765432099  0.0102729988  0.0355000496  200           0.0874290705 
0.4953703704  0.5663580247  0.5555555556  0.5725308642  0.5493827160  0.5756172840  0.5370370370  0.5694444444  0.5555555556  0.5580246914  0.4567901235  0.4901234568  0.4765432099  12.345679012  0.0047928586  0.0355000496  250           0.0849661922 
0.5308641975  0.6250000000  0.6234567901  0.6311728395  0.6234567901  0.6373456790  0.6419753086  0.6358024691  0.5987654321  0.5617283951  0.5037037037  0.5432098765  0.5148148148  14.814814814  0.0042170149  0.0355429649  300           0.0848744726 
0.5490740741  0.6219135802  0.6234567901  0.6311728395  0.6111111111  0.6296296296  0.6358024691  0.6250000000  0.6049382716  0.5901234568  0.5086419753  0.5567901235  0.5407407407  17.283950617  0.0015448344  0.0355792046  350           0.0845563173 
0.3830246914  0.4984567901  0.5061728395  0.5216049383  0.5000000000  0.5046296296  0.4938271605  0.4953703704  0.5061728395  0.4185185185  0.3580246914  0.3827160494  0.3728395062  19.753086419  0.0015988805  0.0355792046  400           0.0844160557 
0.6095679012  0.6604938272  0.6604938272  0.6604938272  0.6604938272  0.6635802469  0.6604938272  0.6589506173  0.6604938272  0.6259259259  0.5790123457  0.6185185185  0.6148148148  22.222222222  0.0012502554  0.0355792046  450           0.0833936357 
0.3765432099  0.4922839506  0.5061728395  0.5169753086  0.4753086420  0.4969135802  0.4753086420  0.4814814815  0.4814814815  0.4000000000  0.3580246914  0.3740740741  0.3740740741  24.691358024  0.0012689605  0.0355792046  500           0.0840869236 
0.4419753086  0.5740740741  0.5802469136  0.5787037037  0.5679012346  0.5802469136  0.5679012346  0.5663580247  0.5555555556  0.4802469136  0.4024691358  0.4506172840  0.4345679012  27.160493827  0.0021763045  0.0355792046  550           0.0841528130 
0.4030864198  0.5308641975  0.5370370370  0.5555555556  0.5246913580  0.5509259259  0.5246913580  0.5200617284  0.5370370370  0.4358024691  0.3691358025  0.4074074074  0.4000000000  29.629629629  0.0009465373  0.0355792046  600           0.0850699425 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 562, in update
    inner_net = copy.deepcopy(self.network)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 306, in _reconstruct
    value = deepcopy(value, memo)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 306, in _reconstruct
    value = deepcopy(value, memo)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/copy.py", line 183, in deepcopy
    if y is not x:
KeyboardInterrupt
trails: 0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0005
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.0000000000  1.1818408966  0.0539774895  0             0.3392746449 
0.7540123457  0.9783950617  0.9814814815  0.9922839506  0.9938271605  0.9706790123  0.9753086420  0.9737654321  0.9259259259  0.7098765432  0.7358024691  0.7777777778  0.7925925926  2.4691358025  0.4120967099  0.0604319572  50            0.0125411272 
0.7993827160  0.9969135802  0.9938271605  1.0000000000  1.0000000000  0.9969135802  0.9938271605  0.9938271605  0.9506172840  0.7864197531  0.7493827160  0.8209876543  0.8407407407  4.9382716049  0.0432578161  0.0604319572  100           0.0127678061 
0.8447530864  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9984567901  1.0000000000  0.9984567901  0.9567901235  0.8432098765  0.7913580247  0.8679012346  0.8765432099  7.4074074074  0.0151206301  0.0604319572  150           0.0128815269 
0.8533950617  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9567901235  0.8543209877  0.7814814815  0.8851851852  0.8925925926  9.8765432099  0.0070171223  0.0604319572  200           0.0120929432 
0.8391975309  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9567901235  0.8382716049  0.7666666667  0.8654320988  0.8864197531  12.345679012  0.0018897972  0.0604319572  250           0.0124252796 
0.8546296296  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8617283951  0.8086419753  0.8814814815  0.8666666667  14.814814814  0.0040219786  0.0604319572  300           0.0126837206 
0.8645061728  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9629629630  0.8753086420  0.7765432099  0.8987654321  0.9074074074  17.283950617  0.0021997933  0.0604557991  350           0.0159931326 
0.8311728395  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9629629630  0.8259259259  0.7530864198  0.8666666667  0.8790123457  19.753086419  0.0009804739  0.0624608994  400           0.0124651384 
0.8564814815  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8567901235  0.7975308642  0.8888888889  0.8827160494  22.222222222  0.0007067391  0.0624608994  450           0.0135035658 
0.8503086420  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8481481481  0.7901234568  0.8827160494  0.8802469136  24.691358024  0.0005996858  0.0624608994  500           0.0126775694 
0.8515432099  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8617283951  0.8012345679  0.8827160494  0.8604938272  27.160493827  0.0009924389  0.0624608994  550           0.0125455952 
0.8577160494  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8679012346  0.7888888889  0.8925925926  0.8814814815  29.629629629  0.0006856152  0.0624608994  600           0.0120920229 
0.8537037037  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8567901235  0.8000000000  0.8864197531  0.8716049383  32.098765432  0.0004365390  0.0624608994  650           0.0137476158 
0.8540123457  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8641975309  0.8049382716  0.8802469136  0.8666666667  34.567901234  0.0003938820  0.0624608994  700           0.0136545801 
0.8154320988  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8407407407  0.7641975309  0.8481481481  0.8086419753  37.037037037  0.0009433121  0.0624608994  750           0.0188742876 
0.8441358025  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9629629630  0.8395061728  0.7962962963  0.8691358025  0.8716049383  39.506172839  0.0012063266  0.0624608994  800           0.0125765085 
0.8577160494  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8716049383  0.8012345679  0.8876543210  0.8703703704  41.975308642  0.0005857041  0.0624608994  850           0.0159481812 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 940, in __init__
    self._reset(loader, first_iter=True)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 971, in _reset
    self._try_put_index()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1216, in _try_put_index
    self._index_queues[worker_queue_idx].put((self._send_idx, index))
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 87, in put
    self._start_thread()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 169, in _start_thread
    self._thread.start()
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 3304226) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3675925926  0.3858024691  0.3888888889  0.3765432099  0.3703703704  0.3904320988  0.3765432099  0.3796296296  0.4012345679  0.3580246914  0.3765432099  0.3629629630  0.3728395062  0.0000000000  4.5427684784  0.1364407539  0             0.4022605419 
0.7925925926  0.9830246914  0.9814814815  0.9938271605  0.9567901235  0.9614197531  0.9691358025  0.9753086420  0.9320987654  0.7703703704  0.7728395062  0.8012345679  0.8259259259  2.4691358025  2.3907544136  0.3011631966  50            0.0780972385 
0.8354938272  0.9969135802  0.9876543210  0.9984567901  0.9814814815  0.9938271605  0.9876543210  0.9938271605  0.9629629630  0.8543209877  0.7629629630  0.8395061728  0.8851851852  4.9382716049  1.1786456943  0.3011631966  100           0.0749513435 
0.8398148148  0.9984567901  0.9876543210  0.9984567901  0.9938271605  1.0000000000  0.9876543210  0.9969135802  0.9629629630  0.8555555556  0.7506172840  0.8518518519  0.9012345679  7.4074074074  0.9326094115  0.3011631966  150           0.0749754667 
0.8367283951  1.0000000000  0.9814814815  1.0000000000  0.9876543210  1.0000000000  0.9876543210  0.9938271605  0.9691358025  0.8753086420  0.7395061728  0.8444444444  0.8876543210  9.8765432099  0.8839950156  0.3011631966  200           0.0753045940 
0.8274691358  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8567901235  0.7641975309  0.8555555556  0.8333333333  12.345679012  0.8538678372  0.3011631966  250           0.0742864656 
0.8410493827  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9814814815  0.8444444444  0.7654320988  0.8703703704  0.8839506173  14.814814814  0.8340550041  0.3011631966  300           0.0790368605 
0.7666666667  0.9938271605  0.9938271605  1.0000000000  0.9938271605  0.9984567901  0.9876543210  0.9984567901  0.9814814815  0.7938271605  0.7123456790  0.7962962963  0.7641975309  17.283950617  0.8032928646  0.3011846542  350           0.0773040485 
0.8404320988  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.8567901235  0.7518518519  0.8765432099  0.8765432099  19.753086419  0.7846713102  0.3011846542  400           0.0749626112 
0.8333333333  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8530864198  0.7493827160  0.8629629630  0.8679012346  22.222222222  0.7776873446  0.3011846542  450           0.0765929461 
0.8296296296  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8444444444  0.7456790123  0.8679012346  0.8604938272  24.691358024  0.7613133752  0.3011846542  500           0.0765857410 
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
KeyboardInterrupt
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)

During handling of the above exception, another exception occurred:

  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
Traceback (most recent call last):
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 3360111) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3675925926  0.3858024691  0.3888888889  0.3765432099  0.3703703704  0.3904320988  0.3765432099  0.3796296296  0.4012345679  0.3580246914  0.3765432099  0.3629629630  0.3728395062  0.0000000000  4.5427684784  0.1364407539  0             0.3661444187 
0.7916666667  0.9845679012  0.9876543210  0.9938271605  0.9691358025  0.9660493827  0.9691358025  0.9783950617  0.9382716049  0.7716049383  0.7728395062  0.8024691358  0.8197530864  2.4691358025  2.3792417002  0.3011631966  50            0.0749540615 
0.8459876543  0.9969135802  0.9938271605  1.0000000000  0.9938271605  0.9938271605  1.0000000000  0.9938271605  0.9629629630  0.8617283951  0.7814814815  0.8641975309  0.8765432099  4.9382716049  1.1946287680  0.3832902908  100           0.0761258078 
0.8552469136  0.9984567901  0.9938271605  0.9984567901  0.9938271605  1.0000000000  1.0000000000  0.9969135802  0.9753086420  0.8753086420  0.7703703704  0.8827160494  0.8925925926  7.4074074074  0.9309018505  0.3832902908  150           0.0745825720 
0.8422839506  1.0000000000  0.9814814815  0.9984567901  0.9814814815  1.0000000000  0.9876543210  0.9953703704  0.9691358025  0.8814814815  0.7407407407  0.8555555556  0.8913580247  9.8765432099  0.8780241370  0.3832902908  200           0.0747002220 
0.8540123457  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.8777777778  0.7654320988  0.8864197531  0.8864197531  12.345679012  0.8518926096  0.3832902908  250           0.0747733021 
0.8348765432  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9567901235  0.8580246914  0.7469135802  0.8580246914  0.8765432099  14.814814814  0.8537163424  0.3832902908  300           0.0767123795 
0.8104938272  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8444444444  0.7444444444  0.8382716049  0.8148148148  17.283950617  0.8097029614  0.3832902908  350           0.0750131416 
0.8358024691  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9876543210  0.8641975309  0.7456790123  0.8666666667  0.8666666667  19.753086419  0.7691986883  0.3832902908  400           0.0782949018 
0.8200617284  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8555555556  0.7345679012  0.8506172840  0.8395061728  22.222222222  0.8077611804  0.3832902908  450           0.0785816002 
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 292, in _on_run
    r = self.sock.recv(1024)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 296, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 218, in run
    self._on_run()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1846, in _on_run
    return ReaderThread._on_run(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 328, in _on_run
    self.handle_except()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1849, in handle_except
    ReaderThread.handle_except(self)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_comm.py", line 332, in handle_except
    self.global_debugger_holder.global_dbg.finish_debugging_session()
AttributeError: 'NoneType' object has no attribute 'finish_debugging_session'
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 379, in select
    for fd, event in fd_event_list:
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 3380948) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.27
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3762345679  0.3950617284  0.3888888889  0.3827160494  0.3950617284  0.3996913580  0.3765432099  0.3842592593  0.4074074074  0.3666666667  0.3864197531  0.3728395062  0.3790123457  0.0000000000  4.5427684784  0.1364407539  0             89.757477998 
0.7805555556  0.9768518519  0.9629629630  0.9737654321  0.9567901235  0.9475308642  0.9382716049  0.9706790123  0.9320987654  0.7395061728  0.7691358025  0.8024691358  0.8111111111  2.4691358025  2.3789997363  0.3012366295  50            0.0796848202 
0.8317901235  0.9953703704  0.9876543210  0.9984567901  0.9938271605  0.9938271605  0.9876543210  0.9953703704  0.9567901235  0.8493827160  0.7567901235  0.8382716049  0.8827160494  4.9382716049  1.2502559590  0.3012366295  100           0.0771863317 
0.8320987654  0.9984567901  0.9876543210  0.9984567901  1.0000000000  1.0000000000  0.9876543210  0.9969135802  0.9567901235  0.8530864198  0.7432098765  0.8444444444  0.8876543210  7.4074074074  0.9610146737  0.3012366295  150           0.0748569298 
0.8515432099  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9938271605  0.9984567901  0.9691358025  0.8777777778  0.7555555556  0.8691358025  0.9037037037  9.8765432099  0.9162518382  0.3012366295  200           0.0745679569 
0.8336419753  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9814814815  0.8654320988  0.7666666667  0.8580246914  0.8444444444  12.345679012  0.8767253423  0.3012366295  250           0.0748856449 
0.8398148148  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.8358024691  0.7740740741  0.8740740741  0.8753086420  14.814814814  0.8803340375  0.3012366295  300           0.0756779337 
0.7645061728  0.9938271605  1.0000000000  1.0000000000  0.9938271605  0.9984567901  0.9938271605  0.9984567901  0.9691358025  0.7962962963  0.7061728395  0.7864197531  0.7691358025  17.283950617  0.8245678616  0.3833422661  350           0.0750072336 
0.8422839506  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8518518519  0.7580246914  0.8827160494  0.8765432099  19.753086419  0.8074791205  0.3833422661  400           0.0744452667 
0.8466049383  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8728395062  0.7567901235  0.8802469136  0.8765432099  22.222222222  0.7846655989  0.3833422661  450           0.0755126858 
0.8268518519  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8543209877  0.7493827160  0.8654320988  0.8382716049  24.691358024  0.7528272831  0.3833422661  500           0.0758411074 
0.8487654321  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8691358025  0.7666666667  0.8839506173  0.8753086420  27.160493827  0.7737342572  0.3833422661  550           0.0745515013 
0.8376543210  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8641975309  0.7493827160  0.8666666667  0.8703703704  29.629629629  0.7471002114  0.3833422661  600           0.0761893749 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1171, in _next_data
    self._shutdown_workers()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1297, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 124, in join
    res = self._popen.wait(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 47, in wait
    if not wait([self.sentinel], timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1.27
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3660493827  0.3842592593  0.3888888889  0.3765432099  0.3765432099  0.3827160494  0.3765432099  0.3765432099  0.4012345679  0.3604938272  0.3753086420  0.3617283951  0.3666666667  0.0000000000  4.5427684784  0.1364407539  0             0.3862948418 
0.7929012346  0.9753086420  0.9876543210  0.9861111111  0.9567901235  0.9521604938  0.9629629630  0.9753086420  0.9259259259  0.7691358025  0.7703703704  0.8000000000  0.8320987654  2.4691358025  2.4050193357  0.3011693954  50            0.0743712521 
0.8327160494  0.9969135802  0.9876543210  1.0000000000  0.9876543210  0.9922839506  0.9876543210  0.9953703704  0.9629629630  0.8493827160  0.7604938272  0.8382716049  0.8827160494  4.9382716049  1.2181714833  0.3011693954  100           0.0752767181 
0.8459876543  0.9984567901  0.9938271605  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9969135802  0.9629629630  0.8604938272  0.7530864198  0.8716049383  0.8987654321  7.4074074074  0.9498276925  0.3012123108  150           0.0755370474 
0.8475308642  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9691358025  0.8666666667  0.7555555556  0.8703703704  0.8975308642  9.8765432099  0.9020955324  0.3013591766  200           0.0770317650 
0.8373456790  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.8506172840  0.7716049383  0.8716049383  0.8555555556  12.345679012  0.8631722546  0.3013591766  250           0.0753989983 
0.8395061728  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8481481481  0.7567901235  0.8679012346  0.8851851852  14.814814814  0.8549973786  0.3013591766  300           0.0747894096 
0.7987654321  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9984567901  1.0000000000  1.0000000000  0.9814814815  0.8345679012  0.7320987654  0.8271604938  0.8012345679  17.283950617  0.8130050993  0.3013591766  350           0.0740762281 
0.8385802469  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8629629630  0.7518518519  0.8765432099  0.8629629630  19.753086419  0.7908947122  0.3013591766  400           0.0740813684 
0.8327160494  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8456790123  0.7382716049  0.8765432099  0.8703703704  22.222222222  0.7756559229  0.3013591766  450           0.0751421261 
0.8388888889  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8530864198  0.7481481481  0.8839506173  0.8703703704  24.691358024  0.7627494442  0.3013591766  500           0.0758332109 
0.8305555556  0.9984567901  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.8296296296  0.7604938272  0.8518518519  0.8802469136  27.160493827  0.7824980223  0.3013591766  550           0.0749530697 
0.8333333333  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8592592593  0.7469135802  0.8592592593  0.8679012346  29.629629629  0.7564310503  0.3020892143  600           0.0758224916 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2298, in update
    meta_train_loss_dg.backward(create_graph=True)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3734567901  0.3919753086  0.3827160494  0.3796296296  0.3950617284  0.3950617284  0.3703703704  0.3858024691  0.4012345679  0.3654320988  0.3839506173  0.3691358025  0.3753086420  0.0000000000  4.5427684784  0.1364407539  0             0.3935709000 
0.7962962963  0.9830246914  0.9814814815  0.9861111111  0.9629629630  0.9567901235  0.9691358025  0.9768518519  0.9320987654  0.7716049383  0.7814814815  0.8061728395  0.8259259259  2.4691358025  2.3873885417  0.3011631966  50            0.0759791899 
0.8487654321  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9907407407  1.0000000000  0.9938271605  0.9691358025  0.8641975309  0.7827160494  0.8703703704  0.8777777778  4.9382716049  1.2025333560  0.3011631966  100           0.0780317736 
0.8453703704  0.9984567901  0.9938271605  0.9984567901  0.9938271605  1.0000000000  0.9876543210  0.9969135802  0.9691358025  0.8617283951  0.7617283951  0.8580246914  0.9000000000  7.4074074074  0.9598318362  0.3011631966  150           0.0770274544 
0.8506172840  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9969135802  0.9691358025  0.8728395062  0.7617283951  0.8641975309  0.9037037037  9.8765432099  0.9220091891  0.3011631966  200           0.0763326120 
0.8271604938  1.0000000000  1.0000000000  0.9984567901  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9876543210  0.8555555556  0.7691358025  0.8567901235  0.8271604938  12.345679012  0.8847013509  0.3011631966  250           0.0783563519 
0.8305555556  0.9969135802  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.9984567901  0.9691358025  0.8518518519  0.7382716049  0.8469135802  0.8851851852  14.814814814  0.8940472758  0.3011631966  300           0.0847026110 
0.8061728395  0.9984567901  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8469135802  0.7481481481  0.8271604938  0.8024691358  17.283950617  0.8480069673  0.3011689186  350           0.0804360342 
0.8317901235  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9876543210  0.8395061728  0.7617283951  0.8691358025  0.8567901235  19.753086419  0.8084160054  0.3011689186  400           0.0788657093 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 896, in __init__
    index_queue = multiprocessing_context.Queue()  # type: ignore
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 102, in Queue
    return Queue(maxsize, ctx=self.get_context())
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 41, in __init__
    self._reader, self._writer = connection.Pipe(duplex=False)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/linecache.py", line 74, in checkcache
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
RuntimeError: DataLoader worker (pid 3433254) is killed by signal: Terminated. 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.27
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 3451160) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.27
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3762345679  0.3950617284  0.3888888889  0.3827160494  0.3950617284  0.3996913580  0.3765432099  0.3842592593  0.4074074074  0.3666666667  0.3864197531  0.3728395062  0.3790123457  0.0000000000  4.5427684784  0.1364407539  0             0.3845508099 
0.7805555556  0.9768518519  0.9629629630  0.9737654321  0.9567901235  0.9475308642  0.9382716049  0.9706790123  0.9320987654  0.7395061728  0.7691358025  0.8024691358  0.8111111111  2.4691358025  2.3789997363  0.3011689186  50            0.0751093864 
0.8317901235  0.9953703704  0.9876543210  0.9984567901  0.9938271605  0.9938271605  0.9876543210  0.9953703704  0.9567901235  0.8493827160  0.7567901235  0.8382716049  0.8827160494  4.9382716049  1.2502559590  0.3011689186  100           0.0750931835 
0.8320987654  0.9984567901  0.9876543210  0.9984567901  1.0000000000  1.0000000000  0.9876543210  0.9969135802  0.9567901235  0.8530864198  0.7432098765  0.8444444444  0.8876543210  7.4074074074  0.9610146737  0.3011689186  150           0.0753119946 
0.8515432099  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9938271605  0.9984567901  0.9691358025  0.8777777778  0.7555555556  0.8691358025  0.9037037037  9.8765432099  0.9162518382  0.3011689186  200           0.0748382521 
0.8614197531  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9629629630  0.8740740741  0.7691358025  0.8925925926  0.9098765432  12.345679012  0.8824813139  0.3011689186  250           0.0794320536 
0.8604938272  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8691358025  0.7716049383  0.8962962963  0.9049382716  14.814814814  0.8905579829  0.3011689186  300           0.0766609764 
0.8583333333  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8629629630  0.7802469136  0.8975308642  0.8925925926  17.283950617  0.8897003472  0.3011689186  350           0.0752061844 
0.8577160494  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8666666667  0.7728395062  0.8901234568  0.9012345679  19.753086419  0.8816904843  0.3861279488  400           0.0770854998 
0.8589506173  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8679012346  0.7827160494  0.9012345679  0.8839506173  22.222222222  0.8910470927  0.3861279488  450           0.0755971289 
0.8570987654  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8654320988  0.7740740741  0.8925925926  0.8962962963  24.691358024  0.8645925689  0.3861279488  500           0.0769404507 
0.8592592593  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8728395062  0.7716049383  0.8962962963  0.8962962963  27.160493827  0.8852713633  0.3861279488  550           0.0773408604 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.27
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3293209877  0.3117283951  0.3086419753  0.3904320988  0.3888888889  0.3796296296  0.3333333333  0.3472222222  0.3395061728  0.3234567901  0.3345679012  0.3160493827  0.3432098765  0.0000000000  4.5427684784  0.1364407539  0             0.3721742630 
0.7814814815  0.8611111111  0.8024691358  0.8209876543  0.8148148148  0.8179012346  0.7901234568  0.8317901235  0.8086419753  0.8049382716  0.7234567901  0.7864197531  0.8111111111  2.4691358025  3.1294067383  0.3011631966  50            0.0737769270 
0.7787037037  0.9768518519  0.9382716049  0.9722222222  0.9444444444  0.9459876543  0.9382716049  0.9691358025  0.9444444444  0.7530864198  0.7493827160  0.7938271605  0.8185185185  4.9382716049  2.0443388820  0.3011631966  100           0.0742933702 
0.8101851852  0.9922839506  0.9753086420  0.9891975309  0.9629629630  0.9753086420  0.9506172840  0.9814814815  0.9567901235  0.8012345679  0.7543209877  0.8333333333  0.8518518519  7.4074074074  1.5391199350  0.3011631966  150           0.0762116432 
0.7966049383  0.9969135802  0.9876543210  0.9953703704  0.9876543210  0.9922839506  1.0000000000  0.9922839506  0.9567901235  0.7851851852  0.7456790123  0.8135802469  0.8419753086  9.8765432099  1.1969802856  0.3011631966  200           0.0762060881 
0.8317901235  0.9969135802  0.9938271605  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9953703704  0.9567901235  0.8320987654  0.7765432099  0.8456790123  0.8728395062  12.345679012  1.0472196543  0.3011631966  250           0.0745456743 
0.8070987654  0.9953703704  0.9938271605  1.0000000000  1.0000000000  0.9953703704  1.0000000000  0.9969135802  0.9567901235  0.7925925926  0.7641975309  0.8259259259  0.8456790123  14.814814814  0.9721242106  0.3011631966  300           0.0755514145 
0.8265432099  0.9984567901  1.0000000000  1.0000000000  1.0000000000  0.9907407407  0.9938271605  0.9969135802  0.9753086420  0.8234567901  0.7913580247  0.8432098765  0.8481481481  17.283950617  0.9488990557  0.3011631966  350           0.0742620087 
0.8348765432  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9629629630  0.8419753086  0.7753086420  0.8506172840  0.8716049383  19.753086419  0.9038252449  0.3011631966  400           0.0748599911 
0.8003086420  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9691358025  0.7802469136  0.7604938272  0.8246913580  0.8358024691  22.222222222  0.9046211207  0.3020715714  450           0.0767934275 
0.8243827160  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9567901235  0.8283950617  0.7617283951  0.8432098765  0.8641975309  24.691358024  0.8705210686  0.3020715714  500           0.0766733074 
0.8376543210  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9814814815  0.8481481481  0.7802469136  0.8555555556  0.8666666667  27.160493827  0.8831840146  0.3020715714  550           0.0828485298 
0.8317901235  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9938271605  0.9984567901  0.9691358025  0.8506172840  0.7604938272  0.8432098765  0.8728395062  29.629629629  0.8674445164  0.3020715714  600           0.0807084274 
0.8188271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9629629630  0.8197530864  0.7592592593  0.8419753086  0.8543209877  32.098765432  0.8359605706  0.3020715714  650           0.0785563898 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2298, in update
    meta_train_loss_dg.backward(create_graph=True)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt
trails: 0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0001
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3212962963  0.3194444444  0.3333333333  0.3595679012  0.3395061728  0.3611111111  0.3271604938  0.3425925926  0.3271604938  0.3259259259  0.3185185185  0.3123456790  0.3283950617  0.0000000000  1.1818408966  0.0539774895  0             0.3249566555 
0.7876543210  0.8873456790  0.8518518519  0.8904320988  0.8333333333  0.8703703704  0.8148148148  0.8811728395  0.8395061728  0.8061728395  0.7432098765  0.7913580247  0.8098765432  2.4691358025  0.7085318673  0.0604319572  50            0.0121601295 
0.7845679012  0.9891975309  0.9814814815  0.9938271605  0.9814814815  0.9783950617  0.9753086420  0.9768518519  0.9382716049  0.7382716049  0.7654320988  0.8037037037  0.8308641975  4.9382716049  0.2780550751  0.0629329681  100           0.0133445978 
0.8089506173  0.9938271605  0.9938271605  0.9984567901  0.9938271605  0.9845679012  0.9876543210  0.9907407407  0.9444444444  0.7765432099  0.7703703704  0.8333333333  0.8555555556  7.4074074074  0.1096547963  0.0629329681  150           0.0128163385 
0.7919753086  0.9953703704  0.9938271605  1.0000000000  0.9938271605  0.9984567901  1.0000000000  0.9953703704  0.9444444444  0.7703703704  0.7382716049  0.8148148148  0.8444444444  9.8765432099  0.0537597116  0.0629329681  200           0.0121299887 
0.8064814815  0.9984567901  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9969135802  0.9506172840  0.7950617284  0.7407407407  0.8333333333  0.8567901235  12.345679012  0.0281533571  0.0629329681  250           0.0123127699 
0.8138888889  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9969135802  0.9567901235  0.8123456790  0.7395061728  0.8345679012  0.8691358025  14.814814814  0.0214861989  0.0629329681  300           0.0126127815 
0.8435185185  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9629629630  0.8407407407  0.7827160494  0.8666666667  0.8839506173  17.283950617  0.0140513372  0.0634121895  350           0.0125034237 
0.8191358025  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9506172840  0.8061728395  0.7530864198  0.8456790123  0.8716049383  19.753086419  0.0097339738  0.0634121895  400           0.0130543566 
0.8351851852  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8271604938  0.7765432099  0.8629629630  0.8740740741  22.222222222  0.0063612482  0.0634121895  450           0.0124833822 
0.8330246914  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9691358025  0.8246913580  0.7641975309  0.8654320988  0.8777777778  24.691358024  0.0071640660  0.0634121895  500           0.0125103951 
0.8370370370  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8370370370  0.7703703704  0.8629629630  0.8777777778  27.160493827  0.0053696915  0.0634121895  550           0.0125236607 
0.8401234568  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8407407407  0.7753086420  0.8716049383  0.8728395062  29.629629629  0.0040698151  0.0634121895  600           0.0125126457 
0.8410493827  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8358024691  0.7790123457  0.8716049383  0.8777777778  32.098765432  0.0032599581  0.0634121895  650           0.0125232887 
0.8441358025  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8395061728  0.7814814815  0.8728395062  0.8827160494  34.567901234  0.0023510342  0.0634121895  700           0.0136379242 
0.8429012346  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8395061728  0.7925925926  0.8691358025  0.8703703704  37.037037037  0.0024889680  0.0634121895  750           0.0125217390 
0.8453703704  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8444444444  0.7790123457  0.8765432099  0.8814814815  39.506172839  0.0020323968  0.0634121895  800           0.0144699717 
0.8475308642  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8444444444  0.7888888889  0.8777777778  0.8790123457  41.975308642  0.0017873651  0.0634121895  850           0.0142819214 
0.8469135802  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8493827160  0.7802469136  0.8691358025  0.8888888889  44.444444444  0.0014867713  0.0634121895  900           0.0126880980 
0.8537037037  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8555555556  0.7864197531  0.8777777778  0.8950617284  46.913580246  0.0018082693  0.0634121895  950           0.0128905439 
0.8441358025  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8419753086  0.7790123457  0.8703703704  0.8851851852  49.382716049  0.0015948786  0.0634121895  1000          0.0137335110 
0.8493827160  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8493827160  0.7851851852  0.8777777778  0.8851851852  51.851851851  0.0009029198  0.0634121895  1050          0.0138321972 
0.8416666667  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8395061728  0.7777777778  0.8679012346  0.8814814815  54.320987654  0.0008764266  0.0634121895  1100          0.0126875639 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 63, in handler
    def handler(signum, frame):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 3546067) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.27
	class_balanced: False
	data_augmentation: True
	heldout_p: 500
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3762345679  0.3950617284  0.3888888889  0.3827160494  0.3950617284  0.3996913580  0.3765432099  0.3842592593  0.4074074074  0.3666666667  0.3864197531  0.3728395062  0.3790123457  0.0000000000  4.5427684784  0.1364407539  0             0.3875102997 
0.7805555556  0.9768518519  0.9629629630  0.9737654321  0.9567901235  0.9475308642  0.9382716049  0.9706790123  0.9320987654  0.7395061728  0.7691358025  0.8024691358  0.8111111111  2.4691358025  2.3789997363  0.3011631966  50            0.0744957352 
0.8317901235  0.9953703704  0.9876543210  0.9984567901  0.9938271605  0.9938271605  0.9876543210  0.9953703704  0.9567901235  0.8493827160  0.7567901235  0.8382716049  0.8827160494  4.9382716049  1.2502559590  0.3011846542  100           0.0751170826 
0.8320987654  0.9984567901  0.9876543210  0.9984567901  1.0000000000  1.0000000000  0.9876543210  0.9969135802  0.9567901235  0.8530864198  0.7432098765  0.8444444444  0.8876543210  7.4074074074  0.9610146737  0.3832688332  150           0.0752487993 
0.8515432099  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9938271605  0.9984567901  0.9691358025  0.8777777778  0.7555555556  0.8691358025  0.9037037037  9.8765432099  0.9162518382  0.3832688332  200           0.0767845106 
0.8336419753  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9814814815  0.8654320988  0.7666666667  0.8580246914  0.8444444444  12.345679012  0.8767253423  0.3832688332  250           0.0752147913 
0.8398148148  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.8358024691  0.7740740741  0.8740740741  0.8753086420  14.814814814  0.8803340375  0.3832688332  300           0.0748620653 
0.7645061728  0.9938271605  1.0000000000  1.0000000000  0.9938271605  0.9984567901  0.9938271605  0.9984567901  0.9691358025  0.7962962963  0.7061728395  0.7864197531  0.7691358025  17.283950617  0.8245678616  0.3832902908  350           0.0744118404 
0.8422839506  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8518518519  0.7580246914  0.8827160494  0.8765432099  19.753086419  0.8074791205  0.3832902908  400           0.0752467966 
0.8466049383  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8728395062  0.7567901235  0.8802469136  0.8765432099  22.222222222  0.7846655989  0.3832902908  450           0.0763381004 
0.8268518519  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8543209877  0.7493827160  0.8654320988  0.8382716049  24.691358024  0.7528272831  0.3832902908  500           0.0764012527 
0.8487654321  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8691358025  0.7666666667  0.8839506173  0.8753086420  27.160493827  0.7737342572  0.3832902908  550           0.0756385088 
0.8376543210  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8641975309  0.7493827160  0.8666666667  0.8703703704  29.629629629  0.7471002114  0.3832902908  600           0.0759995556 
0.8388888889  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8580246914  0.7629629630  0.8716049383  0.8629629630  32.098765432  0.7107588804  0.3833518028  650           0.0760543537 
0.8361111111  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8469135802  0.7543209877  0.8617283951  0.8814814815  34.567901234  0.7113220787  0.3833518028  700           0.0776111889 
0.8462962963  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8728395062  0.7518518519  0.8716049383  0.8888888889  37.037037037  0.7396599495  0.3833518028  750           0.0781433010 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2276, in update
    meta_train_loss_main.backward(retain_graph=True)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.7
	class_balanced: False
	data_augmentation: True
	heldout_p: 500
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3709876543  0.3904320988  0.3827160494  0.3780864198  0.3765432099  0.3935185185  0.3765432099  0.3811728395  0.4074074074  0.3641975309  0.3814814815  0.3641975309  0.3740740741  0.0000000000  4.5427684784  0.1364407539  0             0.3939085007 
0.7959876543  0.9814814815  0.9938271605  0.9830246914  0.9691358025  0.9583333333  0.9753086420  0.9737654321  0.9382716049  0.7580246914  0.7814814815  0.8148148148  0.8296296296  2.4691358025  2.3607964563  0.3011846542  50            0.0743105459 
0.8305555556  0.9969135802  0.9938271605  1.0000000000  0.9938271605  0.9922839506  0.9938271605  0.9953703704  0.9506172840  0.8432098765  0.7641975309  0.8333333333  0.8814814815  4.9382716049  1.2457254350  0.3832688332  100           0.0755449915 
0.8425925926  0.9984567901  0.9938271605  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9969135802  0.9629629630  0.8555555556  0.7530864198  0.8641975309  0.8975308642  7.4074074074  0.9517836297  0.3832688332  150           0.0774770832 
0.8305555556  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9876543210  0.9938271605  0.9629629630  0.8641975309  0.7395061728  0.8407407407  0.8777777778  9.8765432099  0.8918179679  0.3832688332  200           0.0752962255 
0.8327160494  1.0000000000  1.0000000000  0.9969135802  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9876543210  0.8543209877  0.7703703704  0.8604938272  0.8456790123  12.345679012  0.8538982058  0.3832688332  250           0.0778602791 
0.8358024691  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9629629630  0.8320987654  0.7629629630  0.8691358025  0.8790123457  14.814814814  0.8452972972  0.3832688332  300           0.0832021141 
0.8040123457  0.9984567901  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8333333333  0.7530864198  0.8283950617  0.8012345679  17.283950617  0.7894769907  0.3832688332  350           0.0766237116 
0.8416666667  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8469135802  0.7691358025  0.8777777778  0.8728395062  19.753086419  0.7720579529  0.3832688332  400           0.0789999437 
Exception ignored in: <function WeakValueDictionary.__init__.<locals>.remove at 0x7f4c6c8e6ae8>
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/weakref.py", line 109, in remove
    def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):
KeyboardInterrupt: 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2164, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1476, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1483, in _exec
      File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 940, in __init__
    self._reset(loader, first_iter=True)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 971, in _reset
    self._try_put_index()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1216, in _try_put_index
    self._index_queues[worker_queue_idx].put((self._send_idx, index))
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 89, in put
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/threading.py", line 344, in notify


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 267, in in_project_roots
    return filename_to_in_scope_cache[filename]
KeyError: '/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 557, in get_abs_path_real_path_and_base_from_file
    return NORM_PATHS_AND_BASE_CONTAINER[f]
KeyError: '/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/typeshed/stubs/orjson'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 230, in _NormPaths
    return NORM_PATHS_CONTAINER[filename]
KeyError: '/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/typeshed/stubs/orjson'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2173, in <module>
    main()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2166, in main
    handle_keyboard_interrupt()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1959, in handle_keyboard_interrupt
    if debugger.in_project_scope(filename) and '_pydevd' not in filename:
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 607, in in_project_scope
    return pydevd_utils.in_project_roots(filename)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 280, in in_project_roots
    library_roots = _get_library_roots()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 261, in _get_library_roots
    return _get_roots(library_roots_cache, 'LIBRARY_ROOTS', set_library_roots, _get_default_library_roots)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 208, in _get_roots
    set_when_not_cached(roots)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 256, in set_library_roots
    roots = _set_roots(roots, _LIBRARY_ROOTS_CACHE)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 188, in _set_roots
    new_roots.append(_normpath(root))
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 27, in _normpath
    return pydevd_file_utils.get_abs_path_real_path_and_base_from_file(filename)[0]
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 571, in get_abs_path_real_path_and_base_from_file
    abs_path, real_path = _NormPaths(f)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 238, in _NormPaths
    real_path = _NormPath(filename, rPath)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 246, in _NormPath
    r = normpath(filename)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/posixpath.py", line 395, in realpath
    path, ok = _joinrealpath(filename[:0], filename, {})
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/posixpath.py", line 429, in _joinrealpath
    if not islink(newpath):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/posixpath.py", line 171, in islink
    st = os.lstat(path)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 3576378) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.15
	class_balanced: False
	data_augmentation: True
	heldout_p: 50
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3774691358  0.3981481481  0.3950617284  0.3842592593  0.3950617284  0.3996913580  0.3765432099  0.3858024691  0.4012345679  0.3666666667  0.3876543210  0.3753086420  0.3802469136  0.0000000000  4.5427684784  0.1364407539  0             0.3824238777 
0.7990740741  0.9799382716  0.9938271605  0.9922839506  0.9753086420  0.9629629630  0.9506172840  0.9753086420  0.9382716049  0.7654320988  0.7765432099  0.8148148148  0.8395061728  2.4691358025  2.3984872651  0.3011631966  50            0.0744926023 
0.8413580247  0.9969135802  0.9876543210  1.0000000000  0.9876543210  0.9907407407  0.9938271605  0.9938271605  0.9567901235  0.8580246914  0.7679012346  0.8567901235  0.8827160494  4.9382716049  1.2056292307  0.3011693954  100           0.0745342112 
0.8543209877  0.9984567901  0.9876543210  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9969135802  0.9629629630  0.8765432099  0.7592592593  0.8740740741  0.9074074074  7.4074074074  0.9499684620  0.3011693954  150           0.0754124737 
0.8404320988  1.0000000000  0.9814814815  1.0000000000  0.9876543210  1.0000000000  0.9938271605  0.9969135802  0.9629629630  0.8777777778  0.7407407407  0.8469135802  0.8962962963  9.8765432099  0.9134629130  0.3011693954  200           0.0746393871 
0.8287037037  1.0000000000  0.9938271605  0.9969135802  0.9938271605  0.9984567901  1.0000000000  0.9984567901  0.9876543210  0.8679012346  0.7580246914  0.8567901235  0.8320987654  12.345679012  0.8771137536  0.3011693954  250           0.0751467037 
0.8314814815  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9984567901  0.9753086420  0.8506172840  0.7382716049  0.8493827160  0.8876543210  14.814814814  0.8776236558  0.3011693954  300           0.0755179548 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 3595016) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.15
	class_balanced: False
	data_augmentation: True
	heldout_p: 0.0
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3774691358  0.3981481481  0.3950617284  0.3842592593  0.3950617284  0.3996913580  0.3765432099  0.3858024691  0.4012345679  0.3666666667  0.3876543210  0.3753086420  0.3802469136  0.0000000000  4.5427684784  0.1364407539  0             0.3791551590 
0.7990740741  0.9799382716  0.9938271605  0.9922839506  0.9753086420  0.9629629630  0.9506172840  0.9753086420  0.9382716049  0.7654320988  0.7765432099  0.8148148148  0.8395061728  2.4691358025  2.3984872651  0.3011693954  50            0.0740930080 
0.8413580247  0.9969135802  0.9876543210  1.0000000000  0.9876543210  0.9907407407  0.9938271605  0.9938271605  0.9567901235  0.8580246914  0.7679012346  0.8567901235  0.8827160494  4.9382716049  1.2056292307  0.3012123108  100           0.0737617111 
0.8543209877  0.9984567901  0.9876543210  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9969135802  0.9629629630  0.8765432099  0.7592592593  0.8740740741  0.9074074074  7.4074074074  0.9499684620  0.3012123108  150           0.0751602888 
0.8404320988  1.0000000000  0.9814814815  1.0000000000  0.9876543210  1.0000000000  0.9938271605  0.9969135802  0.9629629630  0.8777777778  0.7407407407  0.8469135802  0.8962962963  9.8765432099  0.9134629130  0.3012123108  200           0.0740957546 
0.8287037037  1.0000000000  0.9938271605  0.9969135802  0.9938271605  0.9984567901  1.0000000000  0.9984567901  0.9876543210  0.8679012346  0.7580246914  0.8567901235  0.8320987654  12.345679012  0.8771137536  0.3012123108  250           0.0759674120 
0.8314814815  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9984567901  0.9753086420  0.8506172840  0.7382716049  0.8493827160  0.8876543210  14.814814814  0.8776236558  0.3012123108  300           0.0785794401 
0.8179012346  0.9984567901  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8419753086  0.7617283951  0.8567901235  0.8111111111  17.283950617  0.8056404543  0.3012123108  350           0.0758623266 
0.8228395062  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8320987654  0.7493827160  0.8580246914  0.8518518519  19.753086419  0.7672930622  0.3012123108  400           0.0749864006 
0.8351851852  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.8555555556  0.7530864198  0.8691358025  0.8629629630  22.222222222  0.7850404823  0.3012123108  450           0.0804455280 
0.8216049383  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8518518519  0.7567901235  0.8481481481  0.8296296296  24.691358024  0.7354812026  0.3012123108  500           0.0765967560 
0.8450617284  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.8580246914  0.7641975309  0.8740740741  0.8839506173  27.160493827  0.7420019662  0.3833365440  550           0.0771836805 
0.8395061728  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9876543210  0.8765432099  0.7506172840  0.8654320988  0.8654320988  29.629629629  0.7281244612  0.3833365440  600           0.0777683067 
0.8083333333  0.9984567901  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.9984567901  0.9629629630  0.8259259259  0.7246913580  0.8234567901  0.8592592593  32.098765432  0.6919941080  0.3833365440  650           0.0749916601 
0.8194444444  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8456790123  0.7320987654  0.8407407407  0.8592592593  34.567901234  0.7398543668  0.3833365440  700           0.0754636860 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.0
	class_balanced: False
	data_augmentation: True
	heldout_p: 0.0
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3790123457  0.3981481481  0.3950617284  0.3904320988  0.4012345679  0.3996913580  0.3827160494  0.3858024691  0.4012345679  0.3703703704  0.3888888889  0.3740740741  0.3827160494  0.0000000000  4.5427684784  0.1364407539  0             0.3843083382 
0.7975308642  0.9583333333  0.9629629630  0.9567901235  0.9197530864  0.9351851852  0.9320987654  0.9629629630  0.9382716049  0.7629629630  0.7876543210  0.8111111111  0.8283950617  2.4691358025  2.4091931987  0.3011693954  50            0.0797871685 
0.8432098765  0.9953703704  0.9876543210  1.0000000000  0.9876543210  0.9891975309  0.9938271605  0.9953703704  0.9629629630  0.8580246914  0.7703703704  0.8506172840  0.8938271605  4.9382716049  1.2822417998  0.3011908531  100           0.0765927505 
0.8450617284  0.9984567901  0.9876543210  0.9984567901  0.9938271605  0.9984567901  0.9938271605  0.9969135802  0.9629629630  0.8629629630  0.7592592593  0.8604938272  0.8975308642  7.4074074074  0.9676210594  0.3011908531  150           0.0775762892 
0.8487654321  1.0000000000  0.9814814815  1.0000000000  0.9938271605  0.9984567901  0.9938271605  1.0000000000  0.9629629630  0.8777777778  0.7469135802  0.8666666667  0.9037037037  9.8765432099  0.9108840907  0.3011908531  200           0.0758793783 
0.7907407407  0.9969135802  0.9814814815  0.9876543210  0.9814814815  0.9953703704  0.9876543210  0.9984567901  0.9876543210  0.8296296296  0.7135802469  0.8209876543  0.7987654321  12.345679012  0.8922284460  0.3011908531  250           0.0804838800 
Python 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.16.1 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 7.16.1

Out[1]: tensor(0.6931, device='cuda:0', grad_fn=<MeanBackward0>)
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2257, in update
    loss_dg = self.hparams['beta'] * self.omega(feat_a)  # aux_loss eq.6/7
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.15
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
Python 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.16.1 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 7.16.1

Out[1]: tensor(0.6149, device='cuda:0', grad_fn=<MeanBackward0>)
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3774691358  0.3981481481  0.3950617284  0.3842592593  0.3950617284  0.3996913580  0.3765432099  0.3858024691  0.4012345679  0.3666666667  0.3876543210  0.3753086420  0.3802469136  0.0000000000  4.5427684784  0.1364469528  0             95.213794946 
0.7990740741  0.9799382716  0.9938271605  0.9922839506  0.9753086420  0.9629629630  0.9506172840  0.9753086420  0.9382716049  0.7654320988  0.7765432099  0.8148148148  0.8395061728  2.4691358025  2.3984872651  0.3728079796  50            0.0728461170 
0.8413580247  0.9969135802  0.9876543210  1.0000000000  0.9876543210  0.9907407407  0.9938271605  0.9938271605  0.9567901235  0.8580246914  0.7679012346  0.8567901235  0.8827160494  4.9382716049  1.2056292307  0.3728079796  100           0.0774004269 
0.8543209877  0.9984567901  0.9876543210  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9969135802  0.9629629630  0.8765432099  0.7592592593  0.8740740741  0.9074074074  7.4074074074  0.9499684620  0.3728079796  150           0.0784597349 
0.8404320988  1.0000000000  0.9814814815  1.0000000000  0.9876543210  1.0000000000  0.9938271605  0.9969135802  0.9629629630  0.8777777778  0.7407407407  0.8469135802  0.8962962963  9.8765432099  0.9134629130  0.3728246689  200           0.0816561460 
0.8287037037  1.0000000000  0.9938271605  0.9969135802  0.9938271605  0.9984567901  1.0000000000  0.9984567901  0.9876543210  0.8679012346  0.7580246914  0.8567901235  0.8320987654  12.345679012  0.8771137536  0.3728308678  250           0.0761568737 
0.8314814815  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9984567901  0.9753086420  0.8506172840  0.7382716049  0.8493827160  0.8876543210  14.814814814  0.8776236558  0.4549641609  300           0.0763217926 
0.8179012346  0.9984567901  1.0000000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8419753086  0.7617283951  0.8567901235  0.8111111111  17.283950617  0.8056404543  0.4549641609  350           0.0767767954 
0.8228395062  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8320987654  0.7493827160  0.8580246914  0.8518518519  19.753086419  0.7672930622  0.4549641609  400           0.0786325264 
0.8351851852  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.8555555556  0.7530864198  0.8691358025  0.8629629630  22.222222222  0.7850404823  0.4549641609  450           0.0814266014 
0.8216049383  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8518518519  0.7567901235  0.8481481481  0.8296296296  24.691358024  0.7354812026  0.4549641609  500           0.0766754675 
0.8450617284  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.8580246914  0.7641975309  0.8740740741  0.8839506173  27.160493827  0.7420019662  0.4549641609  550           0.0782590675 
0.8395061728  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9876543210  0.8765432099  0.7506172840  0.8654320988  0.8654320988  29.629629629  0.7281244612  0.4549641609  600           0.0771420622 
0.8083333333  0.9984567901  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.9984567901  0.9629629630  0.8259259259  0.7246913580  0.8234567901  0.8592592593  32.098765432  0.6919941080  0.4549641609  650           0.0807331562 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2265, in update
    penalty += self.mmd(feat_a, feat_ab)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2200, in mmd
    Kyy = self.gaussian_kernel(y, y).mean()
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2193, in gaussian_kernel
    K.add_(torch.exp(D.mul(-g)))
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.15
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3771604938  0.3966049383  0.3950617284  0.3858024691  0.3950617284  0.3981481481  0.3827160494  0.3858024691  0.4074074074  0.3666666667  0.3876543210  0.3753086420  0.3790123457  0.0000000000  4.5427684784  0.1365151405  0             0.3991858959 
0.8336419753  0.9814814815  0.9814814815  0.9753086420  0.9753086420  0.9567901235  0.9691358025  0.9799382716  0.9382716049  0.8086419753  0.7950617284  0.8493827160  0.8814814815  2.4691358025  2.3766716361  0.3013849258  50            0.0747088289 
0.8524691358  0.9984567901  0.9938271605  0.9984567901  0.9876543210  0.9938271605  0.9938271605  0.9922839506  0.9629629630  0.8654320988  0.7814814815  0.8777777778  0.8851851852  4.9382716049  1.1664525509  0.3013849258  100           0.0751790428 
0.8429012346  1.0000000000  0.9876543210  0.9938271605  0.9876543210  0.9922839506  1.0000000000  0.9922839506  0.9629629630  0.8814814815  0.7691358025  0.8703703704  0.8506172840  7.4074074074  0.9593203366  0.3014464378  150           0.0754447651 
0.8283950617  0.9984567901  1.0000000000  1.0000000000  1.0000000000  0.9969135802  0.9938271605  0.9969135802  0.9691358025  0.8407407407  0.7617283951  0.8481481481  0.8629629630  9.8765432099  0.9090199256  0.3023519516  200           0.0736787224 
0.8120370370  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9984567901  1.0000000000  0.9969135802  0.9753086420  0.8567901235  0.7382716049  0.8345679012  0.8185185185  12.345679012  0.9334980750  0.3023519516  250           0.0763078260 
0.8487654321  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9984567901  0.9814814815  0.8827160494  0.7506172840  0.8728395062  0.8888888889  14.814814814  0.8615004838  0.3023519516  300           0.0780574465 
0.8243827160  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  1.0000000000  0.9984567901  0.9814814815  0.8308641975  0.7703703704  0.8543209877  0.8419753086  17.283950617  0.8904096127  0.3023519516  350           0.0749969864 
0.8410493827  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9984567901  0.9814814815  0.8617283951  0.7555555556  0.8703703704  0.8765432099  19.753086419  0.8499955595  0.3023519516  400           0.0753516531 
0.8388888889  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8666666667  0.7493827160  0.8740740741  0.8654320988  22.222222222  0.8353864861  0.3023519516  450           0.0781985378 
0.8453703704  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8716049383  0.7543209877  0.8765432099  0.8790123457  24.691358024  0.8308809090  0.3023519516  500           0.0785616398 
0.8382716049  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8555555556  0.7666666667  0.8716049383  0.8592592593  27.160493827  0.8023234463  0.3023519516  550           0.0755394459 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1171, in _next_data
    self._shutdown_workers()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1297, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 124, in join
    res = self._popen.wait(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 47, in wait
    if not wait([self.sentinel], timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3734567901  0.3842592593  0.3827160494  0.3827160494  0.3765432099  0.3966049383  0.3765432099  0.3765432099  0.4074074074  0.3679012346  0.3827160494  0.3691358025  0.3740740741  0.0000000000  4.5427684784  0.1365151405  0             0.3962306976 
0.8398148148  0.9753086420  0.9753086420  0.9660493827  0.9567901235  0.9583333333  0.9567901235  0.9799382716  0.9259259259  0.8234567901  0.7938271605  0.8592592593  0.8827160494  2.4691358025  2.3928745341  0.3013696671  50            0.0730925322 
0.8500000000  0.9953703704  1.0000000000  0.9984567901  0.9876543210  0.9907407407  1.0000000000  0.9922839506  0.9691358025  0.8740740741  0.7888888889  0.8716049383  0.8654320988  4.9382716049  1.1599734735  0.3013696671  100           0.0731029224 
0.8311728395  0.9984567901  1.0000000000  0.9953703704  0.9938271605  0.9938271605  1.0000000000  0.9938271605  0.9629629630  0.8716049383  0.7691358025  0.8543209877  0.8296296296  7.4074074074  0.9475052893  0.3013758659  150           0.0746899986 
0.8382716049  0.9984567901  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9938271605  0.9984567901  0.9814814815  0.8753086420  0.7604938272  0.8555555556  0.8617283951  9.8765432099  0.9019617820  0.3013758659  200           0.0743275738 
0.8117283951  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9969135802  1.0000000000  0.9984567901  0.9691358025  0.8604938272  0.7395061728  0.8407407407  0.8061728395  12.345679012  0.9150745618  0.3013758659  250           0.0751256561 
0.8537037037  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8962962963  0.7555555556  0.8888888889  0.8740740741  14.814814814  0.8511524522  0.3013758659  300           0.0764137840 
0.8478395062  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8790123457  0.7592592593  0.8814814815  0.8716049383  17.283950617  0.8388361502  0.3013758659  350           0.0739715481 
0.8537037037  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.8913580247  0.7518518519  0.8839506173  0.8876543210  19.753086419  0.8003602409  0.3013758659  400           0.0718713427 
0.8459876543  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8641975309  0.7691358025  0.8740740741  0.8765432099  22.222222222  0.7906139934  0.3013758659  450           0.0730342865 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2348, in update
    torch.cuda.empty_cache()  # GPU
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/cuda/memory.py", line 114, in empty_cache
    torch._C._cuda_emptyCache()
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 500
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3734567901  0.3842592593  0.3827160494  0.3827160494  0.3765432099  0.3966049383  0.3765432099  0.3765432099  0.4074074074  0.3679012346  0.3827160494  0.3691358025  0.3740740741  0.0000000000  4.5427684784  0.1365151405  0             0.4033257961 
0.8398148148  0.9753086420  0.9753086420  0.9660493827  0.9567901235  0.9583333333  0.9567901235  0.9799382716  0.9259259259  0.8234567901  0.7938271605  0.8592592593  0.8827160494  2.4691358025  2.3928745341  0.3013696671  50            0.0755186605 
0.8500000000  0.9953703704  1.0000000000  0.9984567901  0.9876543210  0.9907407407  1.0000000000  0.9922839506  0.9691358025  0.8740740741  0.7888888889  0.8716049383  0.8654320988  4.9382716049  1.1599734735  0.3013696671  100           0.0743960619 
0.8311728395  0.9984567901  1.0000000000  0.9953703704  0.9938271605  0.9938271605  1.0000000000  0.9938271605  0.9629629630  0.8716049383  0.7691358025  0.8543209877  0.8296296296  7.4074074074  0.9475052893  0.3013696671  150           0.0752792788 
0.8382716049  0.9984567901  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9938271605  0.9984567901  0.9814814815  0.8753086420  0.7604938272  0.8555555556  0.8617283951  9.8765432099  0.9019617820  0.3013911247  200           0.0747571278 
0.8117283951  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9969135802  1.0000000000  0.9984567901  0.9691358025  0.8604938272  0.7395061728  0.8407407407  0.8061728395  12.345679012  0.9150745618  0.3013911247  250           0.0776582241 
0.8537037037  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8962962963  0.7555555556  0.8888888889  0.8740740741  14.814814814  0.8511524522  0.3013911247  300           0.0853477144 
0.8478395062  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8790123457  0.7592592593  0.8814814815  0.8716049383  17.283950617  0.8388361502  0.3013911247  350           0.0792938232 
0.8537037037  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.8913580247  0.7518518519  0.8839506173  0.8876543210  19.753086419  0.8003602409  0.3013911247  400           0.0826083612 
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 113, in get
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 282, in rebuild_storage_fd
    fd = df.detach()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py", line 58, in detach
    return reduction.recv_handle(conn)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/reduction.py", line 182, in recv_handle
    return recvfds(s, 1)[0]
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/reduction.py", line 153, in recvfds
    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_SPACE(bytes_size))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 999, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 3723497, 3723523, 3723542, 3723556, 3723572, 3723574, 3723613) exited unexpectedly
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 500
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3740740741  0.3580246914  0.3641975309  0.3827160494  0.3950617284  0.3873456790  0.3641975309  0.3950617284  0.3888888889  0.3703703704  0.3753086420  0.3691358025  0.3814814815  0.0000000000  4.5427684784  0.1365151405  0             0.3922030926 
0.8203703704  0.9614197531  0.9382716049  0.9521604938  0.9444444444  0.9459876543  0.9259259259  0.9598765432  0.9012345679  0.8098765432  0.7876543210  0.8407407407  0.8432098765  2.4691358025  2.6225488734  0.3013634682  50            0.0743993998 
0.8262345679  0.9938271605  1.0000000000  0.9984567901  1.0000000000  0.9891975309  0.9876543210  0.9876543210  0.9567901235  0.8111111111  0.7827160494  0.8543209877  0.8567901235  4.9382716049  1.4615026951  0.3013634682  100           0.0753799582 
0.8490740741  1.0000000000  1.0000000000  0.9984567901  0.9938271605  0.9907407407  1.0000000000  0.9953703704  0.9691358025  0.8851851852  0.8123456790  0.8580246914  0.8407407407  7.4074074074  1.0352772498  0.3013634682  150           0.0767298460 
0.8540123457  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9969135802  1.0000000000  0.9984567901  0.9753086420  0.8913580247  0.7864197531  0.8765432099  0.8617283951  9.8765432099  0.9168712008  0.3013634682  200           0.0751607132 
0.8447530864  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9969135802  1.0000000000  0.9984567901  0.9753086420  0.8888888889  0.7925925926  0.8617283951  0.8358024691  12.345679012  0.9179187334  0.3013634682  250           0.0800722408 
0.8546296296  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9984567901  0.9753086420  0.8839506173  0.7827160494  0.8802469136  0.8716049383  14.814814814  0.8680278015  0.3013634682  300           0.0736789179 
0.8469135802  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8765432099  0.7654320988  0.8567901235  0.8888888889  17.283950617  0.8647801638  0.3013634682  350           0.0755694723 
0.8564814815  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8950617284  0.7666666667  0.8765432099  0.8876543210  19.753086419  0.8290501559  0.3013634682  400           0.0773580933 
0.8404320988  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8802469136  0.7617283951  0.8679012346  0.8518518519  22.222222222  0.8182809544  0.3013849258  450           0.0796986866 
0.8472222222  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8901234568  0.7839506173  0.8691358025  0.8456790123  24.691358024  0.7787173021  0.3013849258  500           0.0788562441 
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 113, in get
    return _ForkingPickler.loads(res)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 282, in rebuild_storage_fd
    fd = df.detach()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 487, in Client
    c = SocketClient(address)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 614, in SocketClient
    s.connect(address)
FileNotFoundError: [Errno 2] No such file or directory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 999, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 3740877, 3740886) exited unexpectedly
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 500
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3694444444  0.3549382716  0.3641975309  0.3842592593  0.3950617284  0.3827160494  0.3765432099  0.3919753086  0.4135802469  0.3641975309  0.3740740741  0.3617283951  0.3777777778  0.0000000000  4.5427684784  0.1364655495  0             0.3854191303 
0.7111111111  0.8503086420  0.8395061728  0.8765432099  0.8395061728  0.8487654321  0.8333333333  0.8672839506  0.8209876543  0.7259259259  0.6913580247  0.7123456790  0.7148148148  2.4691358025  2.5861645436  0.3012299538  50            0.0751803589 
0.7799382716  0.9907407407  0.9938271605  1.0000000000  1.0000000000  0.9830246914  0.9938271605  0.9830246914  0.9444444444  0.7432098765  0.7555555556  0.7950617284  0.8259259259  4.9382716049  1.4860491395  0.3012299538  100           0.0734380341 
0.8327160494  0.9969135802  0.9938271605  0.9984567901  1.0000000000  0.9938271605  1.0000000000  0.9969135802  0.9629629630  0.8320987654  0.7814814815  0.8493827160  0.8679012346  7.4074074074  1.0391631007  0.3012299538  150           0.0742670107 
0.8253086420  0.9984567901  0.9876543210  0.9984567901  1.0000000000  1.0000000000  0.9938271605  0.9984567901  0.9629629630  0.8481481481  0.7395061728  0.8358024691  0.8777777778  9.8765432099  0.9548068655  0.3834357262  200           0.0737000942 
0.8348765432  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9969135802  0.9753086420  0.8395061728  0.7839506173  0.8666666667  0.8493827160  12.345679012  0.8957103968  0.3834357262  250           0.0749235773 
0.8645061728  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9753086420  0.8827160494  0.7876543210  0.8888888889  0.8987654321  14.814814814  0.8770835519  0.3834357262  300           0.0748636341 
0.8361111111  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8370370370  0.7617283951  0.8604938272  0.8851851852  17.283950617  0.8348173797  0.3834357262  350           0.0754033947 
0.8259259259  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8283950617  0.7765432099  0.8506172840  0.8481481481  19.753086419  0.8326521039  0.3834357262  400           0.0795831013 
0.8379629630  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8506172840  0.7567901235  0.8567901235  0.8876543210  22.222222222  0.7927664948  0.3834357262  450           0.0757549715 
0.8188271605  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9984567901  0.9753086420  0.8345679012  0.7370370370  0.8283950617  0.8753086420  24.691358024  0.7808136475  0.3834357262  500           0.0741058397 
0.8345679012  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8654320988  0.7740740741  0.8530864198  0.8456790123  27.160493827  0.7515564096  0.3834357262  550           0.0773918104 
0.8222222222  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8172839506  0.7592592593  0.8481481481  0.8641975309  29.629629629  0.7319366837  0.3834357262  600           0.0745053244 
0.8509259259  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8703703704  0.7814814815  0.8827160494  0.8691358025  32.098765432  0.7199911547  0.3834357262  650           0.0768063307 
0.8509259259  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8802469136  0.7851851852  0.8728395062  0.8654320988  34.567901234  0.7170862341  0.3834357262  700           0.0769306326 
0.8472222222  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8679012346  0.7753086420  0.8753086420  0.8703703704  37.037037037  0.7228080726  0.3834357262  750           0.0813707209 
0.8188271605  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8234567901  0.7444444444  0.8419753086  0.8654320988  39.506172839  0.7180650687  0.3834357262  800           0.0740178156 
0.8620370370  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8839506173  0.7777777778  0.8950617284  0.8913580247  41.975308642  0.7090470707  0.3834357262  850           0.0769698524 
0.8462962963  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8580246914  0.7654320988  0.8765432099  0.8851851852  44.444444444  0.7222498918  0.3834357262  900           0.0838125420 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 104, in start
    _cleanup()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 55, in _cleanup
    if p._popen.poll() is not None:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/_pylab_helpers.py", line 89, in destroy_all
    gc.collect(1)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 3762969) is killed by signal: Terminated. 
Exception ignored in: '_pydevd_frame_eval.pydevd_frame_evaluator.get_bytecode_while_frame_eval'
Traceback (most recent call last):
  File "_pydevd_frame_eval/pydevd_frame_evaluator_common.pyx", line 159, in _pydevd_frame_eval.pydevd_frame_evaluator_common.get_func_code_info
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 571, in get_abs_path_real_path_and_base_from_file
    abs_path, real_path = _NormPaths(f)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 238, in _NormPaths
    real_path = _NormPath(filename, rPath)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 246, in _NormPath
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/posixpath.py", line 395, in realpath
    path, ok = _joinrealpath(filename[:0], filename, {})
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/posixpath.py", line 429, in _joinrealpath
    if not islink(newpath):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/posixpath.py", line 171, in islink
    st = os.lstat(path)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 3762772) is killed by signal: Terminated. 
Error in atexit._run_exitfuncs:
SystemError: <function _python_exit at 0x7f2f6e11f158> returned NULL without setting an error
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 500
	lr: 0.0005
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3765432099  0.3966049383  0.4012345679  0.3858024691  0.3888888889  0.3981481481  0.3765432099  0.3765432099  0.4074074074  0.3703703704  0.3851851852  0.3728395062  0.3777777778  0.0000000000  4.5427684784  0.1364655495  0             0.3897762299 
0.7901234568  0.9753086420  0.9876543210  0.9876543210  0.9629629630  0.9645061728  0.9691358025  0.9737654321  0.9135802469  0.7543209877  0.7679012346  0.8086419753  0.8296296296  2.4691358025  2.3723855257  0.3012361526  50            0.0738877678 
0.8015432099  0.9953703704  0.9814814815  0.9922839506  0.9691358025  0.9938271605  0.9814814815  0.9876543210  0.9506172840  0.8197530864  0.7283950617  0.8037037037  0.8543209877  4.9382716049  1.1995134032  0.3013148308  100           0.0745814037 
0.8293209877  0.9984567901  0.9938271605  1.0000000000  1.0000000000  0.9984567901  1.0000000000  0.9953703704  0.9567901235  0.8320987654  0.7629629630  0.8481481481  0.8740740741  7.4074074074  0.9760343945  0.3013148308  150           0.0742398643 
0.8429012346  0.9984567901  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9753086420  0.8567901235  0.7777777778  0.8679012346  0.8691358025  9.8765432099  0.9363405597  0.3013148308  200           0.0745715094 
0.8398148148  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9691358025  0.8592592593  0.7864197531  0.8629629630  0.8506172840  12.345679012  0.8937285507  0.3013148308  250           0.0746139097 
0.8484567901  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9691358025  0.8728395062  0.7617283951  0.8728395062  0.8864197531  14.814814814  0.8620105660  0.3013148308  300           0.0749963474 
0.8453703704  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9629629630  0.8592592593  0.7592592593  0.8790123457  0.8839506173  17.283950617  0.7950632608  0.3013148308  350           0.0747516727 
0.8243827160  0.9984567901  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8395061728  0.7728395062  0.8469135802  0.8382716049  19.753086419  0.8007897270  0.3013148308  400           0.0747824764 
0.8493827160  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9567901235  0.8567901235  0.7679012346  0.8765432099  0.8962962963  22.222222222  0.7609650242  0.3013148308  450           0.0752720118 
0.8429012346  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9629629630  0.8530864198  0.7604938272  0.8654320988  0.8925925926  24.691358024  0.7644341075  0.3013148308  500           0.0755953503 
0.8413580247  1.0000000000  1.0000000000  0.9984567901  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8777777778  0.7666666667  0.8666666667  0.8543209877  27.160493827  0.7203949237  0.3013148308  550           0.0755118132 
0.8432098765  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8506172840  0.7703703704  0.8790123457  0.8728395062  29.629629629  0.7078824294  0.3013148308  600           0.0745677090 
0.8373456790  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8506172840  0.7753086420  0.8691358025  0.8543209877  32.098765432  0.7118371785  0.3833527565  650           0.0752651834 
0.8398148148  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8604938272  0.7716049383  0.8740740741  0.8530864198  34.567901234  0.6970800364  0.3833580017  700           0.0745279312 
0.8469135802  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8728395062  0.7604938272  0.8753086420  0.8790123457  37.037037037  0.7093823516  0.3833580017  750           0.0748435545 
0.8478395062  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8716049383  0.7641975309  0.8753086420  0.8802469136  39.506172839  0.7024749315  0.3833580017  800           0.0747879982 
0.8453703704  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8802469136  0.7543209877  0.8666666667  0.8802469136  41.975308642  0.7012854779  0.3833580017  850           0.0747538328 
0.8459876543  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8753086420  0.7716049383  0.8740740741  0.8629629630  44.444444444  0.6980733895  0.3833580017  900           0.0745053005 
0.8101851852  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9984567901  1.0000000000  1.0000000000  0.9876543210  0.8456790123  0.7555555556  0.8185185185  0.8209876543  46.913580246  0.7039251709  0.3833580017  950           0.0742047548 
0.8521604938  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8851851852  0.7567901235  0.8851851852  0.8814814815  49.382716049  0.7090150070  0.3833580017  1000          0.0756433582 
0.8373456790  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.8777777778  0.7604938272  0.8592592593  0.8518518519  51.851851851  0.8126449037  0.3833580017  1050          0.0750609732 
0.8219135802  1.0000000000  1.0000000000  0.9984567901  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.8493827160  0.7456790123  0.8456790123  0.8469135802  54.320987654  0.7174833226  0.3833580017  1100          0.0753317690 
0.8064814815  1.0000000000  0.9938271605  0.9984567901  0.9876543210  1.0000000000  0.9876543210  0.9984567901  0.9691358025  0.8283950617  0.7271604938  0.8345679012  0.8358024691  56.790123456  0.6943491971  0.3833580017  1150          0.0737084723 
0.8219135802  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9876543210  0.8506172840  0.7456790123  0.8469135802  0.8444444444  59.259259259  0.7219428599  0.3833580017  1200          0.0739862394 
0.7944444444  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8234567901  0.7370370370  0.8234567901  0.7938271605  61.728395061  0.7109421527  0.3833580017  1250          0.0742755270 
0.8141975309  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8333333333  0.7604938272  0.8432098765  0.8197530864  64.197530864  0.7219578695  0.3833580017  1300          0.0781493092 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
trails: 0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.00025
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3496913580  0.3441358025  0.3456790123  0.3719135802  0.3703703704  0.3580246914  0.3456790123  0.3611111111  0.3456790123  0.3567901235  0.3456790123  0.3493827160  0.3469135802  0.0000000000  1.1818408966  0.0539774895  0             0.3365623951 
0.7500000000  0.9799382716  0.9629629630  0.9768518519  0.9629629630  0.9583333333  0.9259259259  0.9675925926  0.9320987654  0.7000000000  0.7382716049  0.7777777778  0.7839506173  2.4691358025  0.4940488714  0.0604104996  50            0.0121791792 
0.7858024691  0.9953703704  0.9938271605  0.9984567901  1.0000000000  0.9922839506  0.9938271605  0.9891975309  0.9382716049  0.7518518519  0.7629629630  0.8086419753  0.8197530864  4.9382716049  0.0787665780  0.0604534149  100           0.0123659945 
0.8302469136  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9984567901  1.0000000000  0.9953703704  0.9567901235  0.8234567901  0.7802469136  0.8543209877  0.8629629630  7.4074074074  0.0229466177  0.0604534149  150           0.0124829817 
0.8345679012  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9506172840  0.8333333333  0.7802469136  0.8580246914  0.8666666667  9.8765432099  0.0115222172  0.0604534149  200           0.0124256134 
0.8117283951  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9567901235  0.8135802469  0.7481481481  0.8308641975  0.8543209877  12.345679012  0.0051771805  0.0604534149  250           0.0156357527 
0.8361111111  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9567901235  0.8345679012  0.7728395062  0.8641975309  0.8728395062  14.814814814  0.0065259759  0.0629453659  300           0.0126721001 
0.8447530864  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9567901235  0.8469135802  0.7790123457  0.8728395062  0.8802469136  17.283950617  0.0035155521  0.0629453659  350           0.0124997807 
0.8262345679  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9567901235  0.8185185185  0.7666666667  0.8543209877  0.8654320988  19.753086419  0.0018421028  0.0629453659  400           0.0122556305 
0.8447530864  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8407407407  0.7901234568  0.8728395062  0.8753086420  22.222222222  0.0011371698  0.0633897781  450           0.0124662733 
0.8376543210  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8345679012  0.7827160494  0.8679012346  0.8654320988  24.691358024  0.0011667804  0.0633897781  500           0.0125967264 
0.8364197531  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8333333333  0.7765432099  0.8691358025  0.8666666667  27.160493827  0.0013148864  0.0633897781  550           0.0127693558 
0.8500000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8543209877  0.7876543210  0.8814814815  0.8765432099  29.629629629  0.0012561646  0.0633897781  600           0.0132132292 
0.8438271605  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8469135802  0.7851851852  0.8703703704  0.8728395062  32.098765432  0.0006464903  0.0633897781  650           0.0123319054 
0.8410493827  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8395061728  0.7839506173  0.8703703704  0.8703703704  34.567901234  0.0006236368  0.0633897781  700           0.0151863194 
0.8333333333  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8407407407  0.7975308642  0.8543209877  0.8407407407  37.037037037  0.0013169482  0.0633897781  750           0.0127000475 
0.8432098765  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8469135802  0.7691358025  0.8679012346  0.8888888889  39.506172839  0.0009713729  0.0633897781  800           0.0142286587 
0.8478395062  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8493827160  0.8049382716  0.8716049383  0.8654320988  41.975308642  0.0009898647  0.0633897781  850           0.0137126923 
0.8481481481  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8518518519  0.7814814815  0.8765432099  0.8827160494  44.444444444  0.0004992287  0.0633897781  900           0.0136871862 
0.8558641975  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8617283951  0.7938271605  0.8864197531  0.8814814815  46.913580246  0.0006040304  0.0633897781  950           0.0127718878 
0.8422839506  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8395061728  0.7802469136  0.8716049383  0.8777777778  49.382716049  0.0005115528  0.0633897781  1000          0.0129257154 
0.8490740741  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8469135802  0.7901234568  0.8802469136  0.8790123457  51.851851851  0.0003402611  0.0633897781  1050          0.0144898653 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1171, in _next_data
    self._shutdown_workers()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1297, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 124, in join
    res = self._popen.wait(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 47, in wait
    if not wait([self.sentinel], timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 500
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3651234568  0.3549382716  0.3703703704  0.3796296296  0.4012345679  0.3765432099  0.3765432099  0.3842592593  0.4074074074  0.3580246914  0.3691358025  0.3604938272  0.3728395062  0.0000000000  3.7969198227  0.1362838745  0             0.3959820271 
0.7104938272  0.8580246914  0.8333333333  0.8750000000  0.8395061728  0.8503086420  0.8395061728  0.8719135802  0.8209876543  0.7209876543  0.6925925926  0.7098765432  0.7185185185  2.4691358025  1.7782221305  0.3010482788  50            0.0797786808 
0.7830246914  0.9907407407  0.9938271605  1.0000000000  0.9938271605  0.9845679012  0.9938271605  0.9876543210  0.9506172840  0.7530864198  0.7703703704  0.7913580247  0.8172839506  4.9382716049  0.6560924762  0.3010759354  100           0.0707818079 
0.8370370370  0.9969135802  0.9938271605  1.0000000000  1.0000000000  0.9969135802  1.0000000000  0.9969135802  0.9691358025  0.8395061728  0.7901234568  0.8555555556  0.8629629630  7.4074074074  0.2518146005  0.3010759354  150           0.0740188980 
0.8422839506  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9984567901  0.9938271605  0.9984567901  0.9691358025  0.8567901235  0.7666666667  0.8580246914  0.8876543210  9.8765432099  0.1797304596  0.3010759354  200           0.0735568714 
0.8487654321  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9969135802  0.9753086420  0.8580246914  0.7950617284  0.8753086420  0.8666666667  12.345679012  0.1413030618  0.3010759354  250           0.0691214705 
0.8663580247  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9629629630  0.8876543210  0.8012345679  0.8975308642  0.8790123457  14.814814814  0.1141333890  0.3010759354  300           0.0694460917 
0.8475308642  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8481481481  0.7740740741  0.8790123457  0.8888888889  17.283950617  0.0900281203  0.3010759354  350           0.0697766256 
0.8432098765  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8469135802  0.7962962963  0.8679012346  0.8617283951  19.753086419  0.0831411157  0.3010759354  400           0.0728058195 
0.8558641975  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8641975309  0.7827160494  0.8876543210  0.8888888889  22.222222222  0.0723805643  0.3010759354  450           0.0704685926 
0.8441358025  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8567901235  0.7666666667  0.8691358025  0.8839506173  24.691358024  0.0724405810  0.3010759354  500           0.0757619858 
0.8459876543  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8506172840  0.7950617284  0.8765432099  0.8617283951  27.160493827  0.0598294433  0.3010759354  550           0.0775195026 
0.8435185185  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8506172840  0.7802469136  0.8765432099  0.8666666667  29.629629629  0.0568821133  0.3831710815  600           0.0751187515 
0.8558641975  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8827160494  0.7901234568  0.8802469136  0.8703703704  32.098765432  0.0540957585  0.3831710815  650           0.0767689943 
0.8703703704  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.9049382716  0.7987654321  0.8950617284  0.8827160494  34.567901234  0.0500868087  0.3831710815  700           0.0747857475 
0.8466049383  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8679012346  0.7950617284  0.8728395062  0.8506172840  37.037037037  0.0507300359  0.3831710815  750           0.0732189560 
0.8237654321  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8333333333  0.7604938272  0.8555555556  0.8456790123  39.506172839  0.0655590681  0.3831710815  800           0.0700482035 
0.8422839506  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8567901235  0.7666666667  0.8703703704  0.8753086420  41.975308642  0.0527160040  0.3831710815  850           0.0730547333 
0.8410493827  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8530864198  0.7679012346  0.8740740741  0.8691358025  44.444444444  0.0459585545  0.3831710815  900           0.0794407845 
0.8543209877  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8814814815  0.7950617284  0.8802469136  0.8604938272  46.913580246  0.0430790050  0.3831710815  950           0.0750804996 
0.8632716049  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8888888889  0.7962962963  0.8938271605  0.8740740741  49.382716049  0.0392945544  0.3831710815  1000          0.0722754192 
0.8354938272  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8728395062  0.7777777778  0.8592592593  0.8320987654  51.851851851  0.0377386340  0.3831710815  1050          0.0729226732 
0.8203703704  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9938271605  0.9984567901  0.9691358025  0.8469135802  0.7308641975  0.8419753086  0.8617283951  54.320987654  0.0367086036  0.3831710815  1100          0.0733514881 
0.8435185185  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9691358025  0.8716049383  0.7543209877  0.8716049383  0.8765432099  56.790123456  0.0333408441  0.3831772804  1150          0.0730893660 
0.8361111111  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8444444444  0.7679012346  0.8666666667  0.8654320988  59.259259259  0.0375675469  0.3831772804  1200          0.0739266777 
0.8546296296  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8790123457  0.7716049383  0.8839506173  0.8839506173  61.728395061  0.0345875159  0.3831772804  1250          0.0753497744 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 500
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3629629630  0.3487654321  0.3765432099  0.3842592593  0.3765432099  0.3734567901  0.3703703704  0.3919753086  0.3703703704  0.3691358025  0.3543209877  0.3604938272  0.3679012346  0.0000000000  3.7757043839  0.1006450653  0             0.3698637486 
0.7305555556  0.8827160494  0.8518518519  0.8950617284  0.8580246914  0.8688271605  0.8395061728  0.8935185185  0.8518518519  0.7432098765  0.7024691358  0.7283950617  0.7481481481  2.4691358025  1.7720579028  0.2654094696  50            0.0596666002 
0.7827160494  0.9891975309  0.9938271605  1.0000000000  1.0000000000  0.9845679012  0.9938271605  0.9876543210  0.9382716049  0.7444444444  0.7728395062  0.8000000000  0.8135802469  4.9382716049  0.5046393618  0.2654094696  100           0.0598213100 
0.8388888889  1.0000000000  0.9938271605  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9969135802  0.9567901235  0.8530864198  0.7679012346  0.8530864198  0.8814814815  7.4074074074  0.1373130610  0.2654156685  150           0.0588594675 
0.8049382716  0.9984567901  0.9938271605  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9629629630  0.7962962963  0.7530864198  0.8308641975  0.8395061728  9.8765432099  0.0589902468  0.2654371262  200           0.0578093576 
0.8574074074  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9691358025  0.8753086420  0.7851851852  0.8864197531  0.8827160494  12.345679012  0.0431555521  0.2654371262  250           0.0574179983 
0.8524691358  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9753086420  0.8641975309  0.8012345679  0.8740740741  0.8703703704  14.814814814  0.0196002762  0.2654371262  300           0.0581835222 
0.8524691358  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8691358025  0.7962962963  0.8777777778  0.8666666667  17.283950617  0.0125830314  0.2654371262  350           0.0585151196 
0.8509259259  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8753086420  0.8000000000  0.8790123457  0.8493827160  19.753086419  0.0083914579  0.2654371262  400           0.0583890820 
0.8552469136  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8790123457  0.7901234568  0.8839506173  0.8679012346  22.222222222  0.0080581978  0.2658734322  450           0.0666244555 
0.7984567901  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9691358025  0.8049382716  0.7234567901  0.8209876543  0.8444444444  24.691358024  0.0161683135  0.2658734322  500           0.0602968931 
0.8518518519  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8679012346  0.7876543210  0.8814814815  0.8703703704  27.160493827  0.0056777415  0.3476061821  550           0.0575197220 
0.8555555556  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8790123457  0.7851851852  0.8839506173  0.8740740741  29.629629629  0.0054706105  0.3476061821  600           0.0585943270 
0.8570987654  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8728395062  0.7876543210  0.8839506173  0.8839506173  32.098765432  0.0037649502  0.3476061821  650           0.0586666775 
0.8524691358  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8839506173  0.7950617284  0.8790123457  0.8518518519  34.567901234  0.0033704838  0.3476061821  700           0.0640742826 
0.8367283951  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8629629630  0.7839506173  0.8728395062  0.8271604938  37.037037037  0.0043732779  0.3476061821  750           0.0587657261 
0.8503086420  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8839506173  0.7864197531  0.8839506173  0.8469135802  39.506172839  0.0035548325  0.3476061821  800           0.0610592842 
0.8580246914  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8888888889  0.7888888889  0.8888888889  0.8654320988  41.975308642  0.0022194333  0.3476061821  850           0.0603239536 
0.8645061728  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8950617284  0.7913580247  0.8987654321  0.8728395062  44.444444444  0.0023890172  0.3476061821  900           0.0592296839 
0.8447530864  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8703703704  0.7839506173  0.8703703704  0.8543209877  46.913580246  0.0016978785  0.3476061821  950           0.0607201195 
0.8496913580  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.8827160494  0.7901234568  0.8839506173  0.8419753086  49.382716049  0.0017868915  0.3476061821  1000          0.0593928719 
0.8061728395  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8444444444  0.7567901235  0.8296296296  0.7938271605  51.851851851  0.0026206551  0.3476061821  1050          0.0602005148 
0.8425925926  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8740740741  0.7728395062  0.8728395062  0.8506172840  54.320987654  0.0042655761  0.3476061821  1100          0.0615669060 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 500
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3651234568  0.3549382716  0.3703703704  0.3796296296  0.4012345679  0.3765432099  0.3765432099  0.3842592593  0.4074074074  0.3580246914  0.3691358025  0.3604938272  0.3728395062  0.0000000000  3.7969198227  0.1362838745  0             0.4359016418 
0.7104938272  0.8580246914  0.8333333333  0.8750000000  0.8395061728  0.8503086420  0.8395061728  0.8719135802  0.8209876543  0.7209876543  0.6925925926  0.7098765432  0.7185185185  2.4691358025  1.7782221305  0.3010482788  50            0.0747803307 
0.7830246914  0.9907407407  0.9938271605  1.0000000000  0.9938271605  0.9845679012  0.9938271605  0.9876543210  0.9506172840  0.7530864198  0.7703703704  0.7913580247  0.8172839506  4.9382716049  0.6560924762  0.3010749817  100           0.0736462879 
0.8370370370  0.9969135802  0.9938271605  1.0000000000  1.0000000000  0.9969135802  1.0000000000  0.9969135802  0.9691358025  0.8395061728  0.7901234568  0.8555555556  0.8629629630  7.4074074074  0.2518146005  0.3010749817  150           0.0714903641 
0.8422839506  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9984567901  0.9938271605  0.9984567901  0.9691358025  0.8567901235  0.7666666667  0.8580246914  0.8876543210  9.8765432099  0.1797304596  0.3025979996  200           0.0714813995 
0.8487654321  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9969135802  0.9753086420  0.8580246914  0.7950617284  0.8753086420  0.8666666667  12.345679012  0.1413030618  0.3025979996  250           0.0705540323 
0.8663580247  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9629629630  0.8876543210  0.8012345679  0.8975308642  0.8790123457  14.814814814  0.1141333890  0.3025979996  300           0.0705988884 
0.8475308642  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8481481481  0.7740740741  0.8790123457  0.8888888889  17.283950617  0.0900281203  0.3025979996  350           0.0710295725 
0.8432098765  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8469135802  0.7962962963  0.8679012346  0.8617283951  19.753086419  0.0831411157  0.3025979996  400           0.0718492413 
0.8558641975  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8641975309  0.7827160494  0.8876543210  0.8888888889  22.222222222  0.0723805643  0.3831710815  450           0.0713304901 
0.8441358025  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8567901235  0.7666666667  0.8691358025  0.8839506173  24.691358024  0.0724405810  0.3831710815  500           0.0708986855 
0.8459876543  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8506172840  0.7950617284  0.8765432099  0.8617283951  27.160493827  0.0598294433  0.3831710815  550           0.0719528437 
0.8435185185  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8506172840  0.7802469136  0.8765432099  0.8666666667  29.629629629  0.0568821133  0.3831710815  600           0.0737777901 
0.8558641975  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8827160494  0.7901234568  0.8802469136  0.8703703704  32.098765432  0.0540957585  0.3831710815  650           0.0864422131 
0.8703703704  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.9049382716  0.7987654321  0.8950617284  0.8827160494  34.567901234  0.0500868087  0.3831710815  700           0.0812460041 

trails: 0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.00025
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3617283951  0.3626543210  0.3580246914  0.3441358025  0.3641975309  0.3549382716  0.3580246914  0.3518518519  0.3580246914  0.3530864198  0.3740740741  0.3592592593  0.3604938272  0.0000000000  1.2105740309  0.0539774895  0             0.3488073349 
0.8654320988  0.8348765432  0.8024691358  0.8580246914  0.7530864198  0.8935185185  0.9012345679  0.9197530864  0.8765432099  0.8395061728  0.8703703704  0.8592592593  0.8925925926  2.4691358025  0.5258679467  0.0604372025  50            0.0130260611 
0.7719135802  0.9290123457  0.9135802469  0.9799382716  0.9197530864  0.9552469136  0.9382716049  0.9629629630  0.9259259259  0.7481481481  0.7654320988  0.7814814815  0.7925925926  4.9382716049  0.2048359673  0.0604372025  100           0.0130663204 
0.8398148148  0.9891975309  0.9753086420  0.9907407407  0.9506172840  0.9876543210  0.9506172840  0.9845679012  0.9567901235  0.8481481481  0.8469135802  0.8283950617  0.8358024691  7.4074074074  0.1152415232  0.0604372025  150           0.0125263596 
0.8179012346  0.9953703704  0.9691358025  0.9953703704  0.9876543210  0.9953703704  0.9629629630  0.9984567901  0.9567901235  0.8098765432  0.8333333333  0.8148148148  0.8135802469  9.8765432099  0.0627537750  0.0604372025  200           0.0128625298 
0.8157407407  0.9922839506  0.9691358025  1.0000000000  0.9938271605  0.9984567901  0.9691358025  0.9984567901  0.9691358025  0.8074074074  0.8283950617  0.8074074074  0.8197530864  12.345679012  0.0476942153  0.0604372025  250           0.0128990459 
0.8712962963  1.0000000000  0.9876543210  0.9984567901  0.9753086420  1.0000000000  0.9629629630  0.9969135802  0.9629629630  0.8802469136  0.8691358025  0.8543209877  0.8814814815  14.814814814  0.0303894120  0.0604372025  300           0.0129970455 
0.8503086420  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9753086420  0.9984567901  0.9814814815  0.8604938272  0.8555555556  0.8308641975  0.8543209877  17.283950617  0.0203769091  0.0605044365  350           0.0130339479 
0.8601851852  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9753086420  0.8679012346  0.8679012346  0.8419753086  0.8629629630  19.753086419  0.0121221903  0.0605044365  400           0.0125895214 
0.8524691358  1.0000000000  0.9876543210  1.0000000000  0.9691358025  1.0000000000  0.9567901235  1.0000000000  0.9691358025  0.8592592593  0.8518518519  0.8395061728  0.8592592593  22.222222222  0.0107018162  0.0629825592  450           0.0125418711 
0.8530864198  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9753086420  1.0000000000  0.9691358025  0.8580246914  0.8592592593  0.8382716049  0.8567901235  24.691358024  0.0069603148  0.0629825592  500           0.0127952194 
0.8657407407  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9814814815  0.8691358025  0.8679012346  0.8493827160  0.8765432099  27.160493827  0.0073368373  0.0629825592  550           0.0132889080 
0.8725308642  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9629629630  1.0000000000  0.9691358025  0.8851851852  0.8691358025  0.8543209877  0.8814814815  29.629629629  0.0096173182  0.0629825592  600           0.0128113699 
0.8719135802  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9629629630  1.0000000000  0.9753086420  0.8802469136  0.8691358025  0.8555555556  0.8827160494  32.098765432  0.0035475762  0.0629825592  650           0.0130759525 
0.8345679012  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9753086420  1.0000000000  0.9814814815  0.8283950617  0.8444444444  0.8222222222  0.8432098765  34.567901234  0.0031933496  0.0629825592  700           0.0127948523 
0.8561728395  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8592592593  0.8604938272  0.8395061728  0.8654320988  37.037037037  0.0029291696  0.0629825592  750           0.0129899979 
0.8604938272  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9691358025  1.0000000000  0.9876543210  0.8691358025  0.8617283951  0.8469135802  0.8641975309  39.506172839  0.0027267597  0.0629825592  800           0.0133624697 
0.8561728395  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9691358025  1.0000000000  0.9876543210  0.8592592593  0.8604938272  0.8419753086  0.8629629630  41.975308642  0.0021537017  0.0629825592  850           0.0129315329 
0.8521604938  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9876543210  0.8506172840  0.8580246914  0.8395061728  0.8604938272  44.444444444  0.0018982218  0.0629825592  900           0.0130987215 
0.8635802469  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9629629630  1.0000000000  0.9876543210  0.8728395062  0.8641975309  0.8469135802  0.8703703704  46.913580246  0.0023084756  0.0629825592  950           0.0123522806 
0.8703703704  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9629629630  1.0000000000  0.9876543210  0.8716049383  0.8666666667  0.8592592593  0.8839506173  49.382716049  0.0031075894  0.0629825592  1000          0.0131508970 
0.8589506173  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9876543210  0.8666666667  0.8617283951  0.8407407407  0.8666666667  51.851851851  0.0011114900  0.0633811951  1050          0.0127126503 
0.8311728395  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9753086420  0.8197530864  0.8444444444  0.8259259259  0.8345679012  54.320987654  0.0016214721  0.0633811951  1100          0.0127366781 
0.8574074074  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9629629630  1.0000000000  0.9814814815  0.8592592593  0.8641975309  0.8407407407  0.8654320988  56.790123456  0.0013155369  0.0633811951  1150          0.0129119587 
0.8182098765  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9691358025  0.8074074074  0.8308641975  0.8123456790  0.8222222222  59.259259259  0.0020600031  0.0633811951  1200          0.0132348919 
0.8518518519  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8481481481  0.8629629630  0.8407407407  0.8555555556  61.728395061  0.0019960284  0.0633811951  1250          0.0130958605 
0.8391975309  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9814814815  1.0000000000  0.9814814815  0.8246913580  0.8493827160  0.8382716049  0.8444444444  64.197530864  0.0009430647  0.0633811951  1300          0.0126171827 
0.8524691358  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9629629630  1.0000000000  0.9876543210  0.8506172840  0.8580246914  0.8444444444  0.8567901235  66.666666666  0.0011814574  0.0633811951  1350          0.0129013824 
0.8654320988  1.0000000000  0.9938271605  1.0000000000  0.9691358025  1.0000000000  0.9567901235  1.0000000000  0.9691358025  0.8641975309  0.8604938272  0.8493827160  0.8876543210  69.135802469  0.0023599312  0.0633811951  1400          0.0128982687 
0.8398148148  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9814814815  1.0000000000  0.9753086420  0.8382716049  0.8481481481  0.8234567901  0.8493827160  71.604938271  0.0028335670  0.0633811951  1450          0.0129214764 
0.8521604938  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9876543210  0.8617283951  0.8592592593  0.8333333333  0.8543209877  74.074074074  0.0019530793  0.0633811951  1500          0.0130357647 
0.8601851852  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9691358025  1.0000000000  0.9814814815  0.8654320988  0.8567901235  0.8481481481  0.8703703704  76.543209876  0.0021518582  0.0633811951  1550          0.0129578114 
0.8450617284  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9876543210  0.8469135802  0.8506172840  0.8333333333  0.8493827160  79.012345679  0.0017583926  0.0633811951  1600          0.0128664351 
0.8601851852  1.0000000000  0.9814814815  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9876543210  0.8555555556  0.8567901235  0.8555555556  0.8728395062  81.481481481  0.0046613061  0.0633811951  1650          0.0134012890 
0.8518518519  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9876543210  0.8456790123  0.8580246914  0.8432098765  0.8604938272  83.950617284  0.0013158713  0.0633811951  1700          0.0129419327 
0.8722222222  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9814814815  0.8617283951  0.8679012346  0.8691358025  0.8901234568  86.419753086  0.0008292912  0.0633811951  1750          0.0126607561 
0.8469135802  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9814814815  0.8395061728  0.8555555556  0.8370370370  0.8555555556  88.888888888  0.0007537585  0.0633811951  1800          0.0130470324 
0.8611111111  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9691358025  1.0000000000  0.9814814815  0.8555555556  0.8654320988  0.8543209877  0.8691358025  91.358024691  0.0010902199  0.0633811951  1850          0.0128079128 
0.8395061728  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8345679012  0.8419753086  0.8308641975  0.8506172840  93.827160493  0.0013473938  0.0633811951  1900          0.0126204920 
0.8549382716  1.0000000000  0.9876543210  1.0000000000  0.9444444444  1.0000000000  0.9382716049  1.0000000000  0.9814814815  0.8604938272  0.8506172840  0.8382716049  0.8703703704  96.296296296  0.0034526293  0.0633811951  1950          0.0126611042 
0.8530864198  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9814814815  1.0000000000  0.9691358025  0.8530864198  0.8580246914  0.8419753086  0.8592592593  98.765432098  0.0019819539  0.0633811951  2000          0.0127297688 
0.8391975309  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9814814815  0.8345679012  0.8481481481  0.8259259259  0.8481481481  101.23456790  0.0007454027  0.0634193420  2050          0.0124033880 
0.8512345679  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9753086420  0.8493827160  0.8617283951  0.8370370370  0.8567901235  103.70370370  0.0008959025  0.0634193420  2100          0.0128596878 
0.8663580247  1.0000000000  0.9938271605  1.0000000000  0.9691358025  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8716049383  0.8604938272  0.8518518519  0.8814814815  106.17283950  0.0007233144  0.0634193420  2150          0.0131120825 
0.8293209877  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9814814815  0.8259259259  0.8333333333  0.8185185185  0.8395061728  108.64197530  0.0006095030  0.0634193420  2200          0.0131472778 
0.8558641975  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9876543210  0.8530864198  0.8654320988  0.8456790123  0.8592592593  111.11111111  0.0006122147  0.0634193420  2250          0.0129832315 
0.8500000000  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8518518519  0.8567901235  0.8345679012  0.8567901235  113.58024691  0.0004507082  0.0634193420  2300          0.0128370667 
0.8296296296  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9691358025  1.0000000000  0.9938271605  0.8283950617  0.8358024691  0.8123456790  0.8419753086  116.04938271  0.0006405024  0.0644235611  2350          0.0127438164 
0.8246913580  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9814814815  0.8209876543  0.8333333333  0.8086419753  0.8358024691  118.51851851  0.0005643373  0.0644235611  2400          0.0127320957 
0.8712962963  1.0000000000  0.9938271605  1.0000000000  0.9753086420  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8777777778  0.8654320988  0.8555555556  0.8864197531  120.98765432  0.0007759193  0.0644235611  2450          0.0150703430 
0.8503086420  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9753086420  1.0000000000  0.9753086420  0.8481481481  0.8604938272  0.8345679012  0.8580246914  123.45679012  0.0015942741  0.0644235611  2500          0.0135762215 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 500
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3604938272  0.3456790123  0.3395061728  0.3518518519  0.3703703704  0.3533950617  0.3395061728  0.3641975309  0.3703703704  0.3506172840  0.3629629630  0.3641975309  0.3641975309  0.0000000000  3.7076332569  0.1362838745  0             0.3981685638 
0.7626543210  0.9259259259  0.8888888889  0.7962962963  0.7839506173  0.8472222222  0.8703703704  0.8503086420  0.8827160494  0.7543209877  0.7629629630  0.7506172840  0.7827160494  2.4691358025  1.7936529326  0.3010482788  50            0.0720351076 
0.7854938272  0.9614197531  0.9567901235  0.9351851852  0.8888888889  0.9521604938  0.9135802469  0.9537037037  0.9074074074  0.7777777778  0.7802469136  0.7888888889  0.7950617284  4.9382716049  0.9229963994  0.3010544777  100           0.0719367695 
0.8018518519  0.9753086420  0.9753086420  0.9768518519  0.9320987654  0.9799382716  0.9320987654  0.9783950617  0.9506172840  0.7962962963  0.8024691358  0.8037037037  0.8049382716  7.4074074074  0.5523112947  0.3010544777  150           0.0705337238 
0.8138888889  0.9907407407  0.9876543210  1.0000000000  0.9753086420  0.9969135802  0.9506172840  0.9891975309  0.9567901235  0.7975308642  0.8148148148  0.8185185185  0.8246913580  9.8765432099  0.3503925976  0.3010544777  200           0.0738821411 
0.7981481481  0.9907407407  0.9753086420  0.9984567901  0.9814814815  0.9984567901  0.9629629630  0.9984567901  0.9691358025  0.7876543210  0.8086419753  0.7975308642  0.7987654321  12.345679012  0.2390440875  0.3010544777  250           0.0715742016 
0.7969135802  0.9876543210  0.9814814815  1.0000000000  0.9814814815  1.0000000000  0.9629629630  1.0000000000  0.9814814815  0.7851851852  0.7913580247  0.7987654321  0.8123456790  14.814814814  0.2018493243  0.3010544777  300           0.0734860706 
0.8302469136  1.0000000000  0.9814814815  0.9969135802  0.9691358025  0.9953703704  0.9444444444  1.0000000000  0.9814814815  0.8185185185  0.8395061728  0.8172839506  0.8456790123  17.283950617  0.1423892109  0.3010544777  350           0.0760268402 
0.8163580247  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9753086420  0.7987654321  0.8259259259  0.8098765432  0.8308641975  19.753086419  0.1419628395  0.3831834793  400           0.0770279980 
0.8308641975  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9506172840  1.0000000000  0.9814814815  0.8222222222  0.8395061728  0.8160493827  0.8456790123  22.222222222  0.1172370949  0.3831834793  450           0.0765069294 
0.8432098765  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9506172840  0.9984567901  0.9691358025  0.8271604938  0.8456790123  0.8345679012  0.8654320988  24.691358024  0.0983681761  0.3831834793  500           0.0762990522 
0.8225308642  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8074074074  0.8246913580  0.8123456790  0.8456790123  27.160493827  0.0864212944  0.3831834793  550           0.0749206495 
0.7947530864  0.9953703704  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9629629630  1.0000000000  0.9876543210  0.7765432099  0.7925925926  0.7987654321  0.8111111111  29.629629629  0.1121466237  0.3831834793  600           0.0737913179 
0.8135802469  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9814814815  0.7987654321  0.8098765432  0.8086419753  0.8370370370  32.098765432  0.0838555567  0.3831834793  650           0.0725747442 
0.8209876543  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9753086420  0.8098765432  0.8259259259  0.8098765432  0.8382716049  34.567901234  0.0685065517  0.3831834793  700           0.0831034422 
0.8231481481  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8135802469  0.8259259259  0.8111111111  0.8419753086  37.037037037  0.0648266304  0.3831834793  750           0.0728529787 
0.8324074074  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8172839506  0.8358024691  0.8209876543  0.8555555556  39.506172839  0.0645177414  0.3831834793  800           0.0731308079 
0.8293209877  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9629629630  1.0000000000  0.9814814815  0.8197530864  0.8345679012  0.8148148148  0.8481481481  41.975308642  0.0607271299  0.3831834793  850           0.0820836735 
0.8179012346  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8000000000  0.8209876543  0.8049382716  0.8456790123  44.444444444  0.0551920000  0.3831834793  900           0.0743264198 
0.8157407407  1.0000000000  0.9938271605  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8074074074  0.8135802469  0.8086419753  0.8333333333  46.913580246  0.0550935680  0.3831834793  950           0.0763758993 
0.7981481481  0.9984567901  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  1.0000000000  0.7827160494  0.7938271605  0.7987654321  0.8172839506  49.382716049  0.0581174869  0.3831834793  1000          0.0734801149 
0.8052469136  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.7901234568  0.8024691358  0.8086419753  0.8197530864  51.851851851  0.0489781917  0.3831834793  1050          0.0733807516 
0.8308641975  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8160493827  0.8345679012  0.8234567901  0.8493827160  54.320987654  0.0457029608  0.3831834793  1100          0.0745364952 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 500
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3623456790  0.3472222222  0.3518518519  0.3580246914  0.3703703704  0.3595679012  0.3395061728  0.3672839506  0.3703703704  0.3543209877  0.3641975309  0.3666666667  0.3641975309  0.0000000000  3.7076332569  0.1362590790  0             0.5817480087 
0.7163580247  0.8302469136  0.8333333333  0.7978395062  0.7530864198  0.8533950617  0.8086419753  0.8487654321  0.8333333333  0.7037037037  0.7123456790  0.7135802469  0.7358024691  2.4691358025  1.8235167813  0.3009815216  50            0.0793194532 
0.7333333333  0.9290123457  0.8765432099  0.9320987654  0.8456790123  0.9336419753  0.8888888889  0.9290123457  0.8703703704  0.7148148148  0.7197530864  0.7530864198  0.7456790123  4.9382716049  0.9881522596  0.3009815216  100           0.0756680536 
0.8259259259  0.9907407407  0.9876543210  0.9629629630  0.9320987654  0.9768518519  0.9320987654  0.9691358025  0.9567901235  0.8135802469  0.8320987654  0.8123456790  0.8456790123  7.4074074074  0.6063174212  0.3830871582  150           0.0782595778 
0.8753086420  0.9984567901  0.9938271605  0.9922839506  0.9567901235  0.9876543210  0.9506172840  0.9814814815  0.9629629630  0.8679012346  0.8753086420  0.8555555556  0.9024691358  9.8765432099  0.3457783896  0.3830871582  200           0.0728154707 
0.8734567901  0.9984567901  0.9938271605  0.9953703704  0.9753086420  0.9907407407  0.9567901235  0.9938271605  0.9753086420  0.8580246914  0.8728395062  0.8629629630  0.9000000000  12.345679012  0.2410999760  0.3830871582  250           0.0723779488 
0.8287037037  1.0000000000  0.9938271605  1.0000000000  0.9814814815  1.0000000000  0.9567901235  0.9984567901  0.9814814815  0.8049382716  0.8345679012  0.8197530864  0.8555555556  14.814814814  0.1961026582  0.3830871582  300           0.0727515125 
0.8037037037  0.9953703704  0.9814814815  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.7691358025  0.8086419753  0.8049382716  0.8320987654  17.283950617  0.1572285922  0.3830871582  350           0.0757718849 
0.8435185185  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8209876543  0.8469135802  0.8358024691  0.8703703704  19.753086419  0.1233258028  0.3830871582  400           0.0740426874 
0.8469135802  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8209876543  0.8518518519  0.8407407407  0.8740740741  22.222222222  0.1109356903  0.3830871582  450           0.0752036810 
0.8290123457  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.7975308642  0.8308641975  0.8320987654  0.8555555556  24.691358024  0.0908838490  0.3830871582  500           0.0724842834 
0.8444444444  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8234567901  0.8432098765  0.8382716049  0.8728395062  27.160493827  0.0913578624  0.3830871582  550           0.0730956125 
0.8179012346  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.7888888889  0.8185185185  0.8123456790  0.8518518519  29.629629629  0.0924360071  0.3830871582  600           0.0734295559 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 500
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3623456790  0.3472222222  0.3518518519  0.3580246914  0.3703703704  0.3595679012  0.3395061728  0.3672839506  0.3703703704  0.3543209877  0.3641975309  0.3666666667  0.3641975309  0.0000000000  3.7076332569  0.1362590790  0             0.6389575005 
0.7163580247  0.8302469136  0.8333333333  0.7978395062  0.7530864198  0.8533950617  0.8086419753  0.8487654321  0.8333333333  0.7037037037  0.7123456790  0.7135802469  0.7358024691  2.4691358025  1.8235167813  0.3009815216  50            0.0729117203 
0.7333333333  0.9290123457  0.8765432099  0.9320987654  0.8456790123  0.9336419753  0.8888888889  0.9290123457  0.8703703704  0.7148148148  0.7197530864  0.7530864198  0.7456790123  4.9382716049  0.9881522596  0.3010859489  100           0.0726297855 
0.8259259259  0.9907407407  0.9876543210  0.9629629630  0.9320987654  0.9768518519  0.9320987654  0.9691358025  0.9567901235  0.8135802469  0.8320987654  0.8123456790  0.8456790123  7.4074074074  0.6063174212  0.3010859489  150           0.0731867313 
0.8753086420  0.9984567901  0.9938271605  0.9922839506  0.9567901235  0.9876543210  0.9506172840  0.9814814815  0.9629629630  0.8679012346  0.8753086420  0.8555555556  0.9024691358  9.8765432099  0.3457783896  0.3010859489  200           0.0735724068 
0.8734567901  0.9984567901  0.9938271605  0.9953703704  0.9753086420  0.9907407407  0.9567901235  0.9938271605  0.9753086420  0.8580246914  0.8728395062  0.8629629630  0.9000000000  12.345679012  0.2410999760  0.3830871582  250           0.0724830723 
0.8567901235  1.0000000000  0.9938271605  0.9984567901  0.9753086420  0.9922839506  0.9567901235  0.9953703704  0.9753086420  0.8407407407  0.8604938272  0.8419753086  0.8839506173  14.814814814  0.2130715805  0.3830871582  300           0.0736179733 
0.8475308642  0.9984567901  0.9876543210  1.0000000000  0.9753086420  0.9969135802  0.9691358025  0.9953703704  0.9753086420  0.8209876543  0.8481481481  0.8407407407  0.8802469136  17.283950617  0.2121183079  0.3830871582  350           0.0719700050 
0.8413580247  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.9953703704  0.9567901235  0.9969135802  0.9753086420  0.8172839506  0.8493827160  0.8271604938  0.8716049383  19.753086419  0.1939552261  0.3830871582  400           0.0723347282 
0.8354938272  0.9984567901  0.9938271605  1.0000000000  0.9753086420  0.9969135802  0.9629629630  0.9969135802  0.9753086420  0.8086419753  0.8419753086  0.8283950617  0.8629629630  22.222222222  0.1914295158  0.3830871582  450           0.0735504055 
0.8333333333  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.9969135802  0.9629629630  0.9969135802  0.9753086420  0.8061728395  0.8469135802  0.8222222222  0.8580246914  24.691358024  0.1816152190  0.3830871582  500           0.0735054827 
0.8268518519  1.0000000000  0.9876543210  1.0000000000  0.9753086420  0.9969135802  0.9567901235  0.9969135802  0.9814814815  0.8024691358  0.8395061728  0.8135802469  0.8518518519  27.160493827  0.1900363871  0.3830871582  550           0.0737546015 
0.8253086420  1.0000000000  0.9876543210  1.0000000000  0.9753086420  0.9969135802  0.9567901235  0.9969135802  0.9814814815  0.8012345679  0.8358024691  0.8135802469  0.8506172840  29.629629629  0.1949510103  0.3830871582  600           0.0728803349 
0.8364197531  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.9969135802  0.9629629630  0.9969135802  0.9814814815  0.8111111111  0.8481481481  0.8259259259  0.8604938272  32.098765432  0.1893303278  0.3830871582  650           0.0739715576 
0.8373456790  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.9953703704  0.9567901235  0.9969135802  0.9753086420  0.8135802469  0.8481481481  0.8246913580  0.8629629630  34.567901234  0.1842808646  0.3830871582  700           0.0738634825 
0.8327160494  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.9969135802  0.9629629630  0.9969135802  0.9814814815  0.8061728395  0.8456790123  0.8246913580  0.8543209877  37.037037037  0.2019701822  0.3830871582  750           0.0734514046 
0.8265432099  1.0000000000  0.9876543210  1.0000000000  0.9753086420  0.9984567901  0.9629629630  0.9969135802  0.9814814815  0.8037037037  0.8358024691  0.8172839506  0.8493827160  39.506172839  0.1874624293  0.3830871582  800           0.0728709030 
0.8299382716  1.0000000000  0.9876543210  1.0000000000  0.9753086420  0.9969135802  0.9567901235  0.9969135802  0.9876543210  0.8086419753  0.8395061728  0.8160493827  0.8555555556  41.975308642  0.1901837440  0.3830871582  850           0.0727478504 
0.8237654321  1.0000000000  0.9876543210  1.0000000000  0.9753086420  0.9984567901  0.9629629630  0.9969135802  0.9814814815  0.8012345679  0.8333333333  0.8160493827  0.8444444444  44.444444444  0.1844390066  0.3830871582  900           0.0745172930 
0.8296296296  1.0000000000  0.9876543210  1.0000000000  0.9753086420  0.9984567901  0.9629629630  0.9969135802  0.9814814815  0.8049382716  0.8370370370  0.8209876543  0.8555555556  46.913580246  0.1822957438  0.3830871582  950           0.0728334284 
0.8336419753  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.9984567901  0.9629629630  0.9969135802  0.9814814815  0.8074074074  0.8419753086  0.8259259259  0.8592592593  49.382716049  0.1812226431  0.3830871582  1000          0.0729512835 
0.8324074074  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.9984567901  0.9567901235  0.9969135802  0.9876543210  0.8074074074  0.8432098765  0.8234567901  0.8555555556  51.851851851  0.1858429755  0.3830871582  1050          0.0750322008 
0.8290123457  1.0000000000  0.9876543210  1.0000000000  0.9753086420  0.9984567901  0.9629629630  0.9969135802  0.9814814815  0.8037037037  0.8382716049  0.8209876543  0.8530864198  54.320987654  0.1879152070  0.3830871582  1100          0.0758806610 
0.8388888889  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.9984567901  0.9629629630  0.9969135802  0.9753086420  0.8123456790  0.8506172840  0.8271604938  0.8654320988  56.790123456  0.1800880268  0.3830871582  1150          0.0833733845 
0.8246913580  1.0000000000  0.9876543210  1.0000000000  0.9753086420  0.9984567901  0.9629629630  0.9969135802  0.9876543210  0.8000000000  0.8320987654  0.8185185185  0.8481481481  59.259259259  0.1671485798  0.3830871582  1200          0.0739610004 
0.8382716049  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.9969135802  0.9567901235  0.9969135802  0.9753086420  0.8098765432  0.8493827160  0.8271604938  0.8666666667  61.728395061  0.1820984925  0.3830871582  1250          0.0765541267 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 500
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3629629630  0.3533950617  0.3518518519  0.3564814815  0.3641975309  0.3549382716  0.3333333333  0.3672839506  0.3703703704  0.3592592593  0.3654320988  0.3666666667  0.3604938272  0.0000000000  4.4635930061  0.1364407539  0             0.4296379089 
0.7376543210  0.8703703704  0.8333333333  0.8101851852  0.7777777778  0.8719135802  0.8456790123  0.8688271605  0.8765432099  0.7209876543  0.7345679012  0.7271604938  0.7679012346  2.4691358025  2.6303685308  0.3011631966  50            0.0777570629 
0.7487654321  0.9104938272  0.8641975309  0.9058641975  0.8148148148  0.8904320988  0.8641975309  0.8981481481  0.8395061728  0.7234567901  0.7333333333  0.7629629630  0.7753086420  4.9382716049  1.8111374712  0.3011631966  100           0.0785701609 
0.8126543210  0.9891975309  0.9876543210  0.9351851852  0.8827160494  0.9413580247  0.9074074074  0.9444444444  0.9320987654  0.7888888889  0.8172839506  0.8037037037  0.8407407407  7.4074074074  1.4399100208  0.3012022972  150           0.0795560169 
0.8783950617  0.9984567901  0.9938271605  0.9861111111  0.9444444444  0.9861111111  0.9567901235  0.9830246914  0.9506172840  0.8716049383  0.8753086420  0.8629629630  0.9037037037  9.8765432099  1.1844078660  0.3012022972  200           0.0803781414 
0.8814814815  0.9984567901  0.9938271605  0.9907407407  0.9629629630  0.9891975309  0.9506172840  0.9891975309  0.9691358025  0.8716049383  0.8851851852  0.8666666667  0.9024691358  12.345679012  1.0628276443  0.3012022972  250           0.0796705961 
0.8367283951  1.0000000000  0.9938271605  0.9984567901  0.9753086420  0.9984567901  0.9629629630  0.9984567901  0.9629629630  0.8185185185  0.8419753086  0.8259259259  0.8604938272  14.814814814  1.0004818416  0.3012022972  300           0.0870319080 
0.8138888889  0.9984567901  0.9814814815  0.9984567901  0.9814814815  1.0000000000  0.9506172840  0.9984567901  0.9753086420  0.7864197531  0.8209876543  0.8160493827  0.8320987654  17.283950617  0.9441552401  0.3012022972  350           0.0788580608 
0.8586419753  1.0000000000  0.9938271605  0.9969135802  0.9629629630  0.9984567901  0.9382716049  1.0000000000  0.9691358025  0.8407407407  0.8592592593  0.8481481481  0.8864197531  19.753086419  0.9084417975  0.3012022972  400           0.0786011839 
0.8549382716  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.9984567901  0.9506172840  1.0000000000  0.9753086420  0.8419753086  0.8604938272  0.8407407407  0.8765432099  22.222222222  0.8875498152  0.3015980721  450           0.0828504562 
0.8512345679  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8320987654  0.8567901235  0.8419753086  0.8740740741  24.691358024  0.8492508423  0.3015980721  500           0.0927754641 
0.8638888889  1.0000000000  0.9938271605  1.0000000000  0.9691358025  1.0000000000  0.9506172840  1.0000000000  0.9753086420  0.8543209877  0.8617283951  0.8518518519  0.8876543210  27.160493827  0.8671982718  0.3015980721  550           0.0801325750 
0.8185185185  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.7987654321  0.8185185185  0.8135802469  0.8432098765  29.629629629  0.8566721666  0.3015980721  600           0.0799071455 
0.8453703704  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8222222222  0.8395061728  0.8407407407  0.8790123457  32.098765432  0.8298263395  0.3015980721  650           0.0788810635 
0.8459876543  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8209876543  0.8419753086  0.8432098765  0.8777777778  34.567901234  0.8159580696  0.3015980721  700           0.0802517414 
0.8570987654  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9814814815  0.8296296296  0.8567901235  0.8567901235  0.8851851852  37.037037037  0.8172893739  0.3015980721  750           0.0794482708 
0.8345679012  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8160493827  0.8333333333  0.8333333333  0.8555555556  39.506172839  0.7769850039  0.3015980721  800           0.0799828768 
0.8697530864  1.0000000000  0.9938271605  1.0000000000  0.9753086420  1.0000000000  0.9506172840  1.0000000000  0.9753086420  0.8481481481  0.8641975309  0.8604938272  0.9061728395  41.975308642  0.7754074669  0.3015980721  850           0.0832287025 
0.8237654321  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8012345679  0.8234567901  0.8197530864  0.8506172840  44.444444444  0.7582739902  0.3015980721  900           0.0890206051 
0.8422839506  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8209876543  0.8469135802  0.8296296296  0.8716049383  46.913580246  0.7117002821  0.3015980721  950           0.0788454580 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 500
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3604938272  0.3456790123  0.3395061728  0.3518518519  0.3703703704  0.3533950617  0.3395061728  0.3641975309  0.3703703704  0.3506172840  0.3629629630  0.3641975309  0.3641975309  0.0000000000  3.7076332569  0.1362838745  0             0.3882431984 
0.7626543210  0.9259259259  0.8888888889  0.7962962963  0.7839506173  0.8472222222  0.8703703704  0.8503086420  0.8827160494  0.7543209877  0.7629629630  0.7506172840  0.7827160494  2.4691358025  1.7936529326  0.3010482788  50            0.0735895872 
0.7854938272  0.9614197531  0.9567901235  0.9351851852  0.8888888889  0.9521604938  0.9135802469  0.9537037037  0.9074074074  0.7777777778  0.7802469136  0.7888888889  0.7950617284  4.9382716049  0.9229963994  0.3010482788  100           0.0738304806 
0.8018518519  0.9753086420  0.9753086420  0.9768518519  0.9320987654  0.9799382716  0.9320987654  0.9783950617  0.9506172840  0.7962962963  0.8024691358  0.8037037037  0.8049382716  7.4074074074  0.5523112947  0.3011221886  150           0.0741723680 
0.8138888889  0.9907407407  0.9876543210  1.0000000000  0.9753086420  0.9969135802  0.9506172840  0.9891975309  0.9567901235  0.7975308642  0.8148148148  0.8185185185  0.8246913580  9.8765432099  0.3503925976  0.3011221886  200           0.0739006376 
0.7981481481  0.9907407407  0.9753086420  0.9984567901  0.9814814815  0.9984567901  0.9629629630  0.9984567901  0.9691358025  0.7876543210  0.8086419753  0.7975308642  0.7987654321  12.345679012  0.2390440875  0.3011221886  250           0.0737607861 
0.7969135802  0.9876543210  0.9814814815  1.0000000000  0.9814814815  1.0000000000  0.9629629630  1.0000000000  0.9814814815  0.7851851852  0.7913580247  0.7987654321  0.8123456790  14.814814814  0.2018493243  0.3011221886  300           0.0733536673 
0.8302469136  1.0000000000  0.9814814815  0.9969135802  0.9691358025  0.9953703704  0.9444444444  1.0000000000  0.9814814815  0.8185185185  0.8395061728  0.8172839506  0.8456790123  17.283950617  0.1423892109  0.3011221886  350           0.0741262722 
0.8163580247  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9753086420  0.7987654321  0.8259259259  0.8098765432  0.8308641975  19.753086419  0.1419628395  0.3011221886  400           0.0735291386 
0.8308641975  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9506172840  1.0000000000  0.9814814815  0.8222222222  0.8395061728  0.8160493827  0.8456790123  22.222222222  0.1172370949  0.3011221886  450           0.0808808422 
0.8432098765  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9506172840  0.9984567901  0.9691358025  0.8271604938  0.8456790123  0.8345679012  0.8654320988  24.691358024  0.0983681761  0.3011221886  500           0.0761380434 
0.8225308642  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8074074074  0.8246913580  0.8123456790  0.8456790123  27.160493827  0.0864212944  0.3011221886  550           0.0804735947 
0.7947530864  0.9953703704  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9629629630  1.0000000000  0.9876543210  0.7765432099  0.7925925926  0.7987654321  0.8111111111  29.629629629  0.1121466237  0.3011221886  600           0.0737233782 
0.8135802469  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9814814815  0.7987654321  0.8098765432  0.8086419753  0.8370370370  32.098765432  0.0838555567  0.3033576012  650           0.0730514240 
0.8209876543  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9753086420  0.8098765432  0.8259259259  0.8098765432  0.8382716049  34.567901234  0.0685065517  0.3033576012  700           0.0735321903 
0.8231481481  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8135802469  0.8259259259  0.8111111111  0.8419753086  37.037037037  0.0648266304  0.3033576012  750           0.0745578814 
0.8324074074  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8172839506  0.8358024691  0.8209876543  0.8555555556  39.506172839  0.0645177414  0.3033576012  800           0.0778837538 
0.8293209877  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9629629630  1.0000000000  0.9814814815  0.8197530864  0.8345679012  0.8148148148  0.8481481481  41.975308642  0.0607271299  0.3033576012  850           0.0807670593 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 500
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3604938272  0.3456790123  0.3395061728  0.3518518519  0.3703703704  0.3533950617  0.3395061728  0.3641975309  0.3703703704  0.3506172840  0.3629629630  0.3641975309  0.3641975309  0.0000000000  3.7076332569  0.1362838745  0             0.4181275368 
0.7626543210  0.9259259259  0.8888888889  0.7962962963  0.7839506173  0.8472222222  0.8703703704  0.8503086420  0.8827160494  0.7543209877  0.7629629630  0.7506172840  0.7827160494  2.4691358025  1.7936529326  0.3010482788  50            0.0731094408 
0.7854938272  0.9614197531  0.9567901235  0.9351851852  0.8888888889  0.9521604938  0.9135802469  0.9537037037  0.9074074074  0.7777777778  0.7802469136  0.7888888889  0.7950617284  4.9382716049  0.9229963994  0.3010759354  100           0.0741886330 
0.8018518519  0.9753086420  0.9753086420  0.9768518519  0.9320987654  0.9799382716  0.9320987654  0.9783950617  0.9506172840  0.7962962963  0.8024691358  0.8037037037  0.8049382716  7.4074074074  0.5523112947  0.3010759354  150           0.0719862175 
0.8274691358  0.9922839506  0.9876543210  0.9691358025  0.9074074074  0.9768518519  0.9259259259  0.9768518519  0.9506172840  0.8234567901  0.8259259259  0.8246913580  0.8358024691  9.8765432099  0.3962490445  0.3010759354  200           0.0747735977 
0.8268518519  0.9938271605  0.9876543210  0.9706790123  0.9135802469  0.9768518519  0.9382716049  0.9799382716  0.9506172840  0.8246913580  0.8234567901  0.8259259259  0.8333333333  12.345679012  0.3907658350  0.3010759354  250           0.0735821772 
0.8194444444  0.9861111111  0.9876543210  0.9814814815  0.9259259259  0.9783950617  0.9382716049  0.9814814815  0.9506172840  0.8148148148  0.8160493827  0.8197530864  0.8271604938  14.814814814  0.4037871385  0.3010759354  300           0.0739061499 
0.8265432099  0.9922839506  0.9876543210  0.9706790123  0.9135802469  0.9768518519  0.9320987654  0.9783950617  0.9506172840  0.8259259259  0.8259259259  0.8222222222  0.8320987654  17.283950617  0.3877554867  0.3010759354  350           0.0727541256 
0.8432098765  0.9953703704  0.9876543210  0.9814814815  0.9382716049  0.9814814815  0.9320987654  0.9845679012  0.9444444444  0.8345679012  0.8456790123  0.8345679012  0.8580246914  19.753086419  0.3835686284  0.3010759354  400           0.0730478525 
0.8299382716  0.9938271605  0.9876543210  0.9891975309  0.9382716049  0.9830246914  0.9320987654  0.9845679012  0.9506172840  0.8234567901  0.8333333333  0.8271604938  0.8358024691  22.222222222  0.4099944320  0.3010759354  450           0.0732190990 
0.8302469136  0.9938271605  0.9876543210  0.9830246914  0.9382716049  0.9799382716  0.9320987654  0.9830246914  0.9506172840  0.8234567901  0.8333333333  0.8271604938  0.8370370370  24.691358024  0.3903639388  0.3010759354  500           0.0723234320 
0.8305555556  0.9938271605  0.9876543210  0.9891975309  0.9444444444  0.9830246914  0.9320987654  0.9845679012  0.9506172840  0.8246913580  0.8333333333  0.8271604938  0.8370370370  27.160493827  0.3806210610  0.3831710815  550           0.0742744255 
0.8231481481  0.9907407407  0.9876543210  0.9783950617  0.9259259259  0.9799382716  0.9320987654  0.9830246914  0.9506172840  0.8209876543  0.8246913580  0.8185185185  0.8283950617  29.629629629  0.3705827978  0.3831710815  600           0.0730624151 
0.8484567901  0.9953703704  0.9938271605  0.9845679012  0.9382716049  0.9830246914  0.9382716049  0.9845679012  0.9506172840  0.8395061728  0.8493827160  0.8395061728  0.8654320988  32.098765432  0.3700788531  0.3831710815  650           0.0734133768 
0.8354938272  0.9953703704  0.9876543210  0.9861111111  0.9444444444  0.9830246914  0.9320987654  0.9845679012  0.9506172840  0.8259259259  0.8370370370  0.8320987654  0.8469135802  34.567901234  0.3936643827  0.3831710815  700           0.0736153412 
0.8333333333  0.9953703704  0.9876543210  0.9737654321  0.9259259259  0.9799382716  0.9320987654  0.9830246914  0.9506172840  0.8271604938  0.8333333333  0.8283950617  0.8444444444  37.037037037  0.3866874170  0.3831710815  750           0.0732788706 
0.8222222222  0.9907407407  0.9876543210  0.9830246914  0.9382716049  0.9845679012  0.9320987654  0.9830246914  0.9506172840  0.8209876543  0.8222222222  0.8197530864  0.8259259259  39.506172839  0.3766271889  0.3831710815  800           0.0728918266 
0.8432098765  0.9969135802  0.9938271605  0.9753086420  0.9382716049  0.9814814815  0.9320987654  0.9830246914  0.9506172840  0.8370370370  0.8419753086  0.8320987654  0.8617283951  41.975308642  0.3640407968  0.3831710815  850           0.0812053061 
0.8339506173  0.9969135802  0.9876543210  0.9891975309  0.9444444444  0.9830246914  0.9320987654  0.9845679012  0.9506172840  0.8259259259  0.8358024691  0.8296296296  0.8444444444  44.444444444  0.3898432013  0.3831710815  900           0.0747188950 
0.8364197531  0.9953703704  0.9876543210  0.9814814815  0.9444444444  0.9830246914  0.9320987654  0.9845679012  0.9506172840  0.8271604938  0.8382716049  0.8308641975  0.8493827160  46.913580246  0.3708644396  0.3831710815  950           0.0812657690 
0.8277777778  0.9907407407  0.9876543210  0.9953703704  0.9444444444  0.9891975309  0.9444444444  0.9861111111  0.9567901235  0.8222222222  0.8296296296  0.8222222222  0.8370370370  49.382716049  0.3618566501  0.3831710815  1000          0.0738423443 
0.8339506173  0.9953703704  0.9876543210  0.9922839506  0.9444444444  0.9830246914  0.9320987654  0.9845679012  0.9506172840  0.8259259259  0.8358024691  0.8283950617  0.8456790123  51.851851851  0.3371471521  0.3831710815  1050          0.0746609163 
0.8367283951  0.9953703704  0.9876543210  0.9783950617  0.9382716049  0.9814814815  0.9320987654  0.9814814815  0.9506172840  0.8283950617  0.8382716049  0.8296296296  0.8506172840  54.320987654  0.3564491111  0.3831710815  1100          0.0742237854 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3623456790  0.3472222222  0.3518518519  0.3580246914  0.3703703704  0.3595679012  0.3395061728  0.3672839506  0.3703703704  0.3543209877  0.3641975309  0.3666666667  0.3641975309  0.0000000000  3.7076332569  0.1362590790  0             0.4131114483 
0.7163580247  0.8302469136  0.8333333333  0.7978395062  0.7530864198  0.8533950617  0.8086419753  0.8487654321  0.8333333333  0.7037037037  0.7123456790  0.7135802469  0.7358024691  2.4691358025  1.8235167813  0.3009815216  50            0.0802835083 
0.7333333333  0.9290123457  0.8765432099  0.9320987654  0.8456790123  0.9336419753  0.8888888889  0.9290123457  0.8703703704  0.7148148148  0.7197530864  0.7530864198  0.7456790123  4.9382716049  0.9881522596  0.3009815216  100           0.0781720066 
0.8259259259  0.9907407407  0.9876543210  0.9629629630  0.9320987654  0.9768518519  0.9320987654  0.9691358025  0.9567901235  0.8135802469  0.8320987654  0.8123456790  0.8456790123  7.4074074074  0.6063174212  0.3009815216  150           0.0751987934 
0.8753086420  0.9984567901  0.9938271605  0.9922839506  0.9567901235  0.9876543210  0.9506172840  0.9814814815  0.9629629630  0.8679012346  0.8753086420  0.8555555556  0.9024691358  9.8765432099  0.3457783896  0.3009815216  200           0.0754583836 
0.8734567901  0.9984567901  0.9938271605  0.9953703704  0.9753086420  0.9907407407  0.9567901235  0.9938271605  0.9753086420  0.8580246914  0.8728395062  0.8629629630  0.9000000000  12.345679012  0.2410999760  0.3009815216  250           0.0749876976 
0.8287037037  1.0000000000  0.9938271605  1.0000000000  0.9814814815  1.0000000000  0.9567901235  0.9984567901  0.9814814815  0.8049382716  0.8345679012  0.8197530864  0.8555555556  14.814814814  0.1961026582  0.3830871582  300           0.0745400190 
0.8037037037  0.9953703704  0.9814814815  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.7691358025  0.8086419753  0.8049382716  0.8320987654  17.283950617  0.1572285922  0.3830871582  350           0.0767514515 
0.8435185185  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8209876543  0.8469135802  0.8358024691  0.8703703704  19.753086419  0.1233258028  0.3830871582  400           0.0753663063 
0.8469135802  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8209876543  0.8518518519  0.8407407407  0.8740740741  22.222222222  0.1109356903  0.3830871582  450           0.0765218067 
0.8290123457  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.7975308642  0.8308641975  0.8320987654  0.8555555556  24.691358024  0.0908838490  0.3830871582  500           0.0733855867 
0.8444444444  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8234567901  0.8432098765  0.8382716049  0.8728395062  27.160493827  0.0913578624  0.3830871582  550           0.0735250044 
0.8179012346  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.7888888889  0.8185185185  0.8123456790  0.8518518519  29.629629629  0.0924360071  0.3830871582  600           0.0849691486 
0.8574074074  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8308641975  0.8555555556  0.8518518519  0.8913580247  32.098765432  0.0712838703  0.3830871582  650           0.0788652849 
0.8577160494  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8234567901  0.8493827160  0.8629629630  0.8950617284  34.567901234  0.0655900507  0.3830871582  700           0.0755099726 
0.8487654321  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9938271605  0.8172839506  0.8469135802  0.8481481481  0.8827160494  37.037037037  0.0764787784  0.3830871582  750           0.0754754400 
0.8283950617  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8061728395  0.8283950617  0.8160493827  0.8629629630  39.506172839  0.0604995319  0.3830871582  800           0.0801564407 
0.8419753086  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8296296296  0.8296296296  0.8283950617  0.8802469136  41.975308642  0.0638106418  0.3830871582  850           0.0790766525 
0.8052469136  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  1.0000000000  0.7740740741  0.8061728395  0.8012345679  0.8395061728  44.444444444  0.0593032227  0.3830871582  900           0.0791807318 
0.8342592593  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9938271605  0.8172839506  0.8308641975  0.8234567901  0.8654320988  46.913580246  0.0504269591  0.3830871582  950           0.0808960009 
0.8391975309  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9938271605  0.8135802469  0.8370370370  0.8320987654  0.8740740741  49.382716049  0.0449784368  0.3831763268  1000          0.0771436405 
0.8574074074  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9876543210  0.8283950617  0.8469135802  0.8555555556  0.8987654321  51.851851851  0.0463873527  0.3831763268  1050          0.0826840258 
0.8370370370  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9938271605  0.8086419753  0.8370370370  0.8308641975  0.8716049383  54.320987654  0.0467398425  0.3831763268  1100          0.0840480947 
0.8416666667  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9876543210  0.8172839506  0.8370370370  0.8333333333  0.8790123457  56.790123456  0.0415849962  0.3831763268  1150          0.0769012499 
0.8108024691  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  1.0000000000  0.7827160494  0.8086419753  0.8074074074  0.8444444444  59.259259259  0.0392278935  0.3831763268  1200          0.0744907999 
0.8302469136  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8061728395  0.8296296296  0.8209876543  0.8641975309  61.728395061  0.0408319352  0.3831763268  1250          0.0803325796 
0.8101851852  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9938271605  0.7851851852  0.8135802469  0.8037037037  0.8382716049  64.197530864  0.0375706592  0.3831763268  1300          0.0744834948 
0.8339506173  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  1.0000000000  0.8061728395  0.8271604938  0.8320987654  0.8703703704  66.666666666  0.0361331797  0.3831763268  1350          0.0734922314 
0.8222222222  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  1.0000000000  0.7975308642  0.8148148148  0.8197530864  0.8567901235  69.135802469  0.0324270089  0.3831763268  1400          0.0743077755 
0.8500000000  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9938271605  0.8197530864  0.8481481481  0.8469135802  0.8851851852  71.604938271  0.0347731327  0.3831763268  1450          0.0753532743 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3669753086  0.3657407407  0.3518518519  0.3533950617  0.3518518519  0.3441358025  0.3518518519  0.3595679012  0.3456790123  0.3543209877  0.3790123457  0.3666666667  0.3679012346  0.0000000000  3.7277758121  0.1006202698  0             0.3848199844 
0.6981481481  0.7870370370  0.7962962963  0.7854938272  0.7160493827  0.8194444444  0.7407407407  0.8225308642  0.7592592593  0.6888888889  0.6827160494  0.6975308642  0.7234567901  2.4691358025  1.7932549953  0.2653489113  50            0.0590412569 
0.7111111111  0.8842592593  0.8518518519  0.9089506173  0.8580246914  0.9120370370  0.8333333333  0.9027777778  0.8456790123  0.6987654321  0.6987654321  0.7160493827  0.7308641975  4.9382716049  0.9281982422  0.2653489113  100           0.0607314634 
0.8030864198  0.9861111111  0.9814814815  0.9537037037  0.9197530864  0.9629629630  0.9197530864  0.9598765432  0.9506172840  0.7913580247  0.8111111111  0.7950617284  0.8148148148  7.4074074074  0.5222847247  0.2653489113  150           0.0589978218 
0.8398148148  1.0000000000  0.9876543210  0.9861111111  0.9506172840  0.9845679012  0.9382716049  0.9799382716  0.9567901235  0.8358024691  0.8444444444  0.8222222222  0.8567901235  9.8765432099  0.2745883334  0.2653489113  200           0.0600925255 
0.8521604938  0.9984567901  0.9876543210  0.9969135802  0.9691358025  0.9922839506  0.9629629630  0.9938271605  0.9629629630  0.8456790123  0.8604938272  0.8370370370  0.8654320988  12.345679012  0.1615778111  0.2654318810  250           0.0598835707 
0.8095679012  0.9969135802  0.9814814815  1.0000000000  0.9753086420  1.0000000000  0.9567901235  0.9984567901  0.9691358025  0.8074074074  0.8148148148  0.7975308642  0.8185185185  14.814814814  0.1180622449  0.2654318810  300           0.0622198677 
0.8240740741  1.0000000000  0.9876543210  0.9984567901  0.9753086420  1.0000000000  0.9567901235  0.9984567901  0.9691358025  0.8148148148  0.8271604938  0.8172839506  0.8370370370  17.283950617  0.0836433617  0.2654318810  350           0.0627556181 
0.8290123457  1.0000000000  0.9814814815  1.0000000000  0.9876543210  1.0000000000  0.9506172840  1.0000000000  0.9814814815  0.8259259259  0.8259259259  0.8160493827  0.8481481481  19.753086419  0.0610437312  0.2654318810  400           0.0639663601 
0.8317901235  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9506172840  1.0000000000  0.9753086420  0.8246913580  0.8283950617  0.8185185185  0.8555555556  22.222222222  0.0496778805  0.2654318810  450           0.0626448727 
0.8373456790  1.0000000000  0.9938271605  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8296296296  0.8395061728  0.8148148148  0.8654320988  24.691358024  0.0307663815  0.2654318810  500           0.0621165991 
0.8422839506  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9691358025  1.0000000000  0.9814814815  0.8395061728  0.8407407407  0.8271604938  0.8617283951  27.160493827  0.0226093002  0.2654318810  550           0.0623694897 
0.8166666667  1.0000000000  0.9814814815  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9876543210  0.8037037037  0.8111111111  0.8123456790  0.8395061728  29.629629629  0.0251323464  0.3474483490  600           0.0634814215 
0.8608024691  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9506172840  1.0000000000  0.9814814815  0.8481481481  0.8567901235  0.8444444444  0.8938271605  32.098765432  0.0150207334  0.3474483490  650           0.0611231470 
0.8459876543  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9506172840  1.0000000000  0.9753086420  0.8382716049  0.8382716049  0.8333333333  0.8740740741  34.567901234  0.0104010097  0.3474483490  700           0.0617402601 
0.8262345679  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9629629630  1.0000000000  0.9876543210  0.8148148148  0.8283950617  0.8111111111  0.8506172840  37.037037037  0.0209060422  0.3474483490  750           0.0631250763 
0.8311728395  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9753086420  1.0000000000  0.9876543210  0.8111111111  0.8345679012  0.8246913580  0.8543209877  39.506172839  0.0118883052  0.3474483490  800           0.0611430264 
0.8271604938  1.0000000000  0.9938271605  1.0000000000  0.9691358025  1.0000000000  0.9444444444  1.0000000000  0.9753086420  0.8185185185  0.8234567901  0.8111111111  0.8555555556  41.975308642  0.0157961859  0.3474483490  850           0.0621629572 
0.8503086420  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8333333333  0.8444444444  0.8407407407  0.8827160494  44.444444444  0.0101274926  0.3474483490  900           0.0621648979 
0.8058641975  1.0000000000  0.9814814815  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9814814815  0.7765432099  0.8160493827  0.8074074074  0.8234567901  46.913580246  0.0084194819  0.3474483490  950           0.0615350914 
0.8503086420  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9506172840  1.0000000000  0.9938271605  0.8432098765  0.8481481481  0.8358024691  0.8740740741  49.382716049  0.0093474702  0.3474483490  1000          0.0702291727 
0.8354938272  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9753086420  0.8185185185  0.8358024691  0.8283950617  0.8592592593  51.851851851  0.0059572449  0.3474483490  1050          0.0620724726 
0.8438271605  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9876543210  0.8370370370  0.8345679012  0.8320987654  0.8716049383  54.320987654  0.0066164650  0.3474483490  1100          0.0607532644 
0.8395061728  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9506172840  1.0000000000  0.9938271605  0.8197530864  0.8407407407  0.8296296296  0.8679012346  56.790123456  0.0041626270  0.3474483490  1150          0.0617098618 
0.8469135802  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9506172840  1.0000000000  0.9938271605  0.8259259259  0.8469135802  0.8407407407  0.8740740741  59.259259259  0.0047900392  0.3474483490  1200          0.0638711739 
0.8524691358  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8345679012  0.8469135802  0.8456790123  0.8827160494  61.728395061  0.0046855217  0.3474483490  1250          0.0614748144 
0.8345679012  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9876543210  0.8074074074  0.8382716049  0.8296296296  0.8629629630  64.197530864  0.0043341579  0.3474483490  1300          0.0628879452 
0.8589506173  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9506172840  1.0000000000  0.9876543210  0.8419753086  0.8506172840  0.8555555556  0.8876543210  66.666666666  0.0021911679  0.3474483490  1350          0.0631513023 
0.8219135802  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9753086420  1.0000000000  0.9814814815  0.7925925926  0.8308641975  0.8197530864  0.8444444444  69.135802469  0.0045328723  0.3474483490  1400          0.0627255678 
0.8425925926  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9876543210  0.8172839506  0.8432098765  0.8370370370  0.8728395062  71.604938271  0.0032000377  0.3474483490  1450          0.0620652962 
0.8753086420  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9506172840  1.0000000000  0.9876543210  0.8580246914  0.8654320988  0.8728395062  0.9049382716  74.074074074  0.0057818482  0.3474483490  1500          0.0616072845 
0.8404320988  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9753086420  1.0000000000  0.9938271605  0.8074074074  0.8419753086  0.8407407407  0.8716049383  76.543209876  0.0040958516  0.3474483490  1550          0.0627470064 
0.8648148148  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8456790123  0.8518518519  0.8530864198  0.9086419753  79.012345679  0.0045145964  0.3474483490  1600          0.0619291592 
0.8104938272  0.9984567901  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9753086420  1.0000000000  0.9876543210  0.7975308642  0.8148148148  0.8012345679  0.8283950617  81.481481481  0.0156265956  0.3474483490  1650          0.0612203979 
0.8361111111  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9938271605  0.8222222222  0.8308641975  0.8283950617  0.8629629630  83.950617284  0.0089820558  0.3474483490  1700          0.0664646435 
0.8413580247  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9753086420  1.0000000000  0.9814814815  0.8086419753  0.8481481481  0.8407407407  0.8679012346  86.419753086  0.0035033509  0.3474483490  1750          0.0635394764 
0.8641975309  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8444444444  0.8592592593  0.8604938272  0.8925925926  88.888888888  0.0037062520  0.3474483490  1800          0.0600249004 
0.8379629630  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9814814815  1.0000000000  0.9938271605  0.8074074074  0.8456790123  0.8382716049  0.8604938272  91.358024691  0.0050714147  0.3474483490  1850          0.0598260069 
0.8586419753  1.0000000000  0.9938271605  0.9984567901  0.9567901235  1.0000000000  0.9444444444  1.0000000000  0.9753086420  0.8481481481  0.8456790123  0.8518518519  0.8888888889  93.827160493  0.0033827924  0.3474483490  1900          0.0597878075 
0.8419753086  0.9984567901  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9691358025  1.0000000000  1.0000000000  0.8197530864  0.8333333333  0.8419753086  0.8728395062  96.296296296  0.0401496165  0.3474483490  1950          0.0614005566 
0.8694444444  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9691358025  1.0000000000  0.9938271605  0.8481481481  0.8691358025  0.8641975309  0.8962962963  98.765432098  0.0055652798  0.3474483490  2000          0.0608898306 
0.8867283951  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8728395062  0.8790123457  0.8851851852  0.9098765432  101.23456790  0.0034936919  0.3474483490  2050          0.0608655691 
0.8629629630  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8333333333  0.8679012346  0.8629629630  0.8876543210  103.70370370  0.0082234845  0.3474483490  2100          0.0615223122 
0.8385802469  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9938271605  0.8086419753  0.8432098765  0.8333333333  0.8691358025  106.17283950  0.0036706142  0.3474483490  2150          0.0625693560 
0.8583333333  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8320987654  0.8543209877  0.8604938272  0.8864197531  108.64197530  0.0028433225  0.3474483490  2200          0.0624179792 
0.8589506173  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8345679012  0.8567901235  0.8592592593  0.8851851852  111.11111111  0.0009814900  0.3474483490  2250          0.0623242235 
0.8682098765  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8358024691  0.8666666667  0.8716049383  0.8987654321  113.58024691  0.0046616033  0.3474483490  2300          0.0605609941 
0.8410493827  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8061728395  0.8481481481  0.8382716049  0.8716049383  116.04938271  0.0022489369  0.3474483490  2350          0.0683836222 
0.8679012346  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9506172840  1.0000000000  0.9938271605  0.8506172840  0.8629629630  0.8629629630  0.8950617284  118.51851851  0.0012933223  0.3474483490  2400          0.0611171579 
0.8697530864  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9506172840  1.0000000000  0.9938271605  0.8567901235  0.8641975309  0.8654320988  0.8925925926  120.98765432  0.0033551101  0.3474483490  2450          0.0619712448 
0.8737654321  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9506172840  1.0000000000  0.9938271605  0.8518518519  0.8679012346  0.8716049383  0.9037037037  123.45679012  0.0057543329  0.3474483490  2500          0.0612747145 

trails: 0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0001
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3317374579  0.3328313253  0.3313253012  0.3317307692  0.3293556086  0.3328313253  0.3313253012  0.3323308271  0.3293413174  0.3323308271  0.3293413174  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4467766117  0.4431137725  0.3325301205  0.3333333333  0.0000000000  1.1769193411  0.1045022011  0             0.3567805290 
0.6948302819  0.9367469880  0.9277108434  0.7091346154  0.6372315036  0.8599397590  0.8253012048  0.9639097744  0.9281437126  0.8902255639  0.9101796407  0.7876506024  0.8012048193  0.8313253012  0.8795180723  0.8102409639  0.7831325301  0.9220389805  0.9041916168  0.7566265060  0.6763285024  2.4096385542  0.6876989186  0.1095995903  50            0.0200170088 
0.7417764421  0.9879518072  0.9518072289  0.7043269231  0.7016706444  0.8990963855  0.8253012048  0.9819548872  0.9760479042  0.9909774436  0.9760479042  0.9472891566  0.9457831325  0.8734939759  0.8674698795  0.9277108434  0.9397590361  0.9850074963  0.9760479042  0.7012048193  0.8599033816  4.8192771084  0.3022409067  0.1095995903  100           0.0202205229 
0.7122091669  0.9924698795  0.9819277108  0.6778846154  0.7136038186  0.9548192771  0.8554216867  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9201807229  0.9578313253  0.9126506024  0.9036144578  0.9186746988  0.9397590361  0.9880059970  0.9640718563  0.6578313253  0.7995169082  7.2289156627  0.1971795997  0.1095995903  150           0.0205462027 
0.7038299826  0.9984939759  0.9819277108  0.6850961538  0.6873508353  0.9713855422  0.8734939759  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9819277108  0.9879518072  0.9503012048  0.9216867470  0.9623493976  0.9698795181  0.9910044978  0.9760479042  0.6506024096  0.7922705314  9.6385542169  0.1359677489  0.1097145081  200           0.0197313213 
0.6954624595  1.0000000000  0.9879518072  0.6754807692  0.6634844869  0.9819277108  0.8674698795  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9954819277  0.9939759036  0.9743975904  0.9216867470  0.9924698795  0.9819277108  0.9940029985  0.9760479042  0.6457831325  0.7971014493  12.048192771  0.1089579622  0.1097145081  250           0.0198881006 
0.6936507543  0.9984939759  0.9819277108  0.6850961538  0.6587112172  0.9864457831  0.8975903614  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9939759036  0.9879518072  0.9743975904  0.9457831325  0.9924698795  0.9819277108  0.9955022489  0.9820359281  0.6506024096  0.7801932367  14.457831325  0.0824374300  0.1097145081  300           0.0193886995 
0.7020687056  1.0000000000  0.9879518072  0.6899038462  0.6658711217  0.9849397590  0.8915662651  0.9984962406  0.9940119760  0.9984962406  0.9940119760  0.9954819277  0.9939759036  0.9879518072  0.9397590361  0.9849397590  0.9879518072  0.9985007496  0.9820359281  0.6650602410  0.7874396135  16.867469879  0.0686752450  0.1097145081  350           0.0200387573 
0.6948125179  1.0000000000  0.9879518072  0.6754807692  0.6801909308  0.9954819277  0.9096385542  0.9984962406  0.9940119760  1.0000000000  1.0000000000  0.9954819277  0.9939759036  0.9924698795  0.9457831325  0.9969879518  0.9879518072  1.0000000000  1.0000000000  0.6385542169  0.7850241546  19.277108433  0.0581683867  0.1097145081  400           0.0196539021 
0.6870112564  1.0000000000  1.0000000000  0.6730769231  0.6634844869  0.9984939759  0.9216867470  0.9984962406  0.9940119760  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.9984939759  0.9698795181  0.9969879518  0.9879518072  0.9985007496  0.9820359281  0.6457831325  0.7657004831  21.686746988  0.0483378194  0.1097145081  450           0.0204328156 
0.7057060702  1.0000000000  1.0000000000  0.7043269231  0.6515513126  0.9969879518  0.9156626506  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9954819277  0.9939759036  0.9924698795  0.9457831325  0.9984939759  0.9939759036  1.0000000000  1.0000000000  0.6843373494  0.7826086957  24.096385542  0.0383765383  0.1125650406  500           0.0203538084 
0.7056688763  1.0000000000  1.0000000000  0.6850961538  0.6754176611  1.0000000000  0.9277108434  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9954819277  0.9939759036  0.9984939759  0.9698795181  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6650602410  0.7971014493  26.506024096  0.0355779574  0.1125650406  550           0.0207216501 
0.6720575807  1.0000000000  1.0000000000  0.6490384615  0.6276849642  1.0000000000  0.9156626506  1.0000000000  1.0000000000  0.9984962406  0.9820359281  1.0000000000  1.0000000000  1.0000000000  0.9638554217  1.0000000000  1.0000000000  0.9985007496  0.9820359281  0.6361445783  0.7753623188  28.915662650  0.0302520083  0.1125650406  600           0.0200276995 
0.7014690188  1.0000000000  1.0000000000  0.6971153846  0.6587112172  1.0000000000  0.9277108434  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9759036145  0.9984939759  0.9939759036  1.0000000000  1.0000000000  0.6795180723  0.7705314010  31.325301204  0.0281777035  0.1125650406  650           0.0195189095 
0.6828721651  1.0000000000  1.0000000000  0.6610576923  0.6372315036  1.0000000000  0.9156626506  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9638554217  0.9984939759  0.9939759036  1.0000000000  1.0000000000  0.6554216867  0.7777777778  33.734939759  0.0233800230  0.1125650406  700           0.0203992319 
0.6708383494  1.0000000000  1.0000000000  0.6490384615  0.6324582339  1.0000000000  0.9156626506  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9638554217  0.9984939759  0.9939759036  1.0000000000  1.0000000000  0.6313253012  0.7705314010  36.144578313  0.0218401446  0.1125650406  750           0.0197430658 
0.6804136089  1.0000000000  1.0000000000  0.6610576923  0.6563245823  1.0000000000  0.9277108434  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9759036145  0.9984939759  0.9939759036  1.0000000000  1.0000000000  0.6313253012  0.7729468599  38.554216867  0.0192005089  0.1125650406  800           0.0205208445 
0.6767972868  1.0000000000  1.0000000000  0.6875000000  0.6396181384  1.0000000000  0.9277108434  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9954819277  0.9939759036  1.0000000000  0.9759036145  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6506024096  0.7294685990  40.963855421  0.0203750657  0.1125650406  850           0.0223541307 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 172, in accuracy
    with torch.no_grad():
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/grad_mode.py", line 115, in __init__
    if not torch._jit_internal.is_scripting():
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/_jit_internal.py", line 833, in is_scripting
    def is_scripting():
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1483, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 208, in <module>
    tmpdatay = DataGenerate(args=args, domain_data=train_x[str(i)], labels=train_y[str(i)], ).labels
KeyError: '12'
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3251008497  0.3313253012  0.3373493976  0.3365384615  0.3317422434  0.3343373494  0.3493975904  0.3338345865  0.3353293413  0.3428571429  0.3353293413  0.4412650602  0.4518072289  0.4277108434  0.4337349398  0.4442771084  0.4518072289  0.4452773613  0.4371257485  0.3012048193  0.3309178744  0.0000000000  8.0963048935  0.2904891968  0             0.5465550423 
0.7104442857  0.9382530120  0.9216867470  0.7067307692  0.6658711217  0.8780120482  0.8373493976  0.9593984962  0.9101796407  0.9067669173  0.8922155689  0.7756024096  0.7891566265  0.8162650602  0.8554216867  0.7966867470  0.7409638554  0.8845577211  0.8622754491  0.7614457831  0.7077294686  2.4096385542  4.9597191763  0.4777283669  50            0.1349027300 
0.7391305083  0.9743975904  0.9578313253  0.6971153846  0.7828162291  0.8795180723  0.8433734940  0.9894736842  0.9700598802  0.9969924812  0.9760479042  0.9156626506  0.9397590361  0.7319277108  0.7710843373  0.9111445783  0.9216867470  0.9400299850  0.9281437126  0.6915662651  0.7850241546  4.8192771084  2.3065408778  0.4777283669  100           0.1294569874 
0.7373401854  0.9954819277  0.9939759036  0.7211538462  0.7613365155  0.9111445783  0.8493975904  0.9909774436  0.9880239521  0.9969924812  0.9760479042  0.9593373494  0.9819277108  0.7484939759  0.7891566265  0.9397590361  0.9638554217  0.9790104948  0.9520958084  0.7156626506  0.7512077295  7.2289156627  1.5901848173  0.4777283669  150           0.1311229801 
0.7343063248  1.0000000000  1.0000000000  0.7259615385  0.7613365155  0.9006024096  0.8433734940  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9789156627  0.9879518072  0.7349397590  0.7710843373  0.9563253012  0.9698795181  0.9775112444  0.9461077844  0.7301204819  0.7198067633  9.6385542169  1.2358309972  0.4799914360  200           0.1299411774 
0.7330872184  1.0000000000  1.0000000000  0.7211538462  0.7708830549  0.9171686747  0.8614457831  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9864457831  0.9939759036  0.7123493976  0.7650602410  0.9834337349  0.9819277108  0.9835082459  0.9580838323  0.7108433735  0.7294685990  12.048192771  1.0717247832  0.4799914360  250           0.1300307655 
0.7090627607  0.9969879518  0.9879518072  0.6995192308  0.7446300716  0.9533132530  0.8975903614  0.9954887218  0.9820359281  0.9969924812  0.9760479042  0.9924698795  0.9819277108  0.7424698795  0.7891566265  0.9939759036  0.9759036145  0.9940029985  0.9760479042  0.6698795181  0.7222222222  14.457831325  0.8936850667  0.4799914360  300           0.1293566513 
0.6935442785  0.9924698795  0.9819277108  0.6682692308  0.7016706444  0.9819277108  0.9156626506  0.9879699248  0.9760479042  0.9969924812  0.9760479042  0.9894578313  0.9819277108  0.8674698795  0.8674698795  0.9864457831  0.9698795181  0.9970014993  0.9760479042  0.6457831325  0.7584541063  16.867469879  0.7735641193  0.4799914360  350           0.1306508255 
0.7054743691  0.9984939759  0.9939759036  0.6802884615  0.7422434368  0.9849397590  0.9036144578  0.9969924812  0.9880239521  0.9984962406  0.9940119760  0.9939759036  0.9879518072  0.8734939759  0.8795180723  0.9924698795  0.9819277108  0.9955022489  0.9940119760  0.6626506024  0.7367149758  19.277108433  0.6657178247  0.4799914360  400           0.1306988478 
0.6994529886  1.0000000000  1.0000000000  0.6850961538  0.7350835322  0.9759036145  0.9036144578  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.7936746988  0.8132530120  0.9954819277  0.9819277108  0.9985007496  0.9820359281  0.6602409639  0.7173913043  21.686746988  0.6027213734  0.4799914360  450           0.1317549849 
0.7078723324  1.0000000000  1.0000000000  0.6923076923  0.7398568019  0.9713855422  0.8975903614  0.9984962406  0.9940119760  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.7379518072  0.7710843373  0.9969879518  0.9879518072  0.9985007496  0.9820359281  0.6795180723  0.7198067633  24.096385542  0.5396214378  0.4799914360  500           0.1266381454 
0.6886802962  1.0000000000  1.0000000000  0.6634615385  0.7159904535  0.9969879518  0.9277108434  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.9081325301  0.8734939759  0.9954819277  0.9819277108  0.9985007496  0.9820359281  0.6385542169  0.7367149758  26.506024096  0.4937167352  0.4799914360  550           0.1298881102 
0.6844110802  1.0000000000  1.0000000000  0.6754807692  0.7159904535  0.9638554217  0.8915662651  0.9984962406  0.9940119760  0.9984962406  0.9940119760  0.9969879518  1.0000000000  0.7289156627  0.7710843373  0.9969879518  0.9879518072  0.9985007496  0.9940119760  0.6843373494  0.6618357488  28.915662650  0.4657495397  0.4799914360  600           0.1250814056 
0.6838507709  1.0000000000  1.0000000000  0.6658653846  0.7136038186  0.9894578313  0.9096385542  0.9984962406  0.9940119760  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.8418674699  0.8373493976  0.9984939759  0.9939759036  0.9985007496  0.9820359281  0.6433734940  0.7125603865  31.325301204  0.4318692029  0.4799914360  650           0.1250067759 
0.6928752403  1.0000000000  1.0000000000  0.6778846154  0.7112171838  0.9879518072  0.9156626506  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7454819277  0.7771084337  0.9984939759  0.9939759036  1.0000000000  1.0000000000  0.6867469880  0.6956521739  33.734939759  0.4259263092  0.4799914360  700           0.1254960680 
0.6761172459  1.0000000000  1.0000000000  0.6538461538  0.6754176611  0.9924698795  0.9096385542  0.9984962406  0.9940119760  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.8012048193  0.8313253012  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6650602410  0.7101449275  36.144578313  0.3991429943  0.4799914360  750           0.1246108007 
0.6694594773  0.9984939759  0.9939759036  0.6250000000  0.7040572792  1.0000000000  0.9397590361  0.9984962406  0.9940119760  1.0000000000  0.9880239521  0.9984939759  0.9939759036  0.9849397590  0.9397590361  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6048192771  0.7439613527  38.554216867  0.3585564116  0.4799914360  800           0.1237626553 
0.6682339538  1.0000000000  1.0000000000  0.6538461538  0.6897374702  0.9954819277  0.9216867470  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.8027108434  0.8373493976  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.6481927711  0.6811594203  40.963855421  0.3471761608  0.4800086021  850           0.1254191589 
0.6760797537  1.0000000000  1.0000000000  0.6514423077  0.6873508353  0.9954819277  0.9216867470  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7996987952  0.8373493976  0.9984939759  0.9939759036  1.0000000000  1.0000000000  0.6722891566  0.6932367150  43.373493975  0.3261436513  0.4800086021  900           0.1236737585 
0.6658620708  1.0000000000  1.0000000000  0.6274038462  0.6897374702  1.0000000000  0.9397590361  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9683734940  0.9216867470  0.9984939759  0.9939759036  1.0000000000  1.0000000000  0.6289156627  0.7173913043  45.783132530  0.3035065600  0.4800086021  950           0.1228502131 
0.6712865778  1.0000000000  1.0000000000  0.6442307692  0.6849642005  1.0000000000  0.9397590361  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9141566265  0.8855421687  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.6361445783  0.7198067633  48.192771084  0.2863336506  0.4800086021  1000          0.1250658512 
0.6694761124  1.0000000000  1.0000000000  0.6514423077  0.6730310263  0.9849397590  0.9156626506  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7710843373  0.8072289157  0.9984939759  0.9939759036  1.0000000000  1.0000000000  0.6795180723  0.6739130435  50.602409638  0.2985145512  0.4800086021  1050          0.1307671452 
0.6616550073  1.0000000000  1.0000000000  0.6394230769  0.6754176611  1.0000000000  0.9397590361  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.9201807229  0.8855421687  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.6433734940  0.6884057971  53.012048192  0.2740743691  0.4800086021  1100          0.1335841656 
0.6658905826  1.0000000000  1.0000000000  0.6394230769  0.6682577566  0.9984939759  0.9337349398  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.8253012048  0.8433734940  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.6650602410  0.6908212560  55.421686747  0.2550254557  0.4800086021  1150          0.1277401114 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3257061625  0.3298192771  0.3313253012  0.3341346154  0.3317422434  0.3343373494  0.3373493976  0.3323308271  0.3293413174  0.3428571429  0.3353293413  0.4397590361  0.4457831325  0.4262048193  0.4397590361  0.4427710843  0.4457831325  0.4482758621  0.4371257485  0.3036144578  0.3333333333  0.0000000000  9.8721332550  0.2909131050  0             0.4827551842 
0.6971608275  0.9382530120  0.9337349398  0.6971153846  0.6706443914  0.8780120482  0.8373493976  0.9578947368  0.9041916168  0.9308270677  0.9041916168  0.7786144578  0.7891566265  0.7966867470  0.8373493976  0.7921686747  0.7349397590  0.8785607196  0.8862275449  0.7542168675  0.6666666667  2.4096385542  7.0759645271  0.4781041145  50            0.1421603823 
0.7553051408  0.9683734940  0.9457831325  0.6947115385  0.8257756563  0.8704819277  0.8433734940  0.9834586466  0.9820359281  0.9954887218  0.9700598802  0.9337349398  0.9397590361  0.7469879518  0.7831325301  0.9171686747  0.9337349398  0.9430284858  0.9281437126  0.6963855422  0.8043478261  4.8192771084  4.2806668854  0.4781522751  100           0.1388910675 
0.7439712996  0.9819277108  0.9638554217  0.7091346154  0.7708830549  0.9186746988  0.8554216867  0.9834586466  0.9820359281  0.9969924812  0.9760479042  0.9653614458  0.9578313253  0.7771084337  0.8072289157  0.9533132530  0.9578313253  0.9865067466  0.9461077844  0.7108433735  0.7850241546  7.2289156627  3.4567834425  0.4781522751  150           0.1364733887 
0.7475419871  1.0000000000  1.0000000000  0.7403846154  0.7637231504  0.8840361446  0.8132530120  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9789156627  0.9879518072  0.7409638554  0.7831325301  0.9563253012  0.9578313253  0.9775112444  0.9341317365  0.7710843373  0.7149758454  9.6385542169  3.0692852974  0.4781522751  200           0.1364547253 
0.7391212576  1.0000000000  1.0000000000  0.7331730769  0.7613365155  0.9096385542  0.8554216867  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9894578313  0.9939759036  0.7243975904  0.7771084337  0.9879518072  0.9759036145  0.9850074963  0.9401197605  0.7421686747  0.7198067633  12.048192771  2.8560798836  0.4781637192  250           0.1383111715 
0.7222538567  0.9984939759  0.9819277108  0.7091346154  0.7661097852  0.9427710843  0.8795180723  0.9954887218  0.9820359281  0.9969924812  0.9760479042  0.9939759036  0.9879518072  0.7786144578  0.8012048193  0.9939759036  0.9759036145  0.9940029985  0.9760479042  0.6987951807  0.7149758454  14.457831325  2.6670120335  0.4781637192  300           0.1534844685 
0.7007255932  0.9879518072  0.9397590361  0.6923076923  0.7136038186  0.9819277108  0.9156626506  0.9729323308  0.9401197605  0.9969924812  0.9760479042  0.9849397590  0.9397590361  0.9246987952  0.8795180723  0.9728915663  0.9397590361  0.9970014993  0.9760479042  0.6457831325  0.7512077295  16.867469879  2.4972346020  0.4781637192  350           0.1389060736 
0.7180659172  0.9984939759  0.9939759036  0.6971153846  0.7613365155  0.9864457831  0.9096385542  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9939759036  0.9879518072  0.8930722892  0.8734939759  0.9954819277  0.9819277108  0.9985007496  0.9820359281  0.6819277108  0.7318840580  19.277108433  2.3539424658  0.4781637192  400           0.1378606081 
0.7168666200  1.0000000000  1.0000000000  0.7139423077  0.7494033413  0.9713855422  0.8855421687  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.8192771084  0.8313253012  0.9954819277  0.9819277108  0.9985007496  0.9820359281  0.6939759036  0.7101449275  21.686746988  2.2762937379  0.4781775475  450           0.1394595337 
0.7222366664  1.0000000000  1.0000000000  0.7067307692  0.7756563246  0.9668674699  0.8795180723  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.7981927711  0.8072289157  0.9954819277  0.9819277108  0.9985007496  0.9820359281  0.6843373494  0.7222222222  24.096385542  2.1916062927  0.4781775475  500           0.1382327032 
0.7000943455  0.9984939759  0.9939759036  0.6875000000  0.7231503580  0.9969879518  0.9277108434  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9954819277  0.9939759036  0.9442771084  0.8855421687  0.9954819277  0.9819277108  0.9985007496  0.9820359281  0.6530120482  0.7367149758  26.506024096  2.1473647451  0.4781775475  550           0.1374084949 
0.7048080122  1.0000000000  1.0000000000  0.7067307692  0.7422434368  0.9683734940  0.8855421687  0.9984962406  0.9940119760  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.7695783133  0.7891566265  0.9969879518  0.9879518072  0.9985007496  0.9820359281  0.7132530120  0.6570048309  28.915662650  2.0839333034  0.4781775475  600           0.1386580133 
0.7024448183  0.9984939759  0.9939759036  0.6899038462  0.7398568019  0.9789156627  0.9036144578  0.9984962406  0.9940119760  0.9984962406  0.9820359281  0.9984939759  0.9939759036  0.8599397590  0.8373493976  0.9984939759  0.9939759036  0.9985007496  0.9820359281  0.6722891566  0.7077294686  31.325301204  2.0112320995  0.4781775475  650           0.1523012114 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3281014030  0.3313253012  0.3373493976  0.3389423077  0.3365155131  0.3388554217  0.3313253012  0.3323308271  0.3293413174  0.3398496241  0.3353293413  0.4442771084  0.4518072289  0.4322289157  0.4277108434  0.4427710843  0.4457831325  0.4452773613  0.4491017964  0.3036144578  0.3333333333  0.0000000000  8.0963048935  0.2905368805  0             0.5154597759 
0.7213008040  0.9231927711  0.9216867470  0.7235576923  0.6658711217  0.8674698795  0.8554216867  0.9323308271  0.8982035928  0.8902255639  0.9101796407  0.7545180723  0.7771084337  0.8177710843  0.8493975904  0.7906626506  0.7289156627  0.9115442279  0.8742514970  0.7493975904  0.7463768116  2.4096385542  4.8681198549  0.4777755737  50            0.1320231056 
0.7651262081  0.9819277108  0.9518072289  0.7451923077  0.7422434368  0.8403614458  0.7951807229  0.9789473684  0.9640718563  0.9939849624  0.9760479042  0.9367469880  0.9397590361  0.7153614458  0.7650602410  0.9442771084  0.9337349398  0.9520239880  0.9041916168  0.7493975904  0.8236714976  4.8192771084  2.2244086075  0.4777755737  100           0.1287875843 
0.7301157875  0.9759036145  0.9397590361  0.7019230769  0.7684964200  0.9171686747  0.8614457831  0.9789473684  0.9520958084  0.9969924812  0.9760479042  0.9668674699  0.9518072289  0.7454819277  0.7891566265  0.9578313253  0.9277108434  0.9850074963  0.9640718563  0.6819277108  0.7681159420  7.2289156627  1.5856351423  0.4777755737  150           0.1281434488 
0.7308101915  0.9849397590  0.9759036145  0.7187500000  0.7279236277  0.9292168675  0.8614457831  0.9714285714  0.9580838323  0.9969924812  0.9760479042  0.9804216867  0.9698795181  0.7545180723  0.8012048193  0.9668674699  0.9518072289  0.9970014993  0.9760479042  0.7012048193  0.7753623188  9.6385542169  1.2382079506  0.4778313637  200           0.1275808573 
0.7181636625  0.9984939759  0.9939759036  0.7139423077  0.7183770883  0.9427710843  0.8674698795  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9939759036  0.9879518072  0.7530120482  0.7951807229  0.9804216867  0.9819277108  0.9925037481  0.9580838323  0.7012048193  0.7391304348  12.048192771  1.0058108306  0.4778351784  250           0.1263230515 
0.7164155784  0.9969879518  0.9879518072  0.7043269231  0.7016706444  0.9593373494  0.8855421687  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9924698795  0.9819277108  0.7740963855  0.8192771084  0.9924698795  0.9819277108  0.9970014993  0.9760479042  0.6987951807  0.7608695652  14.457831325  0.8571256816  0.4778351784  300           0.1320486164 
0.7115907097  0.9939759036  0.9759036145  0.6875000000  0.7112171838  0.9849397590  0.9036144578  0.9894736842  0.9700598802  0.9969924812  0.9760479042  0.9909638554  0.9759036145  0.8328313253  0.8373493976  0.9894578313  0.9698795181  0.9970014993  0.9760479042  0.6746987952  0.7729468599  16.867469879  0.7695162284  0.4778351784  350           0.1294116497 
0.6923322047  0.9954819277  0.9939759036  0.6754807692  0.7016706444  0.9924698795  0.9096385542  0.9984962406  0.9940119760  0.9984962406  0.9820359281  0.9909638554  0.9879518072  0.9051204819  0.8855421687  0.9969879518  0.9879518072  0.9985007496  0.9820359281  0.6385542169  0.7536231884  19.277108433  0.6640009433  0.4779100418  400           0.1313148642 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3293134283  0.3328313253  0.3313253012  0.3389423077  0.3341288783  0.3418674699  0.3313253012  0.3323308271  0.3293413174  0.3398496241  0.3353293413  0.4427710843  0.4457831325  0.4382530120  0.4397590361  0.4427710843  0.4457831325  0.4467766117  0.4431137725  0.3084337349  0.3357487923  0.0000000000  9.8721332550  0.2909607887  0             0.7324695587 
0.7134462112  0.9171686747  0.8975903614  0.7283653846  0.6658711217  0.8599397590  0.8253012048  0.9248120301  0.8922155689  0.9398496241  0.9401197605  0.7846385542  0.7771084337  0.8132530120  0.8313253012  0.8147590361  0.7530120482  0.9040479760  0.8922155689  0.7469879518  0.7125603865  2.4096385542  7.0214909363  0.6691193581  50            0.1470174742 
0.7693289064  0.9743975904  0.9337349398  0.7331730769  0.7565632458  0.8463855422  0.7951807229  0.9639097744  0.9401197605  0.9894736842  0.9700598802  0.9487951807  0.9397590361  0.7289156627  0.7831325301  0.9412650602  0.9337349398  0.9550224888  0.9161676647  0.7421686747  0.8454106280  4.8192771084  4.2233353233  0.6691193581  100           0.1446257401 
0.7626563509  0.9759036145  0.9397590361  0.7187500000  0.7756563246  0.8614457831  0.8072289157  0.9699248120  0.9401197605  0.9909774436  0.9760479042  0.9472891566  0.9337349398  0.7500000000  0.7951807229  0.9442771084  0.9337349398  0.9640179910  0.9401197605  0.7253012048  0.8309178744  7.2289156627  3.7181243277  0.6691193581  150           0.1462280321 
0.7644535331  0.9743975904  0.9337349398  0.7187500000  0.7804295943  0.8569277108  0.8012048193  0.9759398496  0.9520958084  0.9909774436  0.9760479042  0.9427710843  0.9277108434  0.7545180723  0.8012048193  0.9412650602  0.9337349398  0.9640179910  0.9401197605  0.7253012048  0.8333333333  9.6385542169  3.6848586464  0.6691193581  200           0.1480419397 
0.7632675466  0.9728915663  0.9277108434  0.7139423077  0.7780429594  0.8750000000  0.8132530120  0.9699248120  0.9401197605  0.9894736842  0.9700598802  0.9533132530  0.9216867470  0.7695783133  0.8012048193  0.9487951807  0.9397590361  0.9715142429  0.9461077844  0.7108433735  0.8502415459  12.048192771  3.6634226561  0.6691751480  250           0.1450579643 
0.7584785347  0.9683734940  0.9216867470  0.7091346154  0.7684964200  0.8825301205  0.8192771084  0.9699248120  0.9401197605  0.9894736842  0.9700598802  0.9503012048  0.9216867470  0.7936746988  0.8373493976  0.9472891566  0.9337349398  0.9760119940  0.9520958084  0.6987951807  0.8574879227  14.457831325  3.6790512085  0.6691751480  300           0.1406841183 
0.7614847765  0.9728915663  0.9277108434  0.7139423077  0.7684964200  0.8750000000  0.8132530120  0.9759398496  0.9520958084  0.9909774436  0.9760479042  0.9442771084  0.9216867470  0.7756024096  0.8012048193  0.9518072289  0.9397590361  0.9745127436  0.9580838323  0.7108433735  0.8526570048  16.867469879  3.6911973524  0.6691751480  350           0.1419332075 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3293134283  0.3328313253  0.3313253012  0.3389423077  0.3341288783  0.3418674699  0.3313253012  0.3323308271  0.3293413174  0.3398496241  0.3353293413  0.4427710843  0.4457831325  0.4382530120  0.4397590361  0.4427710843  0.4457831325  0.4467766117  0.4431137725  0.3084337349  0.3357487923  0.0000000000  9.8721332550  0.2909607887  0             0.6600925922 
0.7134462112  0.9171686747  0.8975903614  0.7283653846  0.6658711217  0.8599397590  0.8253012048  0.9248120301  0.8922155689  0.9398496241  0.9401197605  0.7846385542  0.7771084337  0.8132530120  0.8313253012  0.8147590361  0.7530120482  0.9040479760  0.8922155689  0.7469879518  0.7125603865  2.4096385542  7.0214909363  0.4781994820  50            0.1340223169 
0.7693289064  0.9743975904  0.9337349398  0.7331730769  0.7565632458  0.8463855422  0.7951807229  0.9639097744  0.9401197605  0.9894736842  0.9700598802  0.9487951807  0.9397590361  0.7289156627  0.7831325301  0.9412650602  0.9337349398  0.9550224888  0.9161676647  0.7421686747  0.8454106280  4.8192771084  4.2233353233  0.4781994820  100           0.1345988178 
0.7626563509  0.9759036145  0.9397590361  0.7187500000  0.7756563246  0.8614457831  0.8072289157  0.9699248120  0.9401197605  0.9909774436  0.9760479042  0.9472891566  0.9337349398  0.7500000000  0.7951807229  0.9442771084  0.9337349398  0.9640179910  0.9401197605  0.7253012048  0.8309178744  7.2289156627  3.7181243277  0.4781994820  150           0.1320412493 
0.7644535331  0.9743975904  0.9337349398  0.7187500000  0.7804295943  0.8569277108  0.8012048193  0.9759398496  0.9520958084  0.9909774436  0.9760479042  0.9427710843  0.9277108434  0.7545180723  0.8012048193  0.9412650602  0.9337349398  0.9640179910  0.9401197605  0.7253012048  0.8333333333  9.6385542169  3.6848586464  0.4781994820  200           0.1307531404 
0.7632675466  0.9728915663  0.9277108434  0.7139423077  0.7780429594  0.8750000000  0.8132530120  0.9699248120  0.9401197605  0.9894736842  0.9700598802  0.9533132530  0.9216867470  0.7695783133  0.8012048193  0.9487951807  0.9397590361  0.9715142429  0.9461077844  0.7108433735  0.8502415459  12.048192771  3.6634226561  0.4781994820  250           0.1309579611 
0.7584785347  0.9683734940  0.9216867470  0.7091346154  0.7684964200  0.8825301205  0.8192771084  0.9699248120  0.9401197605  0.9894736842  0.9700598802  0.9503012048  0.9216867470  0.7936746988  0.8373493976  0.9472891566  0.9337349398  0.9760119940  0.9520958084  0.6987951807  0.8574879227  14.457831325  3.6790512085  0.4781994820  300           0.1319315672 
0.7614847765  0.9728915663  0.9277108434  0.7139423077  0.7684964200  0.8750000000  0.8132530120  0.9759398496  0.9520958084  0.9909774436  0.9760479042  0.9442771084  0.9216867470  0.7756024096  0.8012048193  0.9518072289  0.9397590361  0.9745127436  0.9580838323  0.7108433735  0.8526570048  16.867469879  3.6911973524  0.4781994820  350           0.1328061104 
0.7482449223  0.9382530120  0.9096385542  0.6875000000  0.7732696897  0.9126506024  0.8554216867  0.9533834586  0.9221556886  0.9879699248  0.9640718563  0.9322289157  0.9096385542  0.8253012048  0.8433734940  0.9246987952  0.9036144578  0.9835082459  0.9580838323  0.6650602410  0.8671497585  19.277108433  3.6451398897  0.4781994820  400           0.1304495049 
0.7572549451  0.9683734940  0.9216867470  0.7115384615  0.7732696897  0.8900602410  0.8253012048  0.9684210526  0.9461077844  0.9939849624  0.9760479042  0.9503012048  0.9216867470  0.7951807229  0.8313253012  0.9427710843  0.9277108434  0.9805097451  0.9580838323  0.6963855422  0.8478260870  21.686746988  3.6021853352  0.4781994820  450           0.1317884970 
0.7584222167  0.9668674699  0.9277108434  0.7139423077  0.7828162291  0.8765060241  0.8192771084  0.9804511278  0.9700598802  0.9939849624  0.9760479042  0.9427710843  0.9277108434  0.7560240964  0.7951807229  0.9427710843  0.9397590361  0.9685157421  0.9341317365  0.7108433735  0.8260869565  24.096385542  3.6242201090  0.4781994820  500           0.1310611010 
0.7578544515  0.9683734940  0.9216867470  0.7139423077  0.7732696897  0.8885542169  0.8192771084  0.9714285714  0.9461077844  0.9939849624  0.9760479042  0.9503012048  0.9216867470  0.7906626506  0.8253012048  0.9457831325  0.9277108434  0.9805097451  0.9580838323  0.6987951807  0.8454106280  26.506024096  3.6257112360  0.4781994820  550           0.1324178982 
0.7608475420  0.9759036145  0.9518072289  0.7235576923  0.7708830549  0.8659638554  0.8132530120  0.9864661654  0.9820359281  0.9939849624  0.9760479042  0.9397590361  0.9397590361  0.7259036145  0.7710843373  0.9382530120  0.9457831325  0.9565217391  0.9221556886  0.7349397590  0.8140096618  28.915662650  3.5754810476  0.4782543182  600           0.1311586094 
0.7560399541  0.9698795181  0.9277108434  0.7163461538  0.7732696897  0.8915662651  0.8192771084  0.9759398496  0.9640718563  0.9939849624  0.9760479042  0.9457831325  0.9277108434  0.7801204819  0.8072289157  0.9457831325  0.9397590361  0.9805097451  0.9580838323  0.6987951807  0.8357487923  31.325301204  3.6030636930  0.4782543182  650           0.1315242338 
0.7608360472  0.9743975904  0.9457831325  0.7259615385  0.7756563246  0.8689759036  0.8132530120  0.9864661654  0.9820359281  0.9939849624  0.9760479042  0.9397590361  0.9397590361  0.7364457831  0.7771084337  0.9382530120  0.9457831325  0.9595202399  0.9341317365  0.7253012048  0.8164251208  33.734939759  3.6300048494  0.4782543182  700           0.1299862528 
0.7530555038  0.9533132530  0.9216867470  0.6995192308  0.7732696897  0.9096385542  0.8554216867  0.9639097744  0.9401197605  0.9894736842  0.9700598802  0.9382530120  0.9216867470  0.8147590361  0.8373493976  0.9337349398  0.9277108434  0.9775112444  0.9580838323  0.6746987952  0.8647342995  36.144578313  3.5694452667  0.4782543182  750           0.1311246061 
0.7440164905  0.9382530120  0.9216867470  0.6899038462  0.7756563246  0.9201807229  0.8614457831  0.9518796992  0.9161676647  0.9894736842  0.9700598802  0.9322289157  0.9216867470  0.8298192771  0.8493975904  0.9231927711  0.8975903614  0.9850074963  0.9640718563  0.6506024096  0.8599033816  38.554216867  3.6280403423  0.4782543182  800           0.1313213921 
0.7554389232  0.9743975904  0.9457831325  0.7139423077  0.7708830549  0.8825301205  0.8192771084  0.9804511278  0.9700598802  0.9939849624  0.9760479042  0.9427710843  0.9397590361  0.7725903614  0.8012048193  0.9427710843  0.9397590361  0.9775112444  0.9580838323  0.7108433735  0.8260869565  40.963855421  3.5928184509  0.4782543182  850           0.1313646936 
0.7572418702  0.9728915663  0.9397590361  0.7187500000  0.7732696897  0.8885542169  0.8192771084  0.9804511278  0.9700598802  0.9939849624  0.9760479042  0.9427710843  0.9397590361  0.7801204819  0.8072289157  0.9427710843  0.9397590361  0.9805097451  0.9580838323  0.7036144578  0.8333333333  43.373493975  3.5249669456  0.4782543182  900           0.1621174240 
0.7500246088  0.9653614458  0.9216867470  0.6995192308  0.7756563246  0.9006024096  0.8433734940  0.9684210526  0.9461077844  0.9939849624  0.9760479042  0.9503012048  0.9216867470  0.8072289157  0.8313253012  0.9427710843  0.9277108434  0.9805097451  0.9580838323  0.6819277108  0.8429951691  45.783132530  3.5398401403  0.4782543182  950           0.1434614086 
0.7518418430  0.9713855422  0.9216867470  0.7091346154  0.7684964200  0.8960843373  0.8373493976  0.9684210526  0.9461077844  0.9939849624  0.9760479042  0.9533132530  0.9216867470  0.7921686747  0.8313253012  0.9457831325  0.9277108434  0.9805097451  0.9580838323  0.6891566265  0.8405797101  48.192771084  3.5750483847  0.4782543182  1000          0.1434928608 
0.7596053070  0.9789156627  0.9518072289  0.7235576923  0.7852028640  0.8750000000  0.8132530120  0.9864661654  0.9820359281  0.9939849624  0.9760479042  0.9397590361  0.9397590361  0.7364457831  0.7771084337  0.9442771084  0.9457831325  0.9685157421  0.9341317365  0.7204819277  0.8091787440  50.602409638  3.5551427126  0.4800310135  1050          0.1503061628 
0.7560299559  0.9713855422  0.9216867470  0.7091346154  0.7804295943  0.8990963855  0.8373493976  0.9744360902  0.9580838323  0.9939849624  0.9760479042  0.9533132530  0.9216867470  0.7921686747  0.8313253012  0.9487951807  0.9397590361  0.9805097451  0.9580838323  0.6939759036  0.8405797101  53.012048192  3.5284371233  0.4800310135  1100          0.1472329903 
0.7518216176  0.9713855422  0.9216867470  0.7115384615  0.7732696897  0.8945783133  0.8192771084  0.9714285714  0.9580838323  0.9939849624  0.9760479042  0.9533132530  0.9216867470  0.7801204819  0.8072289157  0.9457831325  0.9397590361  0.9805097451  0.9580838323  0.6939759036  0.8285024155  55.421686747  3.5035353041  0.4800310135  1150          0.1431641293 
0.7494205707  0.9728915663  0.9277108434  0.7115384615  0.7684964200  0.8945783133  0.8192771084  0.9759398496  0.9640718563  0.9939849624  0.9760479042  0.9548192771  0.9277108434  0.7771084337  0.8072289157  0.9487951807  0.9397590361  0.9805097451  0.9580838323  0.6939759036  0.8236714976  57.831325301  3.5450574636  0.4800310135  1200          0.1417593861 
0.7542282835  0.9743975904  0.9457831325  0.7139423077  0.7708830549  0.8855421687  0.8192771084  0.9774436090  0.9700598802  0.9939849624  0.9760479042  0.9487951807  0.9397590361  0.7560240964  0.7951807229  0.9503012048  0.9457831325  0.9775112444  0.9580838323  0.7156626506  0.8164251208  60.240963855  3.5121999741  0.4800310135  1250          0.1435417700 
0.7458105751  0.9593373494  0.9216867470  0.6971153846  0.7732696897  0.9081325301  0.8493975904  0.9578947368  0.9401197605  0.9909774436  0.9760479042  0.9472891566  0.9216867470  0.8102409639  0.8313253012  0.9322289157  0.9337349398  0.9790104948  0.9640718563  0.6771084337  0.8357487923  62.650602409  3.4542749262  0.4800310135  1300          0.1431326962 
0.7554330473  0.9789156627  0.9518072289  0.7187500000  0.7684964200  0.8810240964  0.8132530120  0.9864661654  0.9700598802  0.9939849624  0.9760479042  0.9472891566  0.9457831325  0.7454819277  0.7771084337  0.9487951807  0.9397590361  0.9760119940  0.9520958084  0.7228915663  0.8115942029  65.060240963  3.5309151173  0.4800310135  1350          0.1420982218 
0.7512336825  0.9683734940  0.9216867470  0.7091346154  0.7708830549  0.9066265060  0.8433734940  0.9639097744  0.9401197605  0.9939849624  0.9760479042  0.9533132530  0.9216867470  0.8102409639  0.8313253012  0.9412650602  0.9337349398  0.9820089955  0.9640718563  0.6843373494  0.8405797101  67.469879518  3.4968559694  0.4800310135  1400          0.1431163931 

trails: 0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0001
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3317374579  0.3328313253  0.3313253012  0.3317307692  0.3293556086  0.3328313253  0.3313253012  0.3323308271  0.3293413174  0.3323308271  0.3293413174  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4467766117  0.4431137725  0.3325301205  0.3333333333  0.0000000000  1.1769193411  0.1045022011  0             0.5841064453 
0.6948302819  0.9367469880  0.9277108434  0.7091346154  0.6372315036  0.8599397590  0.8253012048  0.9639097744  0.9281437126  0.8902255639  0.9101796407  0.7876506024  0.8012048193  0.8313253012  0.8795180723  0.8102409639  0.7831325301  0.9220389805  0.9041916168  0.7566265060  0.6763285024  2.4096385542  0.6876989186  0.1097197533  50            0.0191980648 
0.7417764421  0.9879518072  0.9518072289  0.7043269231  0.7016706444  0.8990963855  0.8253012048  0.9819548872  0.9760479042  0.9909774436  0.9760479042  0.9472891566  0.9457831325  0.8734939759  0.8674698795  0.9277108434  0.9397590361  0.9850074963  0.9760479042  0.7012048193  0.8599033816  4.8192771084  0.3022409067  0.1097197533  100           0.0193519640 
0.7122091669  0.9924698795  0.9819277108  0.6778846154  0.7136038186  0.9548192771  0.8554216867  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9201807229  0.9578313253  0.9126506024  0.9036144578  0.9186746988  0.9397590361  0.9880059970  0.9640718563  0.6578313253  0.7995169082  7.2289156627  0.1971795997  0.1097197533  150           0.0213308573 
0.7038299826  0.9984939759  0.9819277108  0.6850961538  0.6873508353  0.9713855422  0.8734939759  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9819277108  0.9879518072  0.9503012048  0.9216867470  0.9623493976  0.9698795181  0.9910044978  0.9760479042  0.6506024096  0.7922705314  9.6385542169  0.1359677489  0.1097197533  200           0.0193738985 
0.6954624595  1.0000000000  0.9879518072  0.6754807692  0.6634844869  0.9819277108  0.8674698795  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9954819277  0.9939759036  0.9743975904  0.9216867470  0.9924698795  0.9819277108  0.9940029985  0.9760479042  0.6457831325  0.7971014493  12.048192771  0.1089579622  0.1111268997  250           0.0192119265 
0.6936507543  0.9984939759  0.9819277108  0.6850961538  0.6587112172  0.9864457831  0.8975903614  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9939759036  0.9879518072  0.9743975904  0.9457831325  0.9924698795  0.9819277108  0.9955022489  0.9820359281  0.6506024096  0.7801932367  14.457831325  0.0824374300  0.1111268997  300           0.0193340874 
0.7020687056  1.0000000000  0.9879518072  0.6899038462  0.6658711217  0.9849397590  0.8915662651  0.9984962406  0.9940119760  0.9984962406  0.9940119760  0.9954819277  0.9939759036  0.9879518072  0.9397590361  0.9849397590  0.9879518072  0.9985007496  0.9820359281  0.6650602410  0.7874396135  16.867469879  0.0686752450  0.1111268997  350           0.0192350149 
0.6948125179  1.0000000000  0.9879518072  0.6754807692  0.6801909308  0.9954819277  0.9096385542  0.9984962406  0.9940119760  1.0000000000  1.0000000000  0.9954819277  0.9939759036  0.9924698795  0.9457831325  0.9969879518  0.9879518072  1.0000000000  1.0000000000  0.6385542169  0.7850241546  19.277108433  0.0581683867  0.1111268997  400           0.0196658564 
0.6870112564  1.0000000000  1.0000000000  0.6730769231  0.6634844869  0.9984939759  0.9216867470  0.9984962406  0.9940119760  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.9984939759  0.9698795181  0.9969879518  0.9879518072  0.9985007496  0.9820359281  0.6457831325  0.7657004831  21.686746988  0.0483378194  0.1111268997  450           0.0204166126 
0.7057060702  1.0000000000  1.0000000000  0.7043269231  0.6515513126  0.9969879518  0.9156626506  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9954819277  0.9939759036  0.9924698795  0.9457831325  0.9984939759  0.9939759036  1.0000000000  1.0000000000  0.6843373494  0.7826086957  24.096385542  0.0383765383  0.1111268997  500           0.0198387527 
0.7056688763  1.0000000000  1.0000000000  0.6850961538  0.6754176611  1.0000000000  0.9277108434  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9954819277  0.9939759036  0.9984939759  0.9698795181  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6650602410  0.7971014493  26.506024096  0.0355779574  0.1111268997  550           0.0194708490 
0.6720575807  1.0000000000  1.0000000000  0.6490384615  0.6276849642  1.0000000000  0.9156626506  1.0000000000  1.0000000000  0.9984962406  0.9820359281  1.0000000000  1.0000000000  1.0000000000  0.9638554217  1.0000000000  1.0000000000  0.9985007496  0.9820359281  0.6361445783  0.7753623188  28.915662650  0.0302520083  0.1111268997  600           0.0205733109 
0.7014690188  1.0000000000  1.0000000000  0.6971153846  0.6587112172  1.0000000000  0.9277108434  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9759036145  0.9984939759  0.9939759036  1.0000000000  1.0000000000  0.6795180723  0.7705314010  31.325301204  0.0281777035  0.1125760078  650           0.0195255136 
0.6828721651  1.0000000000  1.0000000000  0.6610576923  0.6372315036  1.0000000000  0.9156626506  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9638554217  0.9984939759  0.9939759036  1.0000000000  1.0000000000  0.6554216867  0.7777777778  33.734939759  0.0233800230  0.1125760078  700           0.0199610424 
0.6708383494  1.0000000000  1.0000000000  0.6490384615  0.6324582339  1.0000000000  0.9156626506  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9638554217  0.9984939759  0.9939759036  1.0000000000  1.0000000000  0.6313253012  0.7705314010  36.144578313  0.0218401446  0.1125760078  750           0.0200546360 
0.6804136089  1.0000000000  1.0000000000  0.6610576923  0.6563245823  1.0000000000  0.9277108434  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9759036145  0.9984939759  0.9939759036  1.0000000000  1.0000000000  0.6313253012  0.7729468599  38.554216867  0.0192005089  0.1151266098  800           0.0198662901 
0.6767972868  1.0000000000  1.0000000000  0.6875000000  0.6396181384  1.0000000000  0.9277108434  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9954819277  0.9939759036  1.0000000000  0.9759036145  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6506024096  0.7294685990  40.963855421  0.0203750657  0.1151266098  850           0.0197132540 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3317374579  0.3328313253  0.3313253012  0.3317307692  0.3293556086  0.3328313253  0.3313253012  0.3323308271  0.3293413174  0.3323308271  0.3293413174  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4467766117  0.4431137725  0.3325301205  0.3333333333  0.0000000000  8.0671997070  0.2073802948  0             0.4269304276 
0.7291839781  0.9623493976  0.9578313253  0.7307692308  0.6467780430  0.8448795181  0.8132530120  0.9624060150  0.9341317365  0.8796992481  0.8562874251  0.7786144578  0.7771084337  0.7861445783  0.8313253012  0.7921686747  0.7590361446  0.8515742129  0.8502994012  0.7759036145  0.7632850242  2.4096385542  4.7728108215  0.3946189880  50            0.0997771358 
0.7580469625  0.9864457831  0.9578313253  0.7475961538  0.6849642005  0.8554216867  0.8072289157  0.9789473684  0.9520958084  0.9954887218  0.9700598802  0.9292168675  0.9578313253  0.7409638554  0.7831325301  0.9201807229  0.9216867470  0.9520239880  0.9161676647  0.7542168675  0.8454106280  4.8192771084  2.0014156461  0.5855388641  100           0.1011803532 
0.7369192915  0.9804216867  0.9457831325  0.7379807692  0.6968973747  0.9382530120  0.8734939759  0.9744360902  0.9580838323  0.9939849624  0.9760479042  0.9668674699  0.9518072289  0.7801204819  0.8192771084  0.9578313253  0.9518072289  0.9850074963  0.9760479042  0.7012048193  0.8115942029  7.2289156627  1.2914198494  0.5855388641  150           0.0986236334 
0.7273507453  0.9969879518  0.9879518072  0.7475961538  0.6515513126  0.8990963855  0.8373493976  0.9939849624  0.9760479042  0.9969924812  0.9760479042  0.9849397590  0.9759036145  0.7424698795  0.8012048193  0.9789156627  0.9638554217  0.9850074963  0.9520958084  0.7542168675  0.7560386473  9.6385542169  0.9406963205  0.5855388641  200           0.0999125767 
0.7302103937  0.9984939759  0.9939759036  0.7355769231  0.7159904535  0.9442771084  0.8734939759  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9804216867  0.9819277108  0.7786144578  0.8253012048  0.9743975904  0.9819277108  0.9835082459  0.9580838323  0.7204819277  0.7487922705  12.048192771  0.7396773261  0.5855388641  250           0.1007463408 
0.7098636955  0.9984939759  0.9939759036  0.7043269231  0.6658711217  0.9593373494  0.8734939759  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9954819277  0.9939759036  0.7680722892  0.8192771084  0.9924698795  0.9819277108  0.9940029985  0.9640718563  0.7277108434  0.7415458937  14.457831325  0.5257803231  0.5855388641  300           0.0979042578 
0.7038052464  0.9924698795  0.9698795181  0.7091346154  0.6849642005  0.9939759036  0.9156626506  0.9954887218  0.9700598802  0.9969924812  0.9760479042  0.9894578313  0.9819277108  0.8870481928  0.8855421687  0.9924698795  0.9578313253  0.9970014993  0.9760479042  0.6554216867  0.7657004831  16.867469879  0.4808847499  0.5855388641  350           0.0985589647 
0.7176400404  0.9984939759  0.9939759036  0.7403846154  0.6730310263  0.9774096386  0.8855421687  0.9984962406  0.9940119760  0.9984962406  0.9820359281  0.9924698795  0.9819277108  0.7620481928  0.8072289157  0.9909638554  0.9879518072  0.9985007496  0.9820359281  0.7421686747  0.7149758454  19.277108433  0.3881925997  0.5855388641  400           0.0977653408 
0.7032933271  0.9969879518  0.9879518072  0.7091346154  0.6443914081  0.9954819277  0.9216867470  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9939759036  0.9879518072  0.8795180723  0.8795180723  0.9954819277  0.9819277108  0.9970014993  0.9760479042  0.7060240964  0.7536231884  21.686746988  0.3346530271  0.5855388641  450           0.0981002522 
0.7037401769  1.0000000000  1.0000000000  0.7067307692  0.6992840095  0.9743975904  0.8975903614  0.9984962406  0.9940119760  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.8072289157  0.8554216867  0.9969879518  0.9879518072  0.9985007496  0.9820359281  0.6963855422  0.7125603865  24.096385542  0.2735098663  0.5855388641  500           0.0970531225 
0.6977261132  1.0000000000  1.0000000000  0.7019230769  0.6945107399  0.9743975904  0.8855421687  0.9984962406  0.9940119760  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.7906626506  0.8253012048  0.9984939759  0.9939759036  0.9985007496  0.9820359281  0.6891566265  0.7053140097  26.506024096  0.2274402027  0.5855388641  550           0.0993038034 
0.7080808074  1.0000000000  1.0000000000  0.7115384615  0.6515513126  0.9909638554  0.9036144578  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.9969879518  1.0000000000  0.7635542169  0.8132530120  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.7373493976  0.7318840580  28.915662650  0.1910124822  0.5855388641  600           0.0976080990 
0.6948148589  1.0000000000  0.9879518072  0.6947115385  0.6587112172  0.9954819277  0.9216867470  0.9984962406  0.9940119760  0.9984962406  0.9820359281  1.0000000000  0.9879518072  0.8493975904  0.8674698795  0.9984939759  0.9939759036  0.9985007496  0.9820359281  0.7036144578  0.7222222222  31.325301204  0.1956463942  0.5855388641  650           0.0973221684 
0.6983991835  1.0000000000  0.9879518072  0.6971153846  0.6730310263  0.9969879518  0.9277108434  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.8328313253  0.8614457831  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.6915662651  0.7318840580  33.734939759  0.1602474003  0.5855655670  700           0.0986827135 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0001
	mldg_beta: 0.2733254999483027
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3317374579  0.3328313253  0.3313253012  0.3317307692  0.3293556086  0.3328313253  0.3313253012  0.3323308271  0.3293413174  0.3323308271  0.3293413174  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4467766117  0.4431137725  0.3325301205  0.3333333333  0.0000000000  1.4670480751  0.0315041542  0             0.5540888309 
0.6083758272  0.7635542169  0.7048192771  0.7091346154  0.4892601432  0.6204819277  0.5783132530  0.7684210526  0.7245508982  0.5052631579  0.5029940120  0.7304216867  0.7409638554  0.4066265060  0.3975903614  0.6626506024  0.6506024096  0.4707646177  0.4910179641  0.6650602410  0.5700483092  2.4096385542  0.8174707902  0.0357069969  50            0.1719672775 
0.6113533143  0.8102409639  0.7590361446  0.7139423077  0.5011933174  0.6460843373  0.5963855422  0.8511278195  0.7904191617  0.6150375940  0.5928143713  0.7424698795  0.7409638554  0.3975903614  0.3855421687  0.7138554217  0.7228915663  0.5412293853  0.5449101796  0.6650602410  0.5652173913  4.8192771084  0.3329096966  0.0358161926  100           0.1758415556 
0.6197309466  0.8915662651  0.8554216867  0.7211538462  0.5226730310  0.6686746988  0.6144578313  0.8962406015  0.8742514970  0.6932330827  0.6646706587  0.7454819277  0.7409638554  0.3885542169  0.3734939759  0.7123493976  0.7168674699  0.5802098951  0.5568862275  0.6698795181  0.5652173913  7.2289156627  0.2050922951  0.0359973907  150           0.1833901978 
0.6173400090  0.8990963855  0.8734939759  0.7187500000  0.5155131265  0.6626506024  0.6144578313  0.9203007519  0.8982035928  0.6887218045  0.6467065868  0.7274096386  0.7168674699  0.3810240964  0.3674698795  0.7033132530  0.6927710843  0.5562218891  0.5449101796  0.6698795181  0.5652173913  9.6385542169  0.1458526053  0.0359973907  200           0.1806527281 
0.6269640822  0.9352409639  0.8975903614  0.7427884615  0.5155131265  0.6807228916  0.6265060241  0.9428571429  0.9401197605  0.7533834586  0.7245508982  0.7349397590  0.7228915663  0.3719879518  0.3674698795  0.7123493976  0.7048192771  0.5532233883  0.5449101796  0.6843373494  0.5652173913  12.048192771  0.1130142269  0.0359973907  250           0.1880557346 
0.6299272682  0.9322289157  0.8855421687  0.7379807692  0.5369928401  0.6867469880  0.6385542169  0.9503759398  0.9461077844  0.7684210526  0.7365269461  0.7560240964  0.7469879518  0.3810240964  0.3674698795  0.7228915663  0.7228915663  0.5862068966  0.5688622754  0.6795180723  0.5652173913  14.457831325  0.0830840461  0.0359973907  300           0.1927559614 
0.6365207308  0.9487951807  0.9397590361  0.7403846154  0.5513126492  0.7590361446  0.7228915663  0.9894736842  0.9820359281  0.9203007519  0.8622754491  0.8584337349  0.8313253012  0.4096385542  0.4096385542  0.8057228916  0.7771084337  0.7226386807  0.7185628743  0.6843373494  0.5700483092  16.867469879  0.0710460904  0.0359973907  350           0.1732225323 
0.5940313789  0.9427710843  0.9156626506  0.7187500000  0.4343675418  0.6445783133  0.5903614458  0.9759398496  0.9880239521  0.7127819549  0.6946107784  0.7394578313  0.7168674699  0.3554216867  0.3493975904  0.6897590361  0.6867469880  0.5187406297  0.5029940120  0.6746987952  0.5483091787  19.277108433  0.0551689749  0.0359973907  400           0.1731465530 
0.6305440899  0.9503012048  0.9337349398  0.7379807692  0.5322195704  0.7725903614  0.7409638554  0.9894736842  0.9820359281  0.9097744361  0.8802395210  0.8674698795  0.8433734940  0.4081325301  0.4156626506  0.8373493976  0.8192771084  0.7391304348  0.7125748503  0.6819277108  0.5700483092  21.686746988  0.0505244204  0.0359973907  450           0.1738873339 
0.6168480867  0.9849397590  0.9879518072  0.7572115385  0.4582338902  0.7093373494  0.6807228916  0.9969924812  0.9880239521  0.8676691729  0.8443113772  0.8298192771  0.7891566265  0.3614457831  0.3493975904  0.7891566265  0.7710843373  0.6131934033  0.5808383234  0.6939759036  0.5579710145  24.096385542  0.0393753157  0.0359973907  500           0.1736636972 
0.6090441924  0.9819277108  0.9879518072  0.7307692308  0.4534606205  0.7123493976  0.6807228916  0.9969924812  0.9880239521  0.8556390977  0.8562874251  0.8298192771  0.7891566265  0.3659638554  0.3554216867  0.7861445783  0.7710843373  0.6161919040  0.5928143713  0.6939759036  0.5579710145  26.506024096  0.0321018676  0.0359973907  550           0.1734106159 
0.6305857805  0.9924698795  0.9939759036  0.7307692308  0.5155131265  0.7650602410  0.7469879518  0.9939849624  0.9760479042  0.9248120301  0.9281437126  0.8734939759  0.8433734940  0.3915662651  0.3734939759  0.8253012048  0.7831325301  0.7181409295  0.6526946108  0.7108433735  0.5652173913  28.915662650  0.0273867189  0.0359973907  600           0.1718212986 
0.6090398685  0.9834337349  0.9819277108  0.7211538462  0.4558472554  0.7063253012  0.6807228916  0.9939849624  0.9760479042  0.8646616541  0.8682634731  0.8388554217  0.7891566265  0.3554216867  0.3493975904  0.7846385542  0.7289156627  0.5937031484  0.5508982036  0.7084337349  0.5507246377  31.325301204  0.0231840414  0.0359973907  650           0.1746991348 
0.6221951638  0.9894578313  0.9819277108  0.7379807692  0.4964200477  0.7500000000  0.7228915663  0.9969924812  0.9760479042  0.9368421053  0.9161676647  0.8780120482  0.8493975904  0.3810240964  0.3674698795  0.8433734940  0.7951807229  0.7076461769  0.6586826347  0.6867469880  0.5676328502  33.734939759  0.0228097659  0.0359973907  700           0.1721704817 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.00025
	mldg_beta: 0.2733254999483027
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.4782380600  0.3960843373  0.4156626506  0.4831730769  0.5178997613  0.3825301205  0.3855421687  0.4195488722  0.4131736527  0.4721804511  0.4910179641  0.4834337349  0.5000000000  0.4322289157  0.4518072289  0.4939759036  0.4939759036  0.5157421289  0.5269461078  0.4819277108  0.4299516908  0.0000000000  1.4643194601  0.0315041542  0             0.5399818420 
0.5999880787  0.8222891566  0.7590361446  0.7019230769  0.4701670644  0.6295180723  0.5903614458  0.8375939850  0.7844311377  0.5503759398  0.5149700599  0.6807228916  0.6626506024  0.3825301205  0.3855421687  0.6581325301  0.6445783133  0.4797601199  0.4910179641  0.6650602410  0.5628019324  2.4096385542  0.5858697103  0.0357069969  50            0.1710252810 
0.5904126113  0.8810240964  0.8493975904  0.7139423077  0.4367541766  0.6129518072  0.5602409639  0.8872180451  0.8502994012  0.6360902256  0.6287425150  0.6972891566  0.6686746988  0.3493975904  0.3373493976  0.6656626506  0.6626506024  0.4707646177  0.4910179641  0.6650602410  0.5458937198  4.8192771084  0.1998018235  0.0357069969  100           0.1727082729 
0.6126442731  0.9623493976  0.9337349398  0.7427884615  0.4582338902  0.6807228916  0.6385542169  0.9774436090  0.9820359281  0.7744360902  0.7245508982  0.7500000000  0.7349397590  0.3719879518  0.3674698795  0.7319277108  0.7228915663  0.5637181409  0.5508982036  0.6843373494  0.5652173913  7.2289156627  0.1079291853  0.0357975960  150           0.1730688572 
0.6203563185  0.9337349398  0.9156626506  0.7307692308  0.5107398568  0.6792168675  0.6325301205  0.9819548872  0.9640718563  0.7804511278  0.7245508982  0.8147590361  0.8012048193  0.3719879518  0.3674698795  0.7680722892  0.7590361446  0.5772113943  0.5688622754  0.6746987952  0.5652173913  9.6385542169  0.0672104801  0.0357975960  200           0.1732812357 
0.6384570510  0.9789156627  0.9879518072  0.7908653846  0.4821002387  0.7168674699  0.6867469880  0.9969924812  0.9880239521  0.8586466165  0.8443113772  0.8358433735  0.8132530120  0.3689759036  0.3674698795  0.7756024096  0.7771084337  0.6041979010  0.5928143713  0.7228915663  0.5579710145  12.048192771  0.0506108789  0.0357975960  250           0.1734809780 
0.6138331143  0.9789156627  0.9879518072  0.7524038462  0.4582338902  0.6837349398  0.6626506024  0.9984962406  0.9940119760  0.8421052632  0.8143712575  0.8283132530  0.7951807229  0.3599397590  0.3554216867  0.7740963855  0.7590361446  0.5757121439  0.5628742515  0.6963855422  0.5483091787  14.457831325  0.0332136339  0.0359678268  300           0.1741085052 
0.6233497213  0.9503012048  0.9096385542  0.7283653846  0.5202863962  0.7484939759  0.7289156627  0.9864661654  0.9700598802  0.9172932331  0.8742514970  0.8840361446  0.8493975904  0.4051204819  0.4156626506  0.8478915663  0.8012048193  0.7136431784  0.6706586826  0.6746987952  0.5700483092  16.867469879  0.0248097852  0.0359678268  350           0.1746484089 
0.5832296126  0.9548192771  0.9397590361  0.7067307692  0.4152744630  0.6265060241  0.5783132530  0.9834586466  0.9940119760  0.7187969925  0.6826347305  0.7740963855  0.7349397590  0.3448795181  0.3313253012  0.7123493976  0.7048192771  0.4602698651  0.4850299401  0.6819277108  0.5289855072  19.277108433  0.0183384911  0.0360236168  400           0.1753462982 
0.6311106218  0.9457831325  0.9156626506  0.7187500000  0.5513126492  0.8057228916  0.7771084337  0.9834586466  0.9700598802  0.9503759398  0.9341317365  0.8930722892  0.8614457831  0.4337349398  0.4457831325  0.8825301205  0.8313253012  0.8095952024  0.7904191617  0.6867469880  0.5676328502  21.686746988  0.0183745310  0.0360236168  450           0.1746403742 
0.6143925302  0.9924698795  0.9939759036  0.7163461538  0.4844868735  0.7379518072  0.7228915663  0.9834586466  0.9700598802  0.9112781955  0.9101796407  0.8704819277  0.8433734940  0.3689759036  0.3674698795  0.8253012048  0.7831325301  0.6521739130  0.6167664671  0.7108433735  0.5458937198  24.096385542  0.0145259307  0.0360236168  500           0.1776282215 
0.5946209770  0.9924698795  0.9939759036  0.6971153846  0.4463007160  0.7168674699  0.6867469880  0.9849624060  0.9760479042  0.8556390977  0.8562874251  0.8644578313  0.8313253012  0.3599397590  0.3674698795  0.8147590361  0.7771084337  0.6206896552  0.5988023952  0.6819277108  0.5531400966  26.506024096  0.0117140751  0.0360236168  550           0.1881242132 
0.6293349328  0.9924698795  0.9939759036  0.7235576923  0.5346062053  0.7831325301  0.7590361446  0.9789473684  0.9640718563  0.9548872180  0.9520958084  0.8915662651  0.8554216867  0.4021084337  0.4156626506  0.8298192771  0.7891566265  0.7481259370  0.6886227545  0.7012048193  0.5579710145  28.915662650  0.0091485808  0.0360236168  600           0.1844603968 
0.6174003240  0.9924698795  0.9939759036  0.7067307692  0.4892601432  0.7364457831  0.7048192771  0.9804511278  0.9580838323  0.9263157895  0.9221556886  0.8795180723  0.8433734940  0.3629518072  0.3674698795  0.8192771084  0.7710843373  0.6566716642  0.6227544910  0.7253012048  0.5483091787  31.325301204  0.0071819069  0.0360236168  650           0.1834855843 
0.6268880864  0.9849397590  0.9759036145  0.6995192308  0.5584725537  0.7710843373  0.7469879518  0.9699248120  0.9520958084  0.9729323308  0.9520958084  0.8885542169  0.8554216867  0.3840361446  0.3915662651  0.8554216867  0.7951807229  0.7676161919  0.7305389222  0.6819277108  0.5676328502  33.734939759  0.0070657789  0.0360236168  700           0.1773032570 

trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	mldg_beta: 0.2733254999483027
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3317374579  0.3328313253  0.3313253012  0.3317307692  0.3293556086  0.3328313253  0.3313253012  0.3323308271  0.3293413174  0.3323308271  0.3293413174  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4467766117  0.4431137725  0.3325301205  0.3333333333  0.0000000000  1.4695294946  0.0315041542  0             0.5405218601 
0.5857044037  0.6927710843  0.6746987952  0.6899038462  0.4057279236  0.5783132530  0.5662650602  0.7233082707  0.7125748503  0.4661654135  0.4670658683  0.7530120482  0.7469879518  0.4382530120  0.4638554217  0.6972891566  0.6686746988  0.4992503748  0.5329341317  0.6650602410  0.5821256039  2.4096385542  1.0370562252  0.0357069969  50            0.1813512516 
0.6239335894  0.7831325301  0.7349397590  0.7139423077  0.5346062053  0.6807228916  0.6385542169  0.7954887218  0.7365269461  0.5864661654  0.5628742515  0.7650602410  0.7590361446  0.4397590361  0.4578313253  0.7334337349  0.7409638554  0.5202398801  0.5329341317  0.6650602410  0.5821256039  4.8192771084  0.5262745305  0.0357069969  100           0.1797763634 
0.6310747791  0.8147590361  0.7650602410  0.7115384615  0.5704057279  0.6867469880  0.6506024096  0.8481203008  0.7904191617  0.6661654135  0.6646706587  0.7695783133  0.7650602410  0.4397590361  0.4578313253  0.7454819277  0.7409638554  0.6101949025  0.6047904192  0.6650602410  0.5772946860  7.2289156627  0.3449024127  0.0357069969  150           0.1939178610 
0.6376695374  0.8313253012  0.7951807229  0.7307692308  0.5775656325  0.7003012048  0.6445783133  0.8721804511  0.8143712575  0.6932330827  0.6766467066  0.7680722892  0.7710843373  0.4337349398  0.4578313253  0.7500000000  0.7469879518  0.6251874063  0.6167664671  0.6698795181  0.5724637681  9.6385542169  0.2553798967  0.0357704163  200           0.1745146370 
0.6340823721  0.8644578313  0.8433734940  0.7283653846  0.5656324582  0.6852409639  0.6204819277  0.8766917293  0.8323353293  0.7218045113  0.6946107784  0.7605421687  0.7530120482  0.3960843373  0.3795180723  0.7259036145  0.7349397590  0.6011994003  0.5928143713  0.6746987952  0.5676328502  12.048192771  0.2046366383  0.0357704163  250           0.1729906034 
0.6113490115  0.8403614458  0.7951807229  0.7115384615  0.5035799523  0.6265060241  0.5903614458  0.8631578947  0.8263473054  0.6390977444  0.6287425150  0.7274096386  0.7168674699  0.3689759036  0.3674698795  0.7033132530  0.7048192771  0.5412293853  0.5329341317  0.6650602410  0.5652173913  14.457831325  0.1626610577  0.0357704163  300           0.1745857716 
0.6412681420  0.8810240964  0.8734939759  0.7355769231  0.5823389021  0.7138554217  0.6626506024  0.9263157895  0.8982035928  0.8150375940  0.7664670659  0.7771084337  0.7710843373  0.4111445783  0.4156626506  0.7620481928  0.7469879518  0.6701649175  0.6646706587  0.6771084337  0.5700483092  16.867469879  0.1446165142  0.0358314514  350           0.1731085253 
0.6125494795  0.8915662651  0.8554216867  0.7163461538  0.5035799523  0.6521084337  0.5963855422  0.9067669173  0.8922155689  0.7082706767  0.7005988024  0.7469879518  0.7349397590  0.3719879518  0.3674698795  0.7138554217  0.7228915663  0.5922038981  0.5808383234  0.6674698795  0.5628019324  19.277108433  0.1211287982  0.0358314514  400           0.1743779516 
0.6472749512  0.9216867470  0.8915662651  0.7475961538  0.5871121718  0.7409638554  0.7228915663  0.9774436090  0.9580838323  0.8947368421  0.8443113772  0.8072289157  0.8072289157  0.4156626506  0.4337349398  0.7861445783  0.7590361446  0.7121439280  0.7245508982  0.6819277108  0.5724637681  21.686746988  0.1077785852  0.0358314514  450           0.1722401810 
0.6263702712  0.9397590361  0.9036144578  0.7451923077  0.5107398568  0.6671686747  0.6325301205  0.9639097744  0.9760479042  0.7954887218  0.7485029940  0.7515060241  0.7409638554  0.3750000000  0.3674698795  0.7198795181  0.7228915663  0.5742128936  0.5568862275  0.6867469880  0.5628019324  24.096385542  0.0914262651  0.0358314514  500           0.1742153835 
0.6281732043  0.9412650602  0.9216867470  0.7451923077  0.5131264916  0.6837349398  0.6385542169  0.9744360902  0.9820359281  0.8150375940  0.7664670659  0.7725903614  0.7650602410  0.3780120482  0.3674698795  0.7349397590  0.7349397590  0.5862068966  0.5808383234  0.6891566265  0.5652173913  26.506024096  0.0783198842  0.0358352661  550           0.1730546093 
0.6389246184  0.9457831325  0.9277108434  0.7403846154  0.5536992840  0.7168674699  0.6867469880  0.9939849624  0.9880239521  0.8842105263  0.8383233533  0.8177710843  0.8132530120  0.3990963855  0.3915662651  0.7665662651  0.7650602410  0.6536731634  0.6347305389  0.6915662651  0.5700483092  28.915662650  0.0687420003  0.0358352661  600           0.1734596300 
0.6270013940  0.9653614458  0.9578313253  0.7548076923  0.4964200477  0.7078313253  0.6867469880  0.9969924812  0.9880239521  0.8616541353  0.8323353293  0.8463855422  0.8313253012  0.3780120482  0.3795180723  0.7740963855  0.7590361446  0.6506746627  0.6347305389  0.6939759036  0.5628019324  31.325301204  0.0571136505  0.0358352661  650           0.1723811483 
0.6430753710  0.9503012048  0.9337349398  0.7355769231  0.5823389021  0.7545180723  0.7289156627  0.9954887218  0.9940119760  0.9233082707  0.8742514970  0.8614457831  0.8313253012  0.4066265060  0.3975903614  0.8087349398  0.7771084337  0.7061469265  0.7005988024  0.6843373494  0.5700483092  33.734939759  0.0566467698  0.0358352661  700           0.1728809500 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 1e-05
	mldg_beta: 0.2733254999483027
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3317374579  0.3328313253  0.3313253012  0.3317307692  0.3293556086  0.3328313253  0.3313253012  0.3323308271  0.3293413174  0.3323308271  0.3293413174  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4467766117  0.4431137725  0.3325301205  0.3333333333  0.0000000000  1.4722137339  0.0315041542  0             0.5334084034 
0.4137846679  0.3403614458  0.3373493976  0.3653846154  0.5322195704  0.3885542169  0.3975903614  0.3669172932  0.3712574850  0.4646616541  0.4850299401  0.4503012048  0.4518072289  0.4984939759  0.5120481928  0.4774096386  0.4879518072  0.5787106447  0.5988023952  0.3807228916  0.3768115942  2.4096385542  1.3522814508  0.0357069969  50            0.1736302710 
0.5956020368  0.5075301205  0.4879518072  0.6394230769  0.5632458234  0.6250000000  0.6325301205  0.5909774436  0.6167664671  0.6511278195  0.6526946108  0.6174698795  0.6024096386  0.6325301205  0.6626506024  0.7033132530  0.7289156627  0.7286356822  0.7425149701  0.5879518072  0.5917874396  4.8192771084  1.1711762314  0.0357069969  100           0.1885900021 
0.5892629457  0.6430722892  0.6566265060  0.6658653846  0.4367541766  0.5888554217  0.5843373494  0.6601503759  0.6646706587  0.5308270677  0.5089820359  0.7469879518  0.7710843373  0.4638554217  0.5180722892  0.7349397590  0.6987951807  0.5502248876  0.5688622754  0.6650602410  0.5893719807  7.2289156627  0.9785348139  0.0357198715  150           0.1782418394 
0.5820814231  0.6581325301  0.6566265060  0.6658653846  0.4152744630  0.5677710843  0.5481927711  0.6842105263  0.6886227545  0.4751879699  0.4790419162  0.7560240964  0.7469879518  0.4141566265  0.4277108434  0.6912650602  0.6686746988  0.5142428786  0.5449101796  0.6650602410  0.5821256039  9.6385542169  0.8006487377  0.0357832909  200           0.1752985287 
0.5970107994  0.6957831325  0.6746987952  0.6730769231  0.4677804296  0.6114457831  0.6024096386  0.7323308271  0.7125748503  0.4857142857  0.4970059880  0.7575301205  0.7650602410  0.4397590361  0.4457831325  0.7349397590  0.7228915663  0.5202398801  0.5449101796  0.6650602410  0.5821256039  12.048192771  0.6828213478  0.0357832909  250           0.1759424543 
0.6053798330  0.7198795181  0.6746987952  0.6778846154  0.4940334129  0.6159638554  0.6084337349  0.7548872180  0.7185628743  0.5067669173  0.4970059880  0.7635542169  0.7650602410  0.4382530120  0.4397590361  0.7319277108  0.7349397590  0.5247376312  0.5508982036  0.6650602410  0.5845410628  14.457831325  0.5870326016  0.0357832909  300           0.1773516512 
0.6179297804  0.7515060241  0.7048192771  0.6850961538  0.5346062053  0.6355421687  0.6265060241  0.7759398496  0.7305389222  0.5563909774  0.5508982036  0.7680722892  0.7710843373  0.4563253012  0.4759036145  0.7439759036  0.7349397590  0.5592203898  0.5688622754  0.6650602410  0.5869565217  16.867469879  0.5244851452  0.0357832909  350           0.1753534174 
0.6161383008  0.7710843373  0.7108433735  0.6923076923  0.5250596659  0.6385542169  0.6265060241  0.7834586466  0.7365269461  0.5639097744  0.5688622754  0.7680722892  0.7590361446  0.4322289157  0.4277108434  0.7319277108  0.7349397590  0.5352323838  0.5449101796  0.6650602410  0.5821256039  19.277108433  0.4725834676  0.0357832909  400           0.1785782862 
0.6280944927  0.7695783133  0.7048192771  0.6971153846  0.5632458234  0.6716867470  0.6385542169  0.7954887218  0.7365269461  0.6105263158  0.6107784431  0.7665662651  0.7650602410  0.4593373494  0.4759036145  0.7530120482  0.7469879518  0.5907046477  0.5868263473  0.6650602410  0.5869565217  21.686746988  0.4262766659  0.0357832909  450           0.1743066216 
0.6322883150  0.7921686747  0.7469879518  0.7067307692  0.5704057279  0.6762048193  0.6325301205  0.8000000000  0.7425149701  0.6375939850  0.6227544910  0.7695783133  0.7650602410  0.4578313253  0.4698795181  0.7469879518  0.7469879518  0.5922038981  0.5928143713  0.6650602410  0.5869565217  24.096385542  0.3928494899  0.0357832909  500           0.1741437340 
0.6298886677  0.7951807229  0.7469879518  0.7115384615  0.5632458234  0.6746987952  0.6385542169  0.8030075188  0.7425149701  0.6345864662  0.6227544910  0.7680722892  0.7590361446  0.4292168675  0.4397590361  0.7304216867  0.7409638554  0.5727136432  0.5628742515  0.6650602410  0.5797101449  26.506024096  0.3564411295  0.0357832909  550           0.1822830391 
0.6298886677  0.7996987952  0.7530120482  0.7115384615  0.5632458234  0.6762048193  0.6445783133  0.8315789474  0.7724550898  0.6451127820  0.6407185629  0.7695783133  0.7650602410  0.4382530120  0.4518072289  0.7394578313  0.7409638554  0.5892053973  0.5928143713  0.6650602410  0.5797101449  28.915662650  0.3267162783  0.0357832909  600           0.1760652590 
0.6370701903  0.8042168675  0.7590361446  0.7115384615  0.5847255370  0.7108433735  0.6626506024  0.8315789474  0.7844311377  0.6977443609  0.6946107784  0.7695783133  0.7771084337  0.4894578313  0.5120481928  0.7665662651  0.7530120482  0.6701649175  0.6526946108  0.6650602410  0.5869565217  31.325301204  0.3038471836  0.0357832909  650           0.1745215130 
0.6358696669  0.8057228916  0.7530120482  0.7115384615  0.5823389021  0.6927710843  0.6506024096  0.8345864662  0.7724550898  0.6781954887  0.6766467066  0.7710843373  0.7710843373  0.4638554217  0.4939759036  0.7620481928  0.7469879518  0.6431784108  0.6407185629  0.6650602410  0.5845410628  33.734939759  0.2943460443  0.0358881950  700           0.1753762150 
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	mldg_beta: 1.0
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3317374579  0.3328313253  0.3313253012  0.3317307692  0.3293556086  0.3328313253  0.3313253012  0.3323308271  0.3293413174  0.3323308271  0.3293413174  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4467766117  0.4431137725  0.3325301205  0.3333333333  0.0000000000  2.2981325686  0.0315041542  0             0.4894645214 
0.5940490200  0.7063253012  0.6807228916  0.6850961538  0.4439140811  0.5963855422  0.5783132530  0.7383458647  0.7125748503  0.4721804511  0.4790419162  0.7575301205  0.7650602410  0.4548192771  0.4698795181  0.7123493976  0.6927710843  0.5217391304  0.5389221557  0.6650602410  0.5821256039  2.4096385542  1.6063050105  0.0357069969  50            0.1729362965 
0.6251139984  0.7876506024  0.7409638554  0.7067307692  0.5465393795  0.6807228916  0.6385542169  0.7954887218  0.7365269461  0.5984962406  0.5748502994  0.7695783133  0.7650602410  0.4427710843  0.4578313253  0.7364457831  0.7409638554  0.5517241379  0.5508982036  0.6650602410  0.5821256039  4.8192771084  0.7980728988  0.0357069969  100           0.1729549408 
0.6382504953  0.8237951807  0.7771084337  0.7163461538  0.5918854415  0.7003012048  0.6566265060  0.8571428571  0.8023952096  0.6947368421  0.6826347305  0.7710843373  0.7710843373  0.4563253012  0.4759036145  0.7590361446  0.7469879518  0.6326836582  0.6227544910  0.6650602410  0.5797101449  7.2289156627  0.5177911640  0.0357208252  150           0.1730082083 
0.6424644251  0.8403614458  0.7951807229  0.7307692308  0.5894988067  0.7319277108  0.6746987952  0.8736842105  0.8323353293  0.7413533835  0.7125748503  0.7725903614  0.7771084337  0.4608433735  0.4939759036  0.7590361446  0.7469879518  0.6566716642  0.6467065868  0.6698795181  0.5797101449  9.6385542169  0.3819687022  0.0357208252  200           0.1745160913 
0.6466639359  0.8870481928  0.8734939759  0.7427884615  0.5918854415  0.7228915663  0.6746987952  0.9278195489  0.8802395210  0.8135338346  0.7724550898  0.7740963855  0.7710843373  0.4201807229  0.4518072289  0.7530120482  0.7469879518  0.6656671664  0.6467065868  0.6795180723  0.5724637681  12.048192771  0.3031167002  0.0357208252  250           0.1721032715 
0.6322765913  0.8524096386  0.8433734940  0.7259615385  0.5656324582  0.6701807229  0.6325301205  0.8812030075  0.8622754491  0.7443609023  0.7245508982  0.7620481928  0.7590361446  0.3930722892  0.3795180723  0.7334337349  0.7409638554  0.6146926537  0.5988023952  0.6698795181  0.5676328502  14.457831325  0.2364669063  0.0357208252  300           0.1935718489 
0.6466740451  0.8825301205  0.8674698795  0.7403846154  0.5894988067  0.7409638554  0.6867469880  0.9563909774  0.9341317365  0.8631578947  0.8143712575  0.8027108434  0.8132530120  0.4367469880  0.4698795181  0.7951807229  0.7590361446  0.7196401799  0.7305389222  0.6795180723  0.5772946860  16.867469879  0.2079744828  0.0357208252  350           0.1793859100 
0.6197151420  0.8855421687  0.8674698795  0.7187500000  0.5298329356  0.6777108434  0.6265060241  0.9203007519  0.8862275449  0.7578947368  0.7305389222  0.7710843373  0.7590361446  0.3915662651  0.3734939759  0.7379518072  0.7469879518  0.6251874063  0.6047904192  0.6650602410  0.5652173913  19.277108433  0.1741038201  0.0357208252  400           0.1801651144 
0.6454692118  0.9186746988  0.8915662651  0.7355769231  0.5894988067  0.7560240964  0.7349397590  0.9834586466  0.9580838323  0.9187969925  0.8562874251  0.8539156627  0.8373493976  0.4412650602  0.4759036145  0.8132530120  0.7831325301  0.7421289355  0.7604790419  0.6843373494  0.5724637681  21.686746988  0.1515130019  0.0357208252  450           0.1869124222 
0.6323670750  0.9548192771  0.9277108434  0.7475961538  0.5226730310  0.7063253012  0.6927710843  0.9924812030  0.9820359281  0.8736842105  0.8323353293  0.8253012048  0.8072289157  0.3810240964  0.3674698795  0.7620481928  0.7469879518  0.6356821589  0.6227544910  0.6915662651  0.5676328502  24.096385542  0.1294780438  0.0357208252  500           0.1857106256 
0.6269712812  0.9563253012  0.9337349398  0.7403846154  0.5131264916  0.7078313253  0.6987951807  0.9954887218  0.9820359281  0.8646616541  0.8083832335  0.8328313253  0.8012048193  0.3840361446  0.3674698795  0.7620481928  0.7469879518  0.6491754123  0.6287425150  0.6891566265  0.5652173913  26.506024096  0.1113416240  0.0357208252  550           0.1718474007 
0.6275550869  0.9563253012  0.9457831325  0.7283653846  0.5250596659  0.7213855422  0.7048192771  0.9969924812  0.9880239521  0.8887218045  0.8443113772  0.8493975904  0.8313253012  0.3975903614  0.3855421687  0.7831325301  0.7710843373  0.6686656672  0.6347305389  0.6867469880  0.5700483092  28.915662650  0.0973986292  0.0357208252  600           0.1788776159 
0.6305857046  0.9759036145  0.9759036145  0.7524038462  0.5107398568  0.7349397590  0.6987951807  0.9969924812  0.9880239521  0.9007518797  0.8562874251  0.8629518072  0.8373493976  0.3855421687  0.3855421687  0.8117469880  0.8012048193  0.6881559220  0.6407185629  0.6915662651  0.5676328502  31.325301204  0.0810778067  0.0357208252  650           0.1821098709 
0.6394595480  0.9668674699  0.9518072289  0.7187500000  0.5871121718  0.7725903614  0.7530120482  0.9924812030  0.9820359281  0.9473684211  0.9101796407  0.8765060241  0.8433734940  0.4141566265  0.4036144578  0.8478915663  0.8253012048  0.7616191904  0.7425149701  0.6819277108  0.5700483092  33.734939759  0.0792796337  0.0357208252  700           0.1800660706 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3673608900  0.3472222222  0.3395061728  0.3518518519  0.3641975309  0.3564814815  0.3333333333  0.3672839506  0.3703703704  0.4040114613  0.3530864198  0.3567901235  0.3555555556  0.0000000000  4.4635930061  0.1364655495  0             0.4023954868 
0.7973186176  0.9074074074  0.8765432099  0.8040123457  0.7716049383  0.8703703704  0.8703703704  0.8641975309  0.8641975309  0.8510028653  0.7790123457  0.7765432099  0.7827160494  2.4691358025  2.5873503971  0.3012299538  50            0.0770291853 
0.8338559907  0.9567901235  0.9382716049  0.9212962963  0.8641975309  0.9459876543  0.9135802469  0.9490740741  0.9197530864  0.9169054441  0.8185185185  0.7925925926  0.8074074074  4.9382716049  1.7382501388  0.3012299538  100           0.0761217213 
0.8584403410  0.9753086420  0.9691358025  0.9706790123  0.9197530864  0.9753086420  0.9382716049  0.9768518519  0.9506172840  0.9584527221  0.8234567901  0.8185185185  0.8333333333  7.4074074074  1.3602556205  0.3012299538  150           0.0764033747 
0.8762080371  0.9969135802  0.9814814815  0.9984567901  0.9567901235  0.9938271605  0.9567901235  0.9861111111  0.9506172840  0.9813753582  0.8333333333  0.8419753086  0.8481481481  9.8765432099  1.1723594046  0.3012299538  200           0.0760910749 
0.8765051824  0.9907407407  0.9814814815  0.9984567901  0.9814814815  0.9969135802  0.9567901235  0.9938271605  0.9691358025  0.9899713467  0.8382716049  0.8456790123  0.8320987654  12.345679012  1.0142321992  0.3012299538  250           0.0810165405 
0.8738874739  0.9876543210  0.9753086420  1.0000000000  0.9876543210  1.0000000000  0.9567901235  0.9984567901  0.9814814815  0.9856733524  0.8395061728  0.8370370370  0.8333333333  14.814814814  0.9786240900  0.3012299538  300           0.0765118599 
0.8483692384  1.0000000000  0.9814814815  0.9984567901  0.9691358025  0.9953703704  0.9444444444  0.9969135802  0.9691358025  0.9885386819  0.8135802469  0.8086419753  0.7827160494  17.283950617  0.8921832645  0.3012299538  350           0.0769882870 
0.8606768899  1.0000000000  0.9938271605  1.0000000000  0.9814814815  1.0000000000  0.9629629630  0.9984567901  0.9567901235  0.9785100287  0.8283950617  0.8271604938  0.8086419753  19.753086419  0.8724750710  0.3012475967  400           0.0771693087 
0.8406036648  1.0000000000  0.9938271605  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9691358025  0.9871060172  0.8074074074  0.8037037037  0.7641975309  22.222222222  0.8390199053  0.3012475967  450           0.0762157965 
0.8544430295  1.0000000000  0.9938271605  1.0000000000  0.9753086420  1.0000000000  0.9444444444  0.9984567901  0.9691358025  0.9856733524  0.8246913580  0.8172839506  0.7901234568  24.691358024  0.8066665995  0.3012475967  500           0.0775750113 
0.8689987265  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9876543210  0.9871060172  0.8320987654  0.8333333333  0.8234567901  27.160493827  0.7695416451  0.3012475967  550           0.0774601889 
0.8646777389  0.9969135802  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9691358025  0.9984567901  0.9876543210  0.9871060172  0.8320987654  0.8283950617  0.8111111111  29.629629629  0.7842644560  0.3012475967  600           0.0774893332 
0.8513681064  1.0000000000  0.9938271605  1.0000000000  0.9814814815  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.9770773639  0.8259259259  0.8148148148  0.7876543210  32.098765432  0.7550012040  0.3012475967  650           0.0795596981 
0.8514061339  1.0000000000  0.9938271605  1.0000000000  0.9814814815  1.0000000000  0.9629629630  1.0000000000  0.9814814815  0.9871060172  0.8222222222  0.8148148148  0.7814814815  34.567901234  0.7146923602  0.3012475967  700           0.0905647230 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3694444444  0.3549382716  0.3641975309  0.3842592593  0.3950617284  0.3827160494  0.3765432099  0.3919753086  0.4135802469  0.3641975309  0.3740740741  0.3617283951  0.3777777778  0.0000000000  4.5427684784  0.1364655495  0             0.4279646873 
0.7111111111  0.8503086420  0.8395061728  0.8765432099  0.8395061728  0.8487654321  0.8333333333  0.8672839506  0.8209876543  0.7259259259  0.6913580247  0.7123456790  0.7148148148  2.4691358025  2.5861645436  0.3012299538  50            0.0826746130 
0.7799382716  0.9907407407  0.9938271605  1.0000000000  1.0000000000  0.9830246914  0.9938271605  0.9830246914  0.9444444444  0.7432098765  0.7555555556  0.7950617284  0.8259259259  4.9382716049  1.4860491395  0.3012299538  100           0.0772172356 
0.8327160494  0.9969135802  0.9938271605  0.9984567901  1.0000000000  0.9938271605  1.0000000000  0.9969135802  0.9629629630  0.8320987654  0.7814814815  0.8493827160  0.8679012346  7.4074074074  1.0391631007  0.3012299538  150           0.0774343872 
0.8253086420  0.9984567901  0.9876543210  0.9984567901  1.0000000000  1.0000000000  0.9938271605  0.9984567901  0.9629629630  0.8481481481  0.7395061728  0.8358024691  0.8777777778  9.8765432099  0.9548068655  0.3012299538  200           0.0791924095 
0.8348765432  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9969135802  0.9753086420  0.8395061728  0.7839506173  0.8666666667  0.8493827160  12.345679012  0.8957103968  0.3012299538  250           0.0784801102 
0.8645061728  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9753086420  0.8827160494  0.7876543210  0.8888888889  0.8987654321  14.814814814  0.8770835519  0.3013129234  300           0.0782402658 
0.8361111111  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8370370370  0.7617283951  0.8604938272  0.8851851852  17.283950617  0.8348173797  0.3013129234  350           0.0777037573 
0.8259259259  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8283950617  0.7765432099  0.8506172840  0.8481481481  19.753086419  0.8326521039  0.3013129234  400           0.0772064781 
0.8379629630  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8506172840  0.7567901235  0.8567901235  0.8876543210  22.222222222  0.7927664948  0.3833527565  450           0.0777166271 
0.8188271605  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9984567901  0.9753086420  0.8345679012  0.7370370370  0.8283950617  0.8753086420  24.691358024  0.7808136475  0.3833527565  500           0.0773053789 
0.8345679012  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8654320988  0.7740740741  0.8530864198  0.8456790123  27.160493827  0.7515564096  0.3833527565  550           0.0774989510 
0.8222222222  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8172839506  0.7592592593  0.8481481481  0.8641975309  29.629629629  0.7319366837  0.3833527565  600           0.0771584892 
0.8509259259  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8703703704  0.7814814815  0.8827160494  0.8691358025  32.098765432  0.7199911547  0.3833527565  650           0.0775382996 
0.8509259259  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8802469136  0.7851851852  0.8728395062  0.8654320988  34.567901234  0.7170862341  0.3833527565  700           0.0765955353 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3651234568  0.3549382716  0.3703703704  0.3796296296  0.4012345679  0.3765432099  0.3765432099  0.3842592593  0.4074074074  0.3580246914  0.3691358025  0.3604938272  0.3728395062  0.0000000000  3.7969198227  0.1362838745  0             0.4057815075 
0.7104938272  0.8580246914  0.8333333333  0.8750000000  0.8395061728  0.8503086420  0.8395061728  0.8719135802  0.8209876543  0.7209876543  0.6925925926  0.7098765432  0.7185185185  2.4691358025  1.7782221305  0.3010482788  50            0.0717236137 
0.7830246914  0.9907407407  0.9938271605  1.0000000000  0.9938271605  0.9845679012  0.9938271605  0.9876543210  0.9506172840  0.7530864198  0.7703703704  0.7913580247  0.8172839506  4.9382716049  0.6560924762  0.3010482788  100           0.0719739008 
0.8370370370  0.9969135802  0.9938271605  1.0000000000  1.0000000000  0.9969135802  1.0000000000  0.9969135802  0.9691358025  0.8395061728  0.7901234568  0.8555555556  0.8629629630  7.4074074074  0.2518146005  0.3831710815  150           0.0723046875 
0.8422839506  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9984567901  0.9938271605  0.9984567901  0.9691358025  0.8567901235  0.7666666667  0.8580246914  0.8876543210  9.8765432099  0.1797304596  0.3831710815  200           0.0718881845 
0.8487654321  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9969135802  0.9753086420  0.8580246914  0.7950617284  0.8753086420  0.8666666667  12.345679012  0.1413030618  0.3831710815  250           0.0717246866 
0.8663580247  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9629629630  0.8876543210  0.8012345679  0.8975308642  0.8790123457  14.814814814  0.1141333890  0.3831710815  300           0.0710684013 
0.8475308642  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8481481481  0.7740740741  0.8790123457  0.8888888889  17.283950617  0.0900281203  0.3831710815  350           0.0721198606 
0.8432098765  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8469135802  0.7962962963  0.8679012346  0.8617283951  19.753086419  0.0831411157  0.3831710815  400           0.0722557163 
0.8558641975  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8641975309  0.7827160494  0.8876543210  0.8888888889  22.222222222  0.0723805643  0.3831710815  450           0.0824710703 
0.8441358025  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8567901235  0.7666666667  0.8691358025  0.8839506173  24.691358024  0.0724405810  0.3831710815  500           0.0728631353 
0.8459876543  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8506172840  0.7950617284  0.8765432099  0.8617283951  27.160493827  0.0598294433  0.3831710815  550           0.0737936020 
0.8435185185  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8506172840  0.7802469136  0.8765432099  0.8666666667  29.629629629  0.0568821133  0.3831710815  600           0.0734704256 
0.8558641975  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8827160494  0.7901234568  0.8802469136  0.8703703704  32.098765432  0.0540957585  0.3831710815  650           0.0744018936 
0.8703703704  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.9049382716  0.7987654321  0.8950617284  0.8827160494  34.567901234  0.0500868087  0.3831710815  700           0.0771990871 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3623456790  0.3472222222  0.3518518519  0.3580246914  0.3703703704  0.3595679012  0.3395061728  0.3672839506  0.3703703704  0.3543209877  0.3641975309  0.3666666667  0.3641975309  0.0000000000  3.7076332569  0.1362590790  0             0.3955898285 
0.7163580247  0.8302469136  0.8333333333  0.7978395062  0.7530864198  0.8533950617  0.8086419753  0.8487654321  0.8333333333  0.7037037037  0.7123456790  0.7135802469  0.7358024691  2.4691358025  1.8235167813  0.3009815216  50            0.0720412779 
0.7333333333  0.9290123457  0.8765432099  0.9320987654  0.8456790123  0.9336419753  0.8888888889  0.9290123457  0.8703703704  0.7148148148  0.7197530864  0.7530864198  0.7456790123  4.9382716049  0.9881522596  0.3009877205  100           0.0772861195 
0.8259259259  0.9907407407  0.9876543210  0.9629629630  0.9320987654  0.9768518519  0.9320987654  0.9691358025  0.9567901235  0.8135802469  0.8320987654  0.8123456790  0.8456790123  7.4074074074  0.6063174212  0.3831763268  150           0.0731228447 
0.8753086420  0.9984567901  0.9938271605  0.9922839506  0.9567901235  0.9876543210  0.9506172840  0.9814814815  0.9629629630  0.8679012346  0.8753086420  0.8555555556  0.9024691358  9.8765432099  0.3457783896  0.3831763268  200           0.0722471476 
0.8734567901  0.9984567901  0.9938271605  0.9953703704  0.9753086420  0.9907407407  0.9567901235  0.9938271605  0.9753086420  0.8580246914  0.8728395062  0.8629629630  0.9000000000  12.345679012  0.2410999760  0.3831977844  250           0.0729595327 
0.8287037037  1.0000000000  0.9938271605  1.0000000000  0.9814814815  1.0000000000  0.9567901235  0.9984567901  0.9814814815  0.8049382716  0.8345679012  0.8197530864  0.8555555556  14.814814814  0.1961026582  0.3831977844  300           0.0713078594 
0.8037037037  0.9953703704  0.9814814815  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.7691358025  0.8086419753  0.8049382716  0.8320987654  17.283950617  0.1572285922  0.3831977844  350           0.0731332302 
0.8435185185  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8209876543  0.8469135802  0.8358024691  0.8703703704  19.753086419  0.1233258028  0.3831977844  400           0.0759851074 
0.8469135802  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8209876543  0.8518518519  0.8407407407  0.8740740741  22.222222222  0.1109356903  0.3831977844  450           0.0795797205 
0.8290123457  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.7975308642  0.8308641975  0.8320987654  0.8555555556  24.691358024  0.0908838490  0.3831977844  500           0.0801073313 
0.8444444444  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8234567901  0.8432098765  0.8382716049  0.8728395062  27.160493827  0.0913578624  0.3831977844  550           0.0741856575 
0.8179012346  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.7888888889  0.8185185185  0.8123456790  0.8518518519  29.629629629  0.0924360071  0.3831977844  600           0.0748304558 
0.8574074074  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8308641975  0.8555555556  0.8518518519  0.8913580247  32.098765432  0.0712838703  0.3831977844  650           0.0801822948 
0.8577160494  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8234567901  0.8493827160  0.8629629630  0.8950617284  34.567901234  0.0655900507  0.3831977844  700           0.0788024664 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3281014030  0.3313253012  0.3373493976  0.3389423077  0.3365155131  0.3388554217  0.3313253012  0.3323308271  0.3293413174  0.3398496241  0.3353293413  0.4442771084  0.4518072289  0.4322289157  0.4277108434  0.4427710843  0.4457831325  0.4452773613  0.4491017964  0.3036144578  0.3333333333  0.0000000000  8.0963048935  0.2905368805  0             0.4747278690 
0.7213008040  0.9231927711  0.9216867470  0.7235576923  0.6658711217  0.8674698795  0.8554216867  0.9323308271  0.8982035928  0.8902255639  0.9101796407  0.7545180723  0.7771084337  0.8177710843  0.8493975904  0.7906626506  0.7289156627  0.9115442279  0.8742514970  0.7493975904  0.7463768116  2.4096385542  4.8681198549  0.4777755737  50            0.1295868587 
0.7651262081  0.9819277108  0.9518072289  0.7451923077  0.7422434368  0.8403614458  0.7951807229  0.9789473684  0.9640718563  0.9939849624  0.9760479042  0.9367469880  0.9397590361  0.7153614458  0.7650602410  0.9442771084  0.9337349398  0.9520239880  0.9041916168  0.7493975904  0.8236714976  4.8192771084  2.2244086075  0.4777755737  100           0.1250281906 
0.7584695963  0.9743975904  0.9457831325  0.7331730769  0.7589498807  0.8569277108  0.8012048193  0.9789473684  0.9520958084  0.9939849624  0.9760479042  0.9382530120  0.9337349398  0.7334337349  0.7891566265  0.9442771084  0.9216867470  0.9595202399  0.9341317365  0.7132530120  0.8285024155  7.2289156627  1.8037268448  0.4777755737  150           0.1236177063 
0.7584782574  0.9743975904  0.9457831325  0.7331730769  0.7565632458  0.8584337349  0.7951807229  0.9789473684  0.9640718563  0.9939849624  0.9760479042  0.9382530120  0.9337349398  0.7364457831  0.7891566265  0.9412650602  0.9337349398  0.9625187406  0.9341317365  0.7108433735  0.8333333333  9.6385542169  1.7822193456  0.4777755737  200           0.1254364967 
0.7536791293  0.9698795181  0.9397590361  0.7283653846  0.7494033413  0.8644578313  0.8072289157  0.9759398496  0.9520958084  0.9939849624  0.9760479042  0.9412650602  0.9337349398  0.7635542169  0.8012048193  0.9382530120  0.9216867470  0.9745127436  0.9580838323  0.7036144578  0.8333333333  12.048192771  1.7603806233  0.4777894020  250           0.1247150850 
0.7542787536  0.9668674699  0.9397590361  0.7235576923  0.7541766110  0.8915662651  0.8433734940  0.9759398496  0.9520958084  0.9939849624  0.9760479042  0.9397590361  0.9397590361  0.7816265060  0.8253012048  0.9382530120  0.9216867470  0.9775112444  0.9580838323  0.6963855422  0.8429951691  14.457831325  1.7479213023  0.4777894020  300           0.1322419977 
0.7548998227  0.9668674699  0.9397590361  0.7307692308  0.7446300716  0.8825301205  0.8313253012  0.9789473684  0.9640718563  0.9939849624  0.9760479042  0.9352409639  0.9337349398  0.7680722892  0.8072289157  0.9412650602  0.9337349398  0.9745127436  0.9580838323  0.7012048193  0.8429951691  16.867469879  1.7878223300  0.4778022766  350           0.1318943262 
0.7471062389  0.9457831325  0.9156626506  0.6995192308  0.7446300716  0.9216867470  0.8554216867  0.9533834586  0.9221556886  0.9894736842  0.9700598802  0.9277108434  0.9156626506  0.8253012048  0.8433734940  0.9186746988  0.8795180723  0.9805097451  0.9700598802  0.6698795181  0.8743961353  19.277108433  1.7463096714  0.4778022766  400           0.1296805286 
0.7512854548  0.9668674699  0.9397590361  0.7139423077  0.7494033413  0.8975903614  0.8433734940  0.9789473684  0.9640718563  0.9939849624  0.9760479042  0.9382530120  0.9337349398  0.7876506024  0.8253012048  0.9412650602  0.9337349398  0.9805097451  0.9580838323  0.6963855422  0.8454106280  21.686746988  1.6972569776  0.4778022766  450           0.1229792833 
0.7536589593  0.9743975904  0.9457831325  0.7259615385  0.7565632458  0.8795180723  0.8192771084  0.9834586466  0.9700598802  0.9939849624  0.9760479042  0.9397590361  0.9397590361  0.7424698795  0.7891566265  0.9427710843  0.9397590361  0.9655172414  0.9341317365  0.7060240964  0.8260869565  24.096385542  1.7328377867  0.4778022766  500           0.1265848017 
0.7518835131  0.9668674699  0.9397590361  0.7187500000  0.7494033413  0.9006024096  0.8433734940  0.9789473684  0.9640718563  0.9939849624  0.9760479042  0.9397590361  0.9397590361  0.7740963855  0.8192771084  0.9412650602  0.9337349398  0.9805097451  0.9580838323  0.6963855422  0.8429951691  26.506024096  1.7522720361  0.4778022766  550           0.1254039860 
0.7500413554  0.9789156627  0.9638554217  0.7403846154  0.7470167064  0.8719879518  0.8132530120  0.9864661654  0.9700598802  0.9969924812  0.9760479042  0.9382530120  0.9457831325  0.7259036145  0.7710843373  0.9292168675  0.9337349398  0.9595202399  0.9101796407  0.7156626506  0.7971014493  28.915662650  1.6684491658  0.4778022766  600           0.1253333139 
0.7512738420  0.9668674699  0.9397590361  0.7235576923  0.7494033413  0.9036144578  0.8433734940  0.9789473684  0.9640718563  0.9939849624  0.9760479042  0.9352409639  0.9337349398  0.7680722892  0.8072289157  0.9412650602  0.9337349398  0.9805097451  0.9580838323  0.6963855422  0.8357487923  31.325301204  1.7154337072  0.4778814316  650           0.1241783428 
0.7530434193  0.9789156627  0.9638554217  0.7379807692  0.7541766110  0.8750000000  0.8132530120  0.9864661654  0.9700598802  0.9969924812  0.9760479042  0.9382530120  0.9457831325  0.7289156627  0.7710843373  0.9292168675  0.9337349398  0.9625187406  0.9221556886  0.7132530120  0.8067632850  33.734939759  1.7352062368  0.4799652100  700           0.1236985302 

trails: 0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1, 2, 3, 4]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0001
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
trails: 0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1, 2, 3, 4]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0001
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
trails: 0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1, 2, 3, 4]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0001
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env1_test_ac  env2_test_ac  env3_test_ac  env4_test_ac  epoch         loss          mem_gb        step          step_time    
0.3978584282  0.2540428733  0.2473684211  0.3975903614  0.3671497585  0.4615384615  0.3651551313  0.0000000000  1.1622893810  0.0164523125  0             0.3011455536 
0.6713476274  0.7754795036  0.7834586466  0.6843373494  0.6570048309  0.7187500000  0.6252983294  0.3008649868  0.8448345482  0.0225863457  50            0.0078358555 
0.7178087446  0.8694998120  0.8736842105  0.7349397590  0.7801932367  0.7355769231  0.6205250597  0.6017299737  0.4833694261  0.0226421356  100           0.0080379534 
0.7591709080  0.8807822490  0.8796992481  0.7590361446  0.8067632850  0.7668269231  0.7040572792  0.9025949605  0.3680425298  0.0226421356  150           0.0084989357 
0.7386980151  0.9244076721  0.9203007519  0.7036144578  0.8260869565  0.7043269231  0.7207637232  1.2034599473  0.3061430663  0.0231795311  200           0.0079741859 
0.7430055608  0.9396389620  0.9360902256  0.7156626506  0.8502415459  0.7211538462  0.6849642005  1.5043249342  0.2455870645  0.0231795311  250           0.0079719782 
0.7020415654  0.9450921399  0.9398496241  0.6481927711  0.8019323671  0.6730769231  0.6849642005  1.8051899210  0.2350227071  0.0243277550  300           0.0079078150 
0.7014592148  0.9586310643  0.9518796992  0.6481927711  0.7995169082  0.6850961538  0.6730310263  2.1060549079  0.2191489928  0.0243277550  350           0.0080818224 
0.7202519333  0.9678450545  0.9601503759  0.7036144578  0.8357487923  0.7163461538  0.6252983294  2.4069198947  0.1983541677  0.0243277550  400           0.0079807615 
0.7091939587  0.9558104551  0.9526315789  0.6795180723  0.7657004831  0.6875000000  0.7040572792  2.7077848815  0.1794653134  0.0251507759  450           0.0079776859 
0.7008896198  0.9695374201  0.9669172932  0.6819277108  0.7729468599  0.6995192308  0.6491646778  3.0086498684  0.1482780827  0.0251507759  500           0.0081921482 
0.6905521294  0.9603234299  0.9481203008  0.6746987952  0.7294685990  0.6730769231  0.6849642005  3.3095148552  0.1365190960  0.0251507759  550           0.0081436491 
0.6893532760  0.9676570139  0.9548872180  0.6674698795  0.7463768116  0.6514423077  0.6921241050  3.6103798420  0.1221731761  0.0251507759  600           0.0078148746 
0.6864548332  0.9687852576  0.9578947368  0.6771084337  0.7584541063  0.6658653846  0.6443914081  3.9112448289  0.1264698335  0.0251507759  650           0.0078801489 
0.6811164522  0.9836404663  0.9736842105  0.6578313253  0.7657004831  0.6971153846  0.6038186158  4.2121098157  0.1329877637  0.0251507759  700           0.0077795458 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: MMD
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0001
	mmd_gamma: 1.5
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        penalty       step          step_time    
0.3323456255  0.3328313253  0.3313253012  0.3341346154  0.3269689737  0.3388554217  0.3313253012  0.3338345865  0.3353293413  0.3323308271  0.3293413174  0.4427710843  0.4457831325  0.4352409639  0.4397590361  0.4442771084  0.4518072289  0.4422788606  0.4371257485  0.3325301205  0.3357487923  0.0000000000  1.1699746847  0.1000323296  0.2607338428  0             0.4307639599 
0.6832549772  0.9126506024  0.9036144578  0.6778846154  0.6897374702  0.8674698795  0.8192771084  0.9112781955  0.8862275449  0.9593984962  0.9580838323  0.8117469880  0.7771084337  0.7695783133  0.8253012048  0.8072289157  0.7469879518  0.8725637181  0.8862275449  0.7253012048  0.6400966184  2.4096385542  0.7835383606  0.1064581871  0.2679954171  50            0.1096907663 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 702, in update
    penalty += self.mmd(features[i], features[j])
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 674, in mmd
    Kyy = self.gaussian_kernel(y, y).mean()
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 663, in gaussian_kernel
    D = self.my_cdist(x, y)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 659, in my_cdist
    x2.transpose(-2, -1), alpha=-2).add_(x1_norm)
KeyboardInterrupt
trails: 0
Args:
	algorithm: CORAL
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0001
	mmd_gamma: 1.5
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        penalty       step          step_time    
0.3347538784  0.3328313253  0.3313253012  0.3341346154  0.3293556086  0.3358433735  0.3313253012  0.3338345865  0.3353293413  0.3338345865  0.3353293413  0.4427710843  0.4457831325  0.4322289157  0.4397590361  0.4412650602  0.4518072289  0.4437781109  0.4431137725  0.3325301205  0.3429951691  0.0000000000  1.1699746847  0.0998263359  0.0020984134  0             0.3976507187 
0.6953608741  0.9337349398  0.9397590361  0.6971153846  0.6706443914  0.8313253012  0.8072289157  0.9142857143  0.8862275449  0.9203007519  0.9221556886  0.8192771084  0.8433734940  0.7951807229  0.8313253012  0.8147590361  0.7530120482  0.9130434783  0.8922155689  0.7349397590  0.6787439614  2.4096385542  0.6853900880  0.1061968803  0.0145013770  50            0.0636059332 
0.7530366085  0.9728915663  0.9518072289  0.7067307692  0.7732696897  0.8644578313  0.8072289157  0.9684210526  0.9341317365  0.9909774436  0.9760479042  0.9533132530  0.9578313253  0.7665662651  0.8012048193  0.9457831325  0.9277108434  0.9760119940  0.9640718563  0.6915662651  0.8405797101  4.8192771084  0.2951800179  0.1062526703  0.0249186214  100           0.0630904150 
0.7210922111  0.9954819277  0.9819277108  0.7115384615  0.7494033413  0.9096385542  0.8433734940  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9352409639  0.9819277108  0.7590361446  0.7951807229  0.9292168675  0.9457831325  0.9835082459  0.9461077844  0.7012048193  0.7222222222  7.2289156627  0.1980481294  0.1062526703  0.0249873525  150           0.0617115688 
0.7108515106  0.9909638554  0.9638554217  0.6826923077  0.7613365155  0.9653614458  0.8855421687  0.9924812030  0.9820359281  0.9969924812  0.9760479042  0.9864457831  0.9698795181  0.7891566265  0.8313253012  0.9789156627  0.9759036145  0.9940029985  0.9760479042  0.6578313253  0.7415458937  9.6385542169  0.1420642221  0.1065416336  0.0232458969  200           0.0623585224 
0.7036167465  0.9969879518  0.9759036145  0.6730769231  0.7613365155  0.9623493976  0.8855421687  0.9924812030  0.9700598802  0.9969924812  0.9760479042  0.9924698795  0.9819277108  0.7891566265  0.8433734940  0.9879518072  0.9638554217  0.9940029985  0.9760479042  0.6578313253  0.7222222222  12.048192771  0.1102469338  0.1077728271  0.0227551587  250           0.0622665215 
0.7054886563  0.9984939759  0.9939759036  0.6850961538  0.7326968974  0.9683734940  0.8975903614  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9939759036  0.9879518072  0.7545180723  0.8012048193  0.9954819277  0.9819277108  0.9970014993  0.9760479042  0.6771084337  0.7270531401  14.457831325  0.0905004289  0.1077728271  0.0225048561  300           0.0633118486 
0.7018439677  1.0000000000  1.0000000000  0.6778846154  0.7446300716  0.9698795181  0.9036144578  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9954819277  0.9939759036  0.7801204819  0.8072289157  0.9954819277  0.9819277108  0.9970014993  0.9760479042  0.6674698795  0.7173913043  16.867469879  0.0767384197  0.1077728271  0.0219128274  350           0.0624480391 
0.6940253840  1.0000000000  1.0000000000  0.6754807692  0.7326968974  0.9713855422  0.9096385542  0.9984962406  0.9940119760  0.9984962406  0.9820359281  0.9954819277  0.9939759036  0.7605421687  0.7891566265  0.9969879518  0.9879518072  0.9985007496  0.9820359281  0.6795180723  0.6884057971  19.277108433  0.0720760234  0.1077728271  0.0201827802  400           0.0620127916 
0.6892881374  1.0000000000  1.0000000000  0.6730769231  0.7016706444  0.9774096386  0.9096385542  0.9984962406  0.9940119760  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.7861445783  0.8192771084  0.9984939759  0.9939759036  0.9985007496  0.9820359281  0.6843373494  0.6980676329  21.686746988  0.0603130564  0.1077728271  0.0201626822  450           0.0630789137 
0.6820806939  1.0000000000  1.0000000000  0.6706730769  0.6897374702  0.9804216867  0.9096385542  0.9984962406  0.9940119760  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.7996987952  0.8253012048  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6843373494  0.6835748792  24.096385542  0.0488933573  0.1077728271  0.0201178471  500           0.0623377895 
0.6778279697  1.0000000000  1.0000000000  0.6586538462  0.7088305489  0.9909638554  0.9156626506  0.9984962406  0.9940119760  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.8328313253  0.8373493976  0.9984939759  0.9939759036  0.9985007496  0.9820359281  0.6554216867  0.6884057971  26.506024096  0.0452667500  0.1091103554  0.0190078405  550           0.0627391863 
0.6653010400  1.0000000000  1.0000000000  0.6562500000  0.6587112172  0.9864457831  0.9216867470  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.7786144578  0.8253012048  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.6554216867  0.6908212560  28.915662650  0.0433532323  0.1091103554  0.0187003991  600           0.0628805065 
0.6717922810  1.0000000000  1.0000000000  0.6514423077  0.7112171838  0.9834337349  0.9216867470  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.8313253012  0.8433734940  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6530120482  0.6714975845  31.325301204  0.0366875520  0.1091103554  0.0185978245  650           0.0642055607 
0.6802707212  1.0000000000  1.0000000000  0.6562500000  0.6968973747  0.9954819277  0.9337349398  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.8719879518  0.8734939759  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6746987952  0.6932367150  33.734939759  0.0325285553  0.1091103554  0.0183446680  700           0.0633390331 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: CORAL
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	mmd_gamma: 1.5
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        penalty       step          step_time    
0.3317374579  0.3328313253  0.3313253012  0.3317307692  0.3293556086  0.3328313253  0.3313253012  0.3323308271  0.3293413174  0.3323308271  0.3293413174  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4467766117  0.4431137725  0.3325301205  0.3333333333  0.0000000000  1.1699746847  0.0998263359  0.0020984134  0             0.3814010620 
0.6099008229  0.9216867470  0.8915662651  0.5336538462  0.6587112172  0.8343373494  0.8072289157  0.8917293233  0.8682634731  0.8616541353  0.8802395210  0.8027108434  0.7650602410  0.6611445783  0.6927710843  0.7560240964  0.7228915663  0.8155922039  0.7904191617  0.6433734940  0.6038647343  2.4096385542  0.8536990476  0.1061458588  0.0062396586  50            0.0627562189 
0.7315393923  0.9382530120  0.9216867470  0.7163461538  0.6849642005  0.8388554217  0.7891566265  0.9308270677  0.8922155689  0.9548872180  0.9401197605  0.8780120482  0.8855421687  0.8087349398  0.8373493976  0.8719879518  0.8012048193  0.9355322339  0.9221556886  0.7132530120  0.8115942029  4.8192771084  0.4573526949  0.1061458588  0.0206212403  100           0.0622492647 
0.7415893143  0.9789156627  0.9638554217  0.6875000000  0.7732696897  0.8885542169  0.8313253012  0.9774436090  0.9700598802  0.9969924812  0.9760479042  0.9051204819  0.9337349398  0.7786144578  0.8253012048  0.8930722892  0.9096385542  0.9670164918  0.9281437126  0.6867469880  0.8188405797  7.2289156627  0.3054064584  0.1061458588  0.0242655559  150           0.0640020227 
0.7301433298  0.9713855422  0.9457831325  0.6899038462  0.7661097852  0.9051204819  0.8373493976  0.9714285714  0.9461077844  0.9939849624  0.9760479042  0.9518072289  0.9518072289  0.7575301205  0.8012048193  0.9397590361  0.9397590361  0.9775112444  0.9580838323  0.6722891566  0.7922705314  9.6385542169  0.2290929040  0.1065030098  0.0233499003  200           0.0629226589 
0.7235093055  0.9804216867  0.9578313253  0.7019230769  0.7565632458  0.9156626506  0.8433734940  0.9834586466  0.9700598802  0.9939849624  0.9760479042  0.9638554217  0.9638554217  0.7756024096  0.8132530120  0.9668674699  0.9638554217  0.9865067466  0.9700598802  0.6819277108  0.7536231884  12.048192771  0.1839297682  0.1081280708  0.0233067754  250           0.0629673576 
0.7271799979  0.9804216867  0.9578313253  0.6923076923  0.7398568019  0.9442771084  0.8734939759  0.9789473684  0.9640718563  0.9939849624  0.9760479042  0.9759036145  0.9518072289  0.7620481928  0.7951807229  0.9683734940  0.9578313253  0.9865067466  0.9580838323  0.7060240964  0.7705314010  14.457831325  0.1545388675  0.1081280708  0.0237195406  300           0.0635164404 
0.7241678942  0.9924698795  0.9819277108  0.6971153846  0.7374701671  0.9548192771  0.8795180723  0.9834586466  0.9700598802  0.9969924812  0.9760479042  0.9879518072  0.9759036145  0.7680722892  0.8072289157  0.9728915663  0.9638554217  0.9925037481  0.9461077844  0.6963855422  0.7657004831  16.867469879  0.1371400289  0.1091365814  0.0236927811  350           0.0638002586 
0.7228922410  0.9969879518  0.9879518072  0.7139423077  0.7517899761  0.9608433735  0.8915662651  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9894578313  0.9819277108  0.7454819277  0.8012048193  0.9864457831  0.9819277108  0.9880059970  0.9640718563  0.7036144578  0.7222222222  19.277108433  0.1303658067  0.1091365814  0.0216751601  400           0.0646835756 
0.7102777306  0.9969879518  0.9879518072  0.6875000000  0.7446300716  0.9638554217  0.8915662651  0.9954887218  0.9820359281  0.9969924812  0.9760479042  0.9924698795  0.9819277108  0.7921686747  0.8192771084  0.9909638554  0.9759036145  0.9940029985  0.9760479042  0.6819277108  0.7270531401  21.686746988  0.1124248430  0.1091365814  0.0216752881  450           0.0632680273 
0.7108973376  0.9969879518  0.9879518072  0.6923076923  0.7350835322  0.9683734940  0.8975903614  0.9924812030  0.9820359281  0.9969924812  0.9760479042  0.9924698795  0.9819277108  0.7710843373  0.8072289157  0.9879518072  0.9759036145  0.9970014993  0.9520958084  0.6939759036  0.7222222222  24.096385542  0.0943487458  0.1091365814  0.0223945157  500           0.0630796003 
0.7096681704  0.9984939759  0.9939759036  0.6826923077  0.7494033413  0.9713855422  0.8975903614  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9939759036  0.9879518072  0.8072289157  0.8433734940  0.9954819277  0.9819277108  0.9970014993  0.9760479042  0.6771084337  0.7294685990  26.506024096  0.0881044908  0.1091365814  0.0217493029  550           0.0641223860 
0.7031351343  0.9984939759  0.9939759036  0.6826923077  0.7112171838  0.9713855422  0.8975903614  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9939759036  0.9879518072  0.7921686747  0.8192771084  0.9924698795  0.9819277108  0.9970014993  0.9760479042  0.6867469880  0.7318840580  28.915662650  0.0838162647  0.1091365814  0.0209275604  600           0.0635357237 
0.7078134580  1.0000000000  1.0000000000  0.6754807692  0.7684964200  0.9698795181  0.9036144578  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9969879518  1.0000000000  0.8132530120  0.8433734940  0.9954819277  0.9819277108  0.9970014993  0.9760479042  0.6674698795  0.7198067633  31.325301204  0.0746962089  0.1091365814  0.0213020972  650           0.0635535717 
0.6995494796  1.0000000000  1.0000000000  0.6754807692  0.7016706444  0.9804216867  0.9096385542  0.9969924812  0.9880239521  0.9984962406  0.9820359281  0.9969879518  1.0000000000  0.8358433735  0.8373493976  0.9954819277  0.9819277108  0.9985007496  0.9700598802  0.6867469880  0.7342995169  33.734939759  0.0674927736  0.1091365814  0.0212954626  700           0.0644492245 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: CORAL
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.00025
	mmd_gamma: 1.5
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        penalty       step          step_time    
0.3601851852  0.3549382716  0.3518518519  0.3456790123  0.3641975309  0.3472222222  0.3518518519  0.3549382716  0.3456790123  0.3469135802  0.3679012346  0.3617283951  0.3641975309  0.0000000000  1.1776857376  0.0521540642  0.0014934132  0             0.3462598324 
0.8287037037  0.9058641975  0.8888888889  0.8842592593  0.8086419753  0.9197530864  0.9320987654  0.9212962963  0.8950617284  0.8086419753  0.8345679012  0.8197530864  0.8518518519  2.4691358025  0.5460964841  0.0584793091  0.0119019795  50            0.0323021269 
0.7737654321  0.9475308642  0.9444444444  0.9814814815  0.9320987654  0.9830246914  0.9382716049  0.9737654321  0.9444444444  0.7592592593  0.7666666667  0.7814814815  0.7876543210  4.9382716049  0.2608438873  0.0584793091  0.0167966220  100           0.0315494442 
0.8518518519  0.9953703704  0.9876543210  0.9861111111  0.9444444444  0.9768518519  0.9382716049  0.9706790123  0.9506172840  0.8395061728  0.8543209877  0.8382716049  0.8753086420  7.4074074074  0.1472846822  0.0584793091  0.0175011731  150           0.0326295185 
0.8287037037  0.9984567901  0.9876543210  0.9984567901  0.9753086420  0.9938271605  0.9506172840  0.9891975309  0.9567901235  0.8172839506  0.8283950617  0.8172839506  0.8518518519  9.8765432099  0.0797619724  0.0614533424  0.0159680529  200           0.0321252298 
0.8280864198  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9506172840  0.9984567901  0.9567901235  0.8209876543  0.8296296296  0.8172839506  0.8444444444  12.345679012  0.0489809506  0.0614533424  0.0148814920  250           0.0315361118 
0.8166666667  0.9984567901  0.9814814815  1.0000000000  0.9814814815  1.0000000000  0.9567901235  0.9984567901  0.9814814815  0.8086419753  0.8172839506  0.8123456790  0.8283950617  14.814814814  0.0383265307  0.0614533424  0.0154562667  300           0.0325787735 
0.8750000000  1.0000000000  0.9876543210  1.0000000000  0.9691358025  1.0000000000  0.9506172840  0.9984567901  0.9691358025  0.8802469136  0.8580246914  0.8629629630  0.8987654321  17.283950617  0.0299752655  0.0614533424  0.0148606537  350           0.0314521599 
0.8040123457  0.9969135802  0.9691358025  1.0000000000  0.9938271605  1.0000000000  0.9629629630  1.0000000000  0.9814814815  0.7827160494  0.8074074074  0.8061728395  0.8197530864  19.753086419  0.0251845817  0.0614533424  0.0124678538  400           0.0317317533 
0.8256172840  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8086419753  0.8345679012  0.8185185185  0.8407407407  22.222222222  0.0184366743  0.0614533424  0.0120709960  450           0.0306608295 
0.8453703704  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9506172840  1.0000000000  0.9753086420  0.8382716049  0.8432098765  0.8308641975  0.8691358025  24.691358024  0.0167831380  0.0614533424  0.0116419091  500           0.0318981171 
0.8160493827  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9691358025  1.0000000000  0.9876543210  0.7962962963  0.8160493827  0.8148148148  0.8370370370  27.160493827  0.0126486814  0.0614533424  0.0107765716  550           0.0314174938 
0.8197530864  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9691358025  1.0000000000  0.9814814815  0.7962962963  0.8172839506  0.8197530864  0.8456790123  29.629629629  0.0123978977  0.0614533424  0.0110129004  600           0.0312744284 
0.8200617284  1.0000000000  0.9938271605  1.0000000000  0.9876543210  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8037037037  0.8172839506  0.8185185185  0.8407407407  32.098765432  0.0105452726  0.0614533424  0.0100982508  650           0.0312523079 
0.8203703704  1.0000000000  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9876543210  0.8111111111  0.8234567901  0.8123456790  0.8345679012  34.567901234  0.0094022547  0.0614533424  0.0092465841  700           0.0317786312 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: CORAL
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0001
	mmd_gamma: 1.5
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        penalty       step          step_time    
0.3722222222  0.3441358025  0.3148148148  0.3194444444  0.3518518519  0.3395061728  0.3827160494  0.3472222222  0.4074074074  0.3679012346  0.3765432099  0.3864197531  0.3580246914  0.0000000000  1.1776857376  0.0521540642  0.0014934132  0             0.3414115906 
0.7237654321  0.8780864198  0.8333333333  0.7916666667  0.7530864198  0.8240740741  0.8580246914  0.8472222222  0.8703703704  0.7074074074  0.7308641975  0.7197530864  0.7370370370  2.4691358025  0.7193211800  0.0584855080  0.0038238510  50            0.0322458649 
0.7330246914  0.9274691358  0.9074074074  0.8379629630  0.8024691358  0.8641975309  0.8641975309  0.8703703704  0.8827160494  0.7234567901  0.7283950617  0.7271604938  0.7530864198  4.9382716049  0.3770428711  0.0584855080  0.0135430708  100           0.0329020166 
0.8104938272  0.9737654321  0.9382716049  0.9490740741  0.8765432099  0.9598765432  0.9259259259  0.9459876543  0.9320987654  0.7987654321  0.8061728395  0.8074074074  0.8296296296  7.4074074074  0.2914066520  0.0611562729  0.0158408790  150           0.0321991825 
0.7845679012  0.9891975309  0.9876543210  0.9305555556  0.8580246914  0.9351851852  0.9135802469  0.9259259259  0.9135802469  0.7617283951  0.7851851852  0.7814814815  0.8098765432  9.8765432099  0.2094170332  0.0611562729  0.0170522218  200           0.0320302773 
0.8268518519  0.9922839506  0.9876543210  0.9876543210  0.9259259259  0.9891975309  0.9320987654  0.9722222222  0.9506172840  0.8209876543  0.8246913580  0.8185185185  0.8432098765  12.345679012  0.1502782260  0.0611562729  0.0166619622  250           0.0313322592 
0.8351851852  0.9953703704  0.9876543210  0.9953703704  0.9629629630  0.9953703704  0.9506172840  0.9876543210  0.9629629630  0.8222222222  0.8382716049  0.8246913580  0.8555555556  14.814814814  0.1140808591  0.0611562729  0.0178192201  300           0.0315382195 
0.8466049383  0.9984567901  0.9876543210  0.9953703704  0.9444444444  0.9969135802  0.9444444444  0.9891975309  0.9629629630  0.8506172840  0.8432098765  0.8234567901  0.8691358025  17.283950617  0.0874207794  0.0611562729  0.0169386021  350           0.0320722151 
0.8126543210  0.9891975309  0.9753086420  1.0000000000  0.9814814815  1.0000000000  0.9567901235  0.9984567901  0.9691358025  0.7975308642  0.8197530864  0.8086419753  0.8246913580  19.753086419  0.0679861030  0.0611562729  0.0157925818  400           0.0315320444 
0.8438271605  1.0000000000  0.9876543210  1.0000000000  0.9629629630  0.9984567901  0.9506172840  0.9969135802  0.9753086420  0.8370370370  0.8469135802  0.8271604938  0.8641975309  22.222222222  0.0540751082  0.0611562729  0.0152928157  450           0.0321479416 
0.8376543210  1.0000000000  0.9876543210  1.0000000000  0.9691358025  0.9984567901  0.9567901235  0.9984567901  0.9753086420  0.8271604938  0.8395061728  0.8246913580  0.8592592593  24.691358024  0.0420953502  0.0611562729  0.0153489639  500           0.0320526838 
0.8373456790  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9567901235  1.0000000000  0.9753086420  0.8259259259  0.8444444444  0.8222222222  0.8567901235  27.160493827  0.0343514553  0.0611562729  0.0140738561  550           0.0312400007 
0.8469135802  1.0000000000  0.9876543210  1.0000000000  0.9691358025  1.0000000000  0.9567901235  1.0000000000  0.9691358025  0.8382716049  0.8518518519  0.8259259259  0.8716049383  29.629629629  0.0317842394  0.0611562729  0.0143208439  600           0.0315787506 
0.8358024691  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9567901235  1.0000000000  0.9753086420  0.8234567901  0.8456790123  0.8185185185  0.8555555556  32.098765432  0.0252446072  0.0611562729  0.0138683977  650           0.0316752481 
0.8407407407  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9567901235  1.0000000000  0.9753086420  0.8296296296  0.8419753086  0.8234567901  0.8679012346  34.567901234  0.0204454243  0.0611562729  0.0134308140  700           0.0318529940 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0001
	mldg_beta: 0.2733254999483027
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3367283951  0.3194444444  0.3209876543  0.3179012346  0.3148148148  0.3364197531  0.3580246914  0.3086419753  0.3456790123  0.3246913580  0.3333333333  0.3506172840  0.3382716049  0.0000000000  1.5083287358  0.0312581062  0             0.5039165020 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  2.4691358025  0.8758927005  0.0354886055  50            0.0877836370 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  4.9382716049  0.4487829636  0.0354886055  100           0.0938287401 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  7.4074074074  0.3248034751  0.0355000496  150           0.0883195591 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  9.8765432099  0.2228795685  0.0355000496  200           0.0991643620 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  12.345679012  0.1465577699  0.0355000496  250           0.0891877651 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  14.814814814  0.1109785963  0.0355024338  300           0.0895890284 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  17.283950617  0.0730012906  0.0355024338  350           0.0871257830 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  19.753086419  0.0606598279  0.0355024338  400           0.0871546888 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  22.222222222  0.0447807054  0.0355024338  450           0.0907816505 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  24.691358024  0.0357387979  0.0355024338  500           0.0894017601 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  27.160493827  0.0278236626  0.0355024338  550           0.0875529242 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  29.629629629  0.0202144507  0.0355024338  600           0.0869843149 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  32.098765432  0.0189744105  0.0355024338  650           0.0867557955 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  34.567901234  0.0159668835  0.0355024338  700           0.0863997221 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	mldg_beta: 0.2733254999483027
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3296296296  0.3348765432  0.3333333333  0.3333333333  0.3333333333  0.3317901235  0.3333333333  0.3302469136  0.3333333333  0.3296296296  0.3283950617  0.3283950617  0.3320987654  0.0000000000  1.5148896277  0.0312581062  0             0.3943970203 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  2.4691358025  1.0946898875  0.0356330872  50            0.0889333439 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  4.9382716049  0.5892528403  0.0356330872  100           0.0903164625 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  7.4074074074  0.4652122677  0.0356330872  150           0.0873701572 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  9.8765432099  0.3744157480  0.0356330872  200           0.0878815365 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 573, in update
    inner_obj.backward()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.005
	mldg_beta: 0.2733254999483027
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3287037037  0.3209876543  0.3024691358  0.3225308642  0.2962962963  0.3086419753  0.3271604938  0.3240740741  0.3271604938  0.3123456790  0.3271604938  0.3407407407  0.3345679012  0.0000000000  2.0551103503  0.0312581062  0             0.3828675747 
0.5154320988  0.5462962963  0.5679012346  0.4228395062  0.4320987654  0.4691358025  0.5000000000  0.4629629630  0.4938271605  0.5135802469  0.5197530864  0.5160493827  0.5123456790  2.4691358025  0.7566751630  0.0354733467  50            0.0866187334 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  4.9382716049  0.4590356351  0.0354733467  100           0.0852026701 
0.6635802469  0.6666666667  0.6666666667  0.6620370370  0.6604938272  0.6620370370  0.6666666667  0.6666666667  0.6543209877  0.6617283951  0.6666666667  0.6641975309  0.6617283951  7.4074074074  0.1785160480  0.0354733467  150           0.0854904127 
0.6018518519  0.6574074074  0.6666666667  0.6496913580  0.6111111111  0.6527777778  0.6419753086  0.6543209877  0.6172839506  0.6283950617  0.5913580247  0.6037037037  0.5839506173  9.8765432099  0.0938962280  0.0354847908  200           0.0858954859 
0.6660493827  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6654320988  0.6666666667  0.6666666667  0.6654320988  12.345679012  0.0554101394  0.0354847908  250           0.0853403234 
0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  14.814814814  0.0464027127  0.0354847908  300           0.0883644295 
0.4046296296  0.4351851852  0.4753086420  0.4706790123  0.4506172840  0.4537037037  0.4074074074  0.4197530864  0.3888888889  0.3913580247  0.4259259259  0.4086419753  0.3925925926  17.283950617  0.0358152358  0.0354847908  350           0.0866208506 
0.6663580247  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6654320988  19.753086419  0.0484471953  0.0354847908  400           0.0864779043 
0.3382716049  0.3518518519  0.3703703704  0.3564814815  0.3456790123  0.3456790123  0.3395061728  0.3333333333  0.3333333333  0.3333333333  0.3444444444  0.3382716049  0.3370370370  22.222222222  0.0322010054  0.0354847908  450           0.0872087908 
0.6500000000  0.6635802469  0.6666666667  0.6620370370  0.6666666667  0.6620370370  0.6666666667  0.6574074074  0.6543209877  0.6469135802  0.6580246914  0.6543209877  0.6407407407  24.691358024  0.0307981735  0.0354847908  500           0.0867092609 
0.6629629630  0.6651234568  0.6666666667  0.6666666667  0.6604938272  0.6666666667  0.6604938272  0.6666666667  0.6666666667  0.6629629630  0.6629629630  0.6654320988  0.6604938272  27.160493827  0.0159522442  0.0354847908  550           0.0860691166 
0.6462962963  0.6666666667  0.6543209877  0.6635802469  0.6604938272  0.6635802469  0.6604938272  0.6666666667  0.6666666667  0.6518518519  0.6382716049  0.6530864198  0.6419753086  29.629629629  0.0125462474  0.0354847908  600           0.0851547527 
0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  32.098765432  0.0234748864  0.0354847908  650           0.0866248655 
0.6635802469  0.6682098765  0.6666666667  0.6666666667  0.6666666667  0.6682098765  0.6666666667  0.6697530864  0.6666666667  0.6641975309  0.6629629630  0.6617283951  0.6654320988  34.567901234  0.0229727752  0.0359058380  700           0.0864126825 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.001
	mldg_beta: 0.2733254999483027
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3336419753  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3345679012  0.3333333333  0.0000000000  1.5009221360  0.0312581062  0             0.4084279537 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  2.4691358025  0.5575446285  0.0355038643  50            0.0881340981 
0.3364197531  0.3364197531  0.3395061728  0.3379629630  0.3333333333  0.3395061728  0.3518518519  0.3441358025  0.3395061728  0.3345679012  0.3395061728  0.3333333333  0.3382716049  4.9382716049  0.2307274049  0.0355038643  100           0.0871308231 
0.3345679012  0.3364197531  0.3333333333  0.3333333333  0.3333333333  0.3348765432  0.3456790123  0.3395061728  0.3333333333  0.3345679012  0.3333333333  0.3333333333  0.3370370370  7.4074074074  0.0910499540  0.0355038643  150           0.0873789597 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  9.8765432099  0.0318113072  0.0355038643  200           0.0875486040 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3364197531  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  12.345679012  0.0126797971  0.0355038643  250           0.0869284678 
0.3333333333  0.3827160494  0.3580246914  0.3395061728  0.3395061728  0.3672839506  0.3703703704  0.3719135802  0.3703703704  0.3333333333  0.3333333333  0.3333333333  0.3333333333  14.814814814  0.0081517761  0.0355653763  300           0.0885792923 
0.4219135802  0.6404320988  0.6234567901  0.5648148148  0.5432098765  0.5972222222  0.5987654321  0.6003086420  0.5864197531  0.4209876543  0.4098765432  0.4209876543  0.4358024691  17.283950617  0.0046277021  0.0355653763  350           0.0867186165 
0.3416666667  0.4938271605  0.4259259259  0.3935185185  0.3580246914  0.4104938272  0.4074074074  0.4212962963  0.3950617284  0.3370370370  0.3419753086  0.3395061728  0.3481481481  19.753086419  0.0084677150  0.0355653763  400           0.0875486803 
0.3339506173  0.3595679012  0.3456790123  0.3672839506  0.3518518519  0.3518518519  0.3456790123  0.3564814815  0.3395061728  0.3333333333  0.3333333333  0.3345679012  0.3345679012  22.222222222  0.0070132949  0.0355653763  450           0.0877320004 
0.3580246914  0.5432098765  0.5185185185  0.4521604938  0.4259259259  0.4722222222  0.4814814815  0.4830246914  0.4567901235  0.3567901235  0.3518518519  0.3530864198  0.3703703704  24.691358024  0.0042450223  0.0355653763  500           0.0903382349 
0.3382716049  0.3996913580  0.3765432099  0.3626543210  0.3580246914  0.3750000000  0.3580246914  0.3657407407  0.3827160494  0.3370370370  0.3370370370  0.3370370370  0.3419753086  27.160493827  0.0022925006  0.0355653763  550           0.0877782202 
0.3336419753  0.3395061728  0.3518518519  0.3348765432  0.3333333333  0.3348765432  0.3333333333  0.3456790123  0.3456790123  0.3333333333  0.3333333333  0.3333333333  0.3345679012  29.629629629  0.0016355242  0.0355653763  600           0.0881939697 
0.3333333333  0.3364197531  0.3395061728  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3348765432  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  32.098765432  0.0017048337  0.0355653763  650           0.0907237244 
0.3422839506  0.4166666667  0.4012345679  0.3780864198  0.3456790123  0.3981481481  0.3888888889  0.4182098765  0.3827160494  0.3432098765  0.3395061728  0.3358024691  0.3506172840  34.567901234  0.0026825782  0.0355653763  700           0.0880604076 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.007
	mldg_beta: 0.2733254999483027
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3435185185  0.3410493827  0.3395061728  0.3425925926  0.3333333333  0.3410493827  0.3456790123  0.3487654321  0.3333333333  0.3444444444  0.3481481481  0.3407407407  0.3407407407  0.0000000000  2.5414644480  0.0312581062  0             0.4078726768 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  2.4691358025  1.4582176691  0.0354876518  50            0.0858425999 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  4.9382716049  1.3991559319  0.0354876518  100           0.0897602987 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 570, in update
    inner_obj = F.cross_entropy(inner_net(xi), yi)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/networks.py", line 285, in forward
    x = self.layer2(x)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 263, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 260, in _conv_forward
    self.padding, self.dilation, self.groups)
KeyboardInterrupt
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0025
	mldg_beta: 0.2733254999483027
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3197530864  0.3086419753  0.3148148148  0.3148148148  0.3086419753  0.3179012346  0.3024691358  0.3179012346  0.3024691358  0.3123456790  0.3222222222  0.3222222222  0.3222222222  0.0000000000  1.6277914718  0.0312581062  0             0.3962008953 
0.3388888889  0.3395061728  0.3333333333  0.3379629630  0.3333333333  0.3410493827  0.3456790123  0.3364197531  0.3333333333  0.3370370370  0.3419753086  0.3370370370  0.3395061728  2.4691358025  0.6078459660  0.0354666710  50            0.0861043930 
0.6234567901  0.6620370370  0.6666666667  0.6296296296  0.5987654321  0.6435185185  0.6419753086  0.6558641975  0.6419753086  0.6049382716  0.6320987654  0.6308641975  0.6259259259  4.9382716049  0.2673483121  0.0354881287  100           0.0857532454 
0.5691358025  0.6280864198  0.6419753086  0.6311728395  0.6172839506  0.6450617284  0.6481481481  0.6496913580  0.6296296296  0.5617283951  0.5666666667  0.5777777778  0.5703703704  7.4074074074  0.1081329854  0.0354881287  150           0.0874480200 
0.3435185185  0.3348765432  0.3333333333  0.3333333333  0.3333333333  0.3395061728  0.3333333333  0.3503086420  0.3395061728  0.3407407407  0.3407407407  0.3407407407  0.3518518519  9.8765432099  0.0397876470  0.0354881287  200           0.0912884951 
0.4308641975  0.4907407407  0.4567901235  0.3950617284  0.3827160494  0.4969135802  0.4876543210  0.5092592593  0.4938271605  0.4209876543  0.4308641975  0.4370370370  0.4345679012  12.345679012  0.0222326137  0.0354881287  250           0.0871613646 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  14.814814814  0.0078586909  0.0354881287  300           0.0875850391 
0.5848765432  0.6358024691  0.6419753086  0.6419753086  0.6358024691  0.6419753086  0.6419753086  0.6435185185  0.6358024691  0.5740740741  0.5740740741  0.5888888889  0.6024691358  17.283950617  0.0118428928  0.0354881287  350           0.0863951302 
0.6280864198  0.6635802469  0.6666666667  0.6620370370  0.6666666667  0.6620370370  0.6666666667  0.6620370370  0.6481481481  0.6246913580  0.6209876543  0.6358024691  0.6308641975  19.753086419  0.0054704718  0.0354881287  400           0.0864722300 
0.4790123457  0.6620370370  0.6604938272  0.6574074074  0.6419753086  0.5817901235  0.5802469136  0.5262345679  0.5370370370  0.5456790123  0.5345679012  0.4370370370  0.3987654321  22.222222222  0.0075257286  0.0354881287  450           0.0877726746 
0.4212962963  0.3564814815  0.3827160494  0.3595679012  0.3641975309  0.3904320988  0.3950617284  0.3996913580  0.3641975309  0.4185185185  0.4222222222  0.4185185185  0.4259259259  24.691358024  0.0202646694  0.0354881287  500           0.0883029938 
0.4033950617  0.3425925926  0.3518518519  0.3364197531  0.3333333333  0.3611111111  0.3518518519  0.3688271605  0.3395061728  0.3938271605  0.3962962963  0.4111111111  0.4123456790  27.160493827  0.0145411769  0.0354881287  550           0.0874436569 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  29.629629629  0.0081456681  0.0354881287  600           0.0880840969 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  32.098765432  0.0057339911  0.0355029106  650           0.0894884729 
0.3938271605  0.3842592593  0.3950617284  0.3549382716  0.3580246914  0.3827160494  0.3765432099  0.4166666667  0.3888888889  0.4000000000  0.3777777778  0.3975308642  0.4000000000  34.567901234  0.0033324493  0.0355029106  700           0.0885731983 
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0009
	mldg_beta: 0.2733254999483027
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.0000000000  1.4976856336  0.0312581062  0             0.3920369148 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  2.4691358025  0.5534721064  0.0354671478  50            0.0871887302 
0.4287037037  0.5509259259  0.5185185185  0.4567901235  0.4444444444  0.4984567901  0.5246913580  0.4891975309  0.5123456790  0.4123456790  0.4407407407  0.4320987654  0.4296296296  4.9382716049  0.2318894248  0.0354814529  100           0.0891554976 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  7.4074074074  0.0903287944  0.0354814529  150           0.0892234230 
0.3333333333  0.3364197531  0.3333333333  0.3333333333  0.3333333333  0.3425925926  0.3456790123  0.3425925926  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  9.8765432099  0.0310721314  0.0370011330  200           0.0949400854 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  12.345679012  0.0124587582  0.0370011330  250           0.0950718451 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 113, in get
    return _ForkingPickler.loads(res)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 282, in rebuild_storage_fd
    fd = df.detach()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 493, in Client
    answer_challenge(c, authkey)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 736, in answer_challenge
    connection.send_bytes(digest)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 404, in _send_bytes
    self._send(header + buf)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 63, in handler
    def handler(signum, frame):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1018554) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.005
	mldg_beta: 0.2733254999483027
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3641975309  0.4012345679  0.3950617284  0.3858024691  0.3827160494  0.3796296296  0.3765432099  0.3750000000  0.3765432099  0.3777777778  0.3567901235  0.3641975309  0.3580246914  0.0000000000  2.1549577266  0.0312581062  0             0.3987891674 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  2.4691358025  0.6623715764  0.0355439186  50            0.0952553320 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  4.9382716049  0.1404203317  0.0355439186  100           0.0999668837 
0.3339506173  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3345679012  0.3345679012  0.3333333333  0.3333333333  7.4074074074  0.0254561582  0.0355439186  150           0.0928358078 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 579, in update
    inner_net.parameters()):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1261, in parameters
    for name, param in self.named_parameters(recurse=recurse):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1288, in named_parameters
    yield elem
KeyboardInterrupt
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0005
	mldg_beta: 0.2733254999483027
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.0000000000  1.5022388399  0.0312581062  0             0.4540750980 
0.3388888889  0.4398148148  0.4691358025  0.4521604938  0.4197530864  0.4351851852  0.4691358025  0.4243827160  0.4135802469  0.3382716049  0.3370370370  0.3395061728  0.3407407407  2.4691358025  0.5003563287  0.0355434418  50            0.0880663395 
0.3530864198  0.4722222222  0.4814814815  0.4706790123  0.4444444444  0.4614197531  0.4753086420  0.4398148148  0.4444444444  0.3604938272  0.3456790123  0.3580246914  0.3481481481  4.9382716049  0.0542387092  0.0355434418  100           0.0904893732 
0.3416666667  0.4151234568  0.4197530864  0.4228395062  0.3888888889  0.4104938272  0.4320987654  0.4012345679  0.3950617284  0.3456790123  0.3395061728  0.3407407407  0.3407407407  7.4074074074  0.0162810157  0.0355434418  150           0.0871174192 
0.3382716049  0.3996913580  0.3950617284  0.3796296296  0.3827160494  0.3811728395  0.4012345679  0.3734567901  0.3518518519  0.3395061728  0.3345679012  0.3395061728  0.3395061728  9.8765432099  0.0063392572  0.0355434418  200           0.0879098272 
0.3382716049  0.4212962963  0.4259259259  0.4012345679  0.3827160494  0.3950617284  0.4074074074  0.3796296296  0.3641975309  0.3395061728  0.3345679012  0.3395061728  0.3395061728  12.345679012  0.0031004052  0.0355434418  250           0.0901317644 
0.3509259259  0.4614197531  0.4876543210  0.4691358025  0.4259259259  0.4552469136  0.4567901235  0.4475308642  0.4197530864  0.3580246914  0.3407407407  0.3518518519  0.3530864198  14.814814814  0.0027484162  0.0355434418  300           0.0885820246 
0.3348765432  0.4058641975  0.4074074074  0.3919753086  0.3827160494  0.3873456790  0.3888888889  0.3672839506  0.3518518519  0.3345679012  0.3333333333  0.3370370370  0.3345679012  17.283950617  0.0011714386  0.0355434418  350           0.0884440947 
0.3333333333  0.3750000000  0.3888888889  0.3688271605  0.3580246914  0.3595679012  0.3641975309  0.3487654321  0.3395061728  0.3333333333  0.3333333333  0.3333333333  0.3333333333  19.753086419  0.0009458783  0.0355434418  400           0.0889540529 
0.3469135802  0.4506172840  0.4753086420  0.4660493827  0.4197530864  0.4320987654  0.4444444444  0.4336419753  0.4197530864  0.3518518519  0.3382716049  0.3493827160  0.3481481481  22.222222222  0.0015857739  0.0355434418  450           0.0887942314 
0.3466049383  0.4629629630  0.4814814815  0.4753086420  0.4320987654  0.4521604938  0.4506172840  0.4367283951  0.4259259259  0.3493827160  0.3407407407  0.3481481481  0.3481481481  24.691358024  0.0014105619  0.0355434418  500           0.0901322269 
0.3354938272  0.4259259259  0.4320987654  0.4305555556  0.4012345679  0.4089506173  0.4382716049  0.4012345679  0.3703703704  0.3358024691  0.3333333333  0.3358024691  0.3370370370  27.160493827  0.0009083834  0.0355434418  550           0.0852510738 
0.3333333333  0.3672839506  0.3703703704  0.3595679012  0.3580246914  0.3564814815  0.3395061728  0.3487654321  0.3395061728  0.3333333333  0.3333333333  0.3333333333  0.3333333333  29.629629629  0.0007334810  0.0355434418  600           0.0866987991 
0.3333333333  0.3364197531  0.3333333333  0.3364197531  0.3395061728  0.3364197531  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  32.098765432  0.0010810107  0.0355434418  650           0.0875789070 
0.3333333333  0.3595679012  0.3641975309  0.3626543210  0.3518518519  0.3487654321  0.3395061728  0.3441358025  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  34.567901234  0.0015531802  0.0355434418  700           0.0868846655 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0001
	mldg_beta: 0.2733254999483027
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3246913580  0.3225308642  0.3395061728  0.3456790123  0.3209876543  0.3472222222  0.3333333333  0.3395061728  0.3456790123  0.3283950617  0.3160493827  0.3197530864  0.3345679012  0.0000000000  1.5158015490  0.0312581062  0             0.4020936489 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  2.4691358025  0.8948939420  0.0354609489  50            0.0856841087 
0.3333333333  0.3348765432  0.3333333333  0.3333333333  0.3395061728  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  4.9382716049  0.3577653930  0.0354609489  100           0.0861095047 
0.3348765432  0.3765432099  0.4135802469  0.3966049383  0.3888888889  0.4012345679  0.3827160494  0.3873456790  0.3641975309  0.3345679012  0.3345679012  0.3358024691  0.3345679012  7.4074074074  0.1587821515  0.0374689102  150           0.0879384613 
0.3379629630  0.3966049383  0.4197530864  0.4043209877  0.3827160494  0.4043209877  0.3765432099  0.3858024691  0.3703703704  0.3395061728  0.3345679012  0.3382716049  0.3395061728  9.8765432099  0.0651434759  0.0374689102  200           0.0878929138 
0.3962962963  0.4984567901  0.4938271605  0.5123456790  0.4814814815  0.5030864198  0.4876543210  0.5092592593  0.4876543210  0.4123456790  0.3604938272  0.4012345679  0.4111111111  12.345679012  0.0358067517  0.0375013351  250           0.0866037226 
0.5432098765  0.6172839506  0.6049382716  0.6280864198  0.6172839506  0.6296296296  0.6172839506  0.6126543210  0.5925925926  0.6024691358  0.4975308642  0.5358024691  0.5370370370  14.814814814  0.0255853918  0.0375013351  300           0.0882888079 
0.5851851852  0.6543209877  0.6419753086  0.6558641975  0.6481481481  0.6574074074  0.6481481481  0.6527777778  0.6358024691  0.6283950617  0.5370370370  0.5827160494  0.5925925926  17.283950617  0.0142201134  0.0375013351  350           0.0855592585 
0.4274691358  0.5432098765  0.5432098765  0.5478395062  0.5185185185  0.5447530864  0.5246913580  0.5462962963  0.5246913580  0.4555555556  0.3802469136  0.4333333333  0.4407407407  19.753086419  0.0111030305  0.0375013351  400           0.0851228046 
0.4811728395  0.5895061728  0.5679012346  0.6095679012  0.5925925926  0.5987654321  0.5802469136  0.5956790123  0.5740740741  0.5209876543  0.4308641975  0.4790123457  0.4938271605  22.222222222  0.0104411597  0.0375013351  450           0.0863203907 
0.6077160494  0.6635802469  0.6604938272  0.6651234568  0.6604938272  0.6635802469  0.6543209877  0.6589506173  0.6543209877  0.6358024691  0.5654320988  0.6123456790  0.6172839506  24.691358024  0.0082605805  0.0375013351  500           0.0890079975 
0.6049382716  0.6651234568  0.6604938272  0.6651234568  0.6604938272  0.6620370370  0.6604938272  0.6589506173  0.6604938272  0.6333333333  0.5555555556  0.6148148148  0.6160493827  27.160493827  0.0051710455  0.0375013351  550           0.0875470400 
0.5716049383  0.6512345679  0.6543209877  0.6604938272  0.6604938272  0.6543209877  0.6481481481  0.6543209877  0.6481481481  0.6037037037  0.5111111111  0.5827160494  0.5888888889  29.629629629  0.0048271021  0.0375013351  600           0.0891329479 
0.6521604938  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6641975309  0.6333333333  0.6518518519  0.6592592593  32.098765432  0.0036767706  0.0375013351  650           0.0938929558 
0.6490740741  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6629629630  0.6259259259  0.6506172840  0.6567901235  34.567901234  0.0035821035  0.0375013351  700           0.0895070076 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 365, in <module>
    save_checkpoint('model.pkl')
KeyboardInterrupt
trails: 0
Args:
	algorithm: MLDG
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0001
	mldg_beta: 0.2733254999483027
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3246913580  0.3225308642  0.3395061728  0.3456790123  0.3209876543  0.3472222222  0.3333333333  0.3395061728  0.3456790123  0.3283950617  0.3160493827  0.3197530864  0.3345679012  0.0000000000  1.5158015490  0.0312581062  0             0.4233820438 
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  2.4691358025  0.8948939420  0.0355439186  50            0.0887639570 
0.3333333333  0.3348765432  0.3333333333  0.3333333333  0.3395061728  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  4.9382716049  0.3577653930  0.0355439186  100           0.0879743099 
0.3348765432  0.3765432099  0.4135802469  0.3966049383  0.3888888889  0.4012345679  0.3827160494  0.3873456790  0.3641975309  0.3345679012  0.3345679012  0.3358024691  0.3345679012  7.4074074074  0.1587821515  0.0355439186  150           0.0864817762 
0.3379629630  0.3966049383  0.4197530864  0.4043209877  0.3827160494  0.4043209877  0.3765432099  0.3858024691  0.3703703704  0.3395061728  0.3345679012  0.3382716049  0.3395061728  9.8765432099  0.0651434759  0.0355439186  200           0.0886735964 
0.3962962963  0.4984567901  0.4938271605  0.5123456790  0.4814814815  0.5030864198  0.4876543210  0.5092592593  0.4876543210  0.4123456790  0.3604938272  0.4012345679  0.4111111111  12.345679012  0.0358067517  0.0355439186  250           0.0881412888 
0.5432098765  0.6172839506  0.6049382716  0.6280864198  0.6172839506  0.6296296296  0.6172839506  0.6126543210  0.5925925926  0.6024691358  0.4975308642  0.5358024691  0.5370370370  14.814814814  0.0255853918  0.0355439186  300           0.0908309221 
0.5851851852  0.6543209877  0.6419753086  0.6558641975  0.6481481481  0.6574074074  0.6481481481  0.6527777778  0.6358024691  0.6283950617  0.5370370370  0.5827160494  0.5925925926  17.283950617  0.0142201134  0.0356645584  350           0.0867108202 
0.4274691358  0.5432098765  0.5432098765  0.5478395062  0.5185185185  0.5447530864  0.5246913580  0.5462962963  0.5246913580  0.4555555556  0.3802469136  0.4333333333  0.4407407407  19.753086419  0.0111030305  0.0356645584  400           0.0887390900 
0.4811728395  0.5895061728  0.5679012346  0.6095679012  0.5925925926  0.5987654321  0.5802469136  0.5956790123  0.5740740741  0.5209876543  0.4308641975  0.4790123457  0.4938271605  22.222222222  0.0104411597  0.0356645584  450           0.0878529453 
0.6077160494  0.6635802469  0.6604938272  0.6651234568  0.6604938272  0.6635802469  0.6543209877  0.6589506173  0.6543209877  0.6358024691  0.5654320988  0.6123456790  0.6172839506  24.691358024  0.0082605805  0.0356645584  500           0.0872558546 
0.6049382716  0.6651234568  0.6604938272  0.6651234568  0.6604938272  0.6620370370  0.6604938272  0.6589506173  0.6604938272  0.6333333333  0.5555555556  0.6148148148  0.6160493827  27.160493827  0.0051710455  0.0356645584  550           0.0895455027 
0.5716049383  0.6512345679  0.6543209877  0.6604938272  0.6604938272  0.6543209877  0.6481481481  0.6543209877  0.6481481481  0.6037037037  0.5111111111  0.5827160494  0.5888888889  29.629629629  0.0048271021  0.0356645584  600           0.0908041430 
0.6521604938  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6641975309  0.6333333333  0.6518518519  0.6592592593  32.098765432  0.0036767706  0.0356645584  650           0.0979087687 
0.6490740741  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6629629630  0.6259259259  0.6506172840  0.6567901235  34.567901234  0.0035821035  0.0356645584  700           0.0978037024 
0.6030864198  0.6635802469  0.6666666667  0.6635802469  0.6604938272  0.6620370370  0.6666666667  0.6574074074  0.6543209877  0.6345679012  0.5493827160  0.6148148148  0.6135802469  37.037037037  0.0034361658  0.0356645584  750           0.0919269896 
0.6533950617  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6629629630  0.6370370370  0.6530864198  0.6604938272  39.506172839  0.0029314329  0.0356645584  800           0.0887850618 
0.6537037037  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6629629630  0.6358024691  0.6555555556  0.6604938272  41.975308642  0.0016653834  0.0356645584  850           0.0866563845 
0.6342592593  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6635802469  0.6666666667  0.6620370370  0.6666666667  0.6530864198  0.5975308642  0.6419753086  0.6444444444  44.444444444  0.0017553959  0.0356645584  900           0.0858641624 
0.6179012346  0.6635802469  0.6666666667  0.6651234568  0.6666666667  0.6635802469  0.6666666667  0.6604938272  0.6666666667  0.6456790123  0.5703703704  0.6296296296  0.6259259259  46.913580246  0.0024092283  0.0356645584  950           0.0876211929 
0.6416666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6635802469  0.6666666667  0.6635802469  0.6666666667  0.6592592593  0.6148148148  0.6432098765  0.6493827160  49.382716049  0.0021855736  0.0356645584  1000          0.0901626253 
0.6382716049  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6555555556  0.6074074074  0.6432098765  0.6469135802  51.851851851  0.0019147741  0.0356645584  1050          0.0876956797 
0.6521604938  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6641975309  0.6296296296  0.6543209877  0.6604938272  54.320987654  0.0021015640  0.0356645584  1100          0.0911079121 
0.6592592593  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6666666667  0.6444444444  0.6617283951  0.6641975309  56.790123456  0.0013493114  0.0356645584  1150          0.0880444860 
0.6361111111  0.6651234568  0.6666666667  0.6666666667  0.6666666667  0.6635802469  0.6666666667  0.6620370370  0.6666666667  0.6567901235  0.6024691358  0.6407407407  0.6444444444  59.259259259  0.0012212800  0.0356645584  1200          0.0883629894 
0.6626543210  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6543209877  0.6654320988  0.6641975309  61.728395061  0.0011001881  0.0356645584  1250          0.0910439301 
0.6645061728  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6592592593  0.6666666667  0.6654320988  64.197530864  0.0011935005  0.0356645584  1300          0.0889148092 
0.6586419753  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6651234568  0.6666666667  0.6654320988  0.6432098765  0.6617283951  0.6641975309  66.666666666  0.0014647739  0.0356645584  1350          0.0897595453 
0.6283950617  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6635802469  0.6666666667  0.6620370370  0.6604938272  0.6419753086  0.5864197531  0.6407407407  0.6444444444  69.135802469  0.0019668260  0.0356645584  1400          0.0891913843 
0.6595679012  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6666666667  0.6444444444  0.6629629630  0.6641975309  71.604938271  0.0011461916  0.0356645584  1450          0.0918450212 
0.6595679012  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6456790123  0.6617283951  0.6641975309  74.074074074  0.0011060411  0.0356645584  1500          0.0910737467 
0.6614197531  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6493827160  0.6654320988  0.6641975309  76.543209876  0.0012264261  0.0356645584  1550          0.0962557173 
0.6466049383  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6666666667  0.6666666667  0.6604938272  0.6160493827  0.6518518519  0.6580246914  79.012345679  0.0016123091  0.0356645584  1600          0.0871089745 
0.6450617284  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6651234568  0.6666666667  0.6635802469  0.6666666667  0.6592592593  0.6135802469  0.6506172840  0.6567901235  81.481481481  0.0007996221  0.0356645584  1650          0.0940736008 
0.6212962963  0.6651234568  0.6666666667  0.6666666667  0.6666666667  0.6635802469  0.6666666667  0.6620370370  0.6666666667  0.6407407407  0.5740740741  0.6358024691  0.6345679012  83.950617284  0.0009286347  0.0356645584  1700          0.0900081015 
0.6549382716  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6629629630  0.6345679012  0.6604938272  0.6617283951  86.419753086  0.0010750148  0.0356645584  1750          0.0901081324 
0.6530864198  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6617283951  0.6296296296  0.6604938272  0.6604938272  88.888888888  0.0007979006  0.0356645584  1800          0.0870719957 
0.5361111111  0.6558641975  0.6481481481  0.6604938272  0.6543209877  0.6574074074  0.6481481481  0.6527777778  0.6543209877  0.5679012346  0.4493827160  0.5555555556  0.5716049383  91.358024691  0.0008499234  0.0356645584  1850          0.0960090876 
0.5141975309  0.6388888889  0.6481481481  0.6558641975  0.6481481481  0.6466049383  0.6419753086  0.6435185185  0.6543209877  0.5345679012  0.4320987654  0.5358024691  0.5543209877  93.827160493  0.0011477354  0.0356645584  1900          0.0893244839 
0.5370370370  0.6466049383  0.6604938272  0.6589506173  0.6543209877  0.6527777778  0.6604938272  0.6481481481  0.6543209877  0.5530864198  0.4629629630  0.5555555556  0.5765432099  96.296296296  0.0009216291  0.0356645584  1950          0.0874982357 
0.6595679012  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6666666667  0.6654320988  0.6493827160  0.6629629630  0.6604938272  98.765432098  0.0007480514  0.0356645584  2000          0.0889294243 

trails: 0
Args:
	algorithm: MMD
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.00025
	mmd_gamma: 1.5
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        penalty       step          step_time    
0.3756172840  0.3503086420  0.3703703704  0.3935185185  0.3827160494  0.3842592593  0.3827160494  0.4043209877  0.3827160494  0.3864197531  0.3666666667  0.3679012346  0.3814814815  0.0000000000  1.1787481308  0.0521540642  0.2513154447  0             0.4114012718 
0.7348765432  0.9074074074  0.8703703704  0.9120370370  0.8641975309  0.8873456790  0.8580246914  0.9166666667  0.8580246914  0.7481481481  0.6987654321  0.7419753086  0.7506172840  2.4691358025  0.6012978691  0.0611500740  0.2550224769  50            0.0433134508 
0.8000000000  0.9922839506  1.0000000000  0.9984567901  1.0000000000  0.9907407407  0.9938271605  0.9876543210  0.9629629630  0.7740740741  0.7679012346  0.8160493827  0.8419753086  4.9382716049  0.1956443033  0.0611500740  0.2740010929  100           0.0425100279 
0.8163580247  0.9969135802  0.9938271605  1.0000000000  1.0000000000  0.9969135802  1.0000000000  0.9969135802  0.9753086420  0.8037037037  0.7641975309  0.8382716049  0.8592592593  7.4074074074  0.0521056159  0.0611500740  0.2757674009  150           0.0418660164 
0.8151234568  0.9984567901  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.9969135802  0.9629629630  0.8148148148  0.7456790123  0.8345679012  0.8654320988  9.8765432099  0.0306522463  0.0611500740  0.2702889389  200           0.0414126730 
0.8021604938  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9938271605  0.9984567901  0.9567901235  0.8074074074  0.7246913580  0.8172839506  0.8592592593  12.345679012  0.0221825401  0.0611500740  0.2514499813  250           0.0407179213 
0.8391975309  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9969135802  1.0000000000  0.9984567901  0.9814814815  0.8530864198  0.7839506173  0.8654320988  0.8543209877  14.814814814  0.0269310952  0.0611500740  0.2522969410  300           0.0412305832 
0.8037037037  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9984567901  0.9691358025  0.8098765432  0.7296296296  0.8160493827  0.8592592593  17.283950617  0.0204553114  0.0611500740  0.2500816014  350           0.0404557133 
0.8432098765  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8555555556  0.7888888889  0.8728395062  0.8555555556  19.753086419  0.0167732088  0.0611500740  0.2409990081  400           0.0416329145 
0.8169753086  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8271604938  0.7432098765  0.8296296296  0.8679012346  22.222222222  0.0158634497  0.0611500740  0.2322977680  450           0.0411187506 
0.8422839506  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8580246914  0.7543209877  0.8604938272  0.8962962963  24.691358024  0.0143428076  0.0611500740  0.2288540119  500           0.0409532547 
0.8271604938  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8209876543  0.7641975309  0.8543209877  0.8691358025  27.160493827  0.0147506065  0.0611500740  0.2274070352  550           0.0425013113 
0.8395061728  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8543209877  0.7580246914  0.8666666667  0.8790123457  29.629629629  0.0135442516  0.0611500740  0.2236883655  600           0.0406022453 
0.8243827160  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8308641975  0.7604938272  0.8469135802  0.8592592593  32.098765432  0.0142712490  0.0611500740  0.2219903928  650           0.0418783092 
0.8401234568  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8432098765  0.7728395062  0.8629629630  0.8814814815  34.567901234  0.0135556364  0.0611500740  0.2230741015  700           0.0418743467 
0.8493827160  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8617283951  0.7814814815  0.8765432099  0.8777777778  37.037037037  0.0113617280  0.0611500740  0.2241660982  750           0.0411805725 
0.8549382716  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8765432099  0.7802469136  0.8851851852  0.8777777778  39.506172839  0.0122088506  0.0611500740  0.2133674070  800           0.0419856167 
0.8101851852  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9753086420  0.8185185185  0.7419753086  0.8271604938  0.8530864198  41.975308642  0.0136571038  0.0611500740  0.2172691700  850           0.0417304039 
0.8299382716  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8432098765  0.7456790123  0.8543209877  0.8765432099  44.444444444  0.0135802655  0.0611500740  0.2144506210  900           0.0417181540 

KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3651234568  0.3549382716  0.3703703704  0.3796296296  0.4012345679  0.3765432099  0.3765432099  0.3842592593  0.4074074074  0.3580246914  0.3691358025  0.3604938272  0.3728395062  0.0000000000  3.7969198227  0.1362838745  0             0.5931973457 
0.7104938272  0.8580246914  0.8333333333  0.8750000000  0.8395061728  0.8503086420  0.8395061728  0.8719135802  0.8209876543  0.7209876543  0.6925925926  0.7098765432  0.7185185185  2.4691358025  1.7782221305  0.3010482788  50            0.0715514803 
0.7830246914  0.9907407407  0.9938271605  1.0000000000  0.9938271605  0.9845679012  0.9938271605  0.9876543210  0.9506172840  0.7530864198  0.7703703704  0.7913580247  0.8172839506  4.9382716049  0.6560924762  0.3011522293  100           0.0720655632 
0.8370370370  0.9969135802  0.9938271605  1.0000000000  1.0000000000  0.9969135802  1.0000000000  0.9969135802  0.9691358025  0.8395061728  0.7901234568  0.8555555556  0.8629629630  7.4074074074  0.2518146005  0.3011522293  150           0.0706715107 
0.8422839506  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9984567901  0.9938271605  0.9984567901  0.9691358025  0.8567901235  0.7666666667  0.8580246914  0.8876543210  9.8765432099  0.1797304596  0.3832535744  200           0.0735435867 
0.8487654321  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9969135802  0.9753086420  0.8580246914  0.7950617284  0.8753086420  0.8666666667  12.345679012  0.1413030618  0.3832535744  250           0.0684635448 
0.8663580247  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9629629630  0.8876543210  0.8012345679  0.8975308642  0.8790123457  14.814814814  0.1141333890  0.3832535744  300           0.0642379856 
0.8475308642  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8481481481  0.7740740741  0.8790123457  0.8888888889  17.283950617  0.0900281203  0.3832535744  350           0.0694551659 
0.8432098765  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8469135802  0.7962962963  0.8679012346  0.8617283951  19.753086419  0.0831411157  0.3832535744  400           0.0713324308 
0.8558641975  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8641975309  0.7827160494  0.8876543210  0.8888888889  22.222222222  0.0723805643  0.3832535744  450           0.0732560253 
0.8441358025  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8567901235  0.7666666667  0.8691358025  0.8839506173  24.691358024  0.0724405810  0.3832535744  500           0.0721770668 
0.8459876543  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8506172840  0.7950617284  0.8765432099  0.8617283951  27.160493827  0.0598294433  0.3832535744  550           0.0716833973 
0.8435185185  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8506172840  0.7802469136  0.8765432099  0.8666666667  29.629629629  0.0568821133  0.3832535744  600           0.0706592560 
0.8558641975  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8827160494  0.7901234568  0.8802469136  0.8703703704  32.098765432  0.0540957585  0.3832535744  650           0.0773987913 
0.8703703704  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.9049382716  0.7987654321  0.8950617284  0.8827160494  34.567901234  0.0500868087  0.3832535744  700           0.0775535107 
0.8466049383  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8679012346  0.7950617284  0.8728395062  0.8506172840  37.037037037  0.0507300359  0.3832535744  750           0.0722093821 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3651234568  0.3549382716  0.3703703704  0.3796296296  0.4012345679  0.3765432099  0.3765432099  0.3842592593  0.4074074074  0.3580246914  0.3691358025  0.3604938272  0.3728395062  0.0000000000  3.7922601700  0.1806769371  0             0.3805203438 
0.7114197531  0.8564814815  0.8395061728  0.8734567901  0.8456790123  0.8549382716  0.8395061728  0.8734567901  0.8148148148  0.7209876543  0.6950617284  0.7098765432  0.7197530864  2.4691358025  1.7483322001  0.5056262016  50            0.0826457739 
0.7802469136  0.9907407407  0.9938271605  1.0000000000  0.9938271605  0.9845679012  0.9938271605  0.9891975309  0.9506172840  0.7530864198  0.7679012346  0.7876543210  0.8123456790  4.9382716049  0.5644124800  0.5056262016  100           0.0792655420 
0.8358024691  0.9969135802  1.0000000000  1.0000000000  1.0000000000  0.9969135802  1.0000000000  0.9969135802  0.9691358025  0.8382716049  0.7913580247  0.8543209877  0.8592592593  7.4074074074  0.1601684451  0.5056262016  150           0.0781276941 
0.8425925926  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9984567901  0.9938271605  0.9984567901  0.9691358025  0.8555555556  0.7716049383  0.8666666667  0.8765432099  9.8765432099  0.1012004386  0.5086221695  200           0.0784629297 
0.8503086420  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9969135802  0.9691358025  0.8604938272  0.7987654321  0.8790123457  0.8629629630  12.345679012  0.0757680217  0.5086221695  250           0.0764905119 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3629629630  0.3487654321  0.3765432099  0.3842592593  0.3765432099  0.3734567901  0.3703703704  0.3919753086  0.3703703704  0.3691358025  0.3543209877  0.3604938272  0.3679012346  0.0000000000  3.7757043839  0.1006450653  0             0.3771078587 
0.7243827160  0.8935185185  0.8703703704  0.8919753086  0.8703703704  0.8750000000  0.8456790123  0.9012345679  0.8333333333  0.7185185185  0.7024691358  0.7358024691  0.7407407407  2.4691358025  1.7633897865  0.2654094696  50            0.0664970112 
0.7885802469  0.9953703704  0.9938271605  1.0000000000  1.0000000000  0.9876543210  0.9938271605  0.9891975309  0.9444444444  0.7592592593  0.7703703704  0.8123456790  0.8123456790  4.9382716049  0.4954256749  0.2654986382  100           0.0656170082 
0.8546296296  1.0000000000  0.9938271605  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9969135802  0.9629629630  0.8765432099  0.7740740741  0.8839506173  0.8839506173  7.4074074074  0.1183425824  0.2654986382  150           0.0652992153 
0.8231481481  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9629629630  0.8296296296  0.7629629630  0.8530864198  0.8469135802  9.8765432099  0.0439345952  0.2654986382  200           0.0650971746 
0.8462962963  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8666666667  0.7777777778  0.8691358025  0.8716049383  12.345679012  0.0336939382  0.2654986382  250           0.0655114508 
0.8496913580  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8740740741  0.7864197531  0.8753086420  0.8629629630  14.814814814  0.0179189387  0.2654986382  300           0.0675356579 
0.8351851852  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8567901235  0.7827160494  0.8530864198  0.8481481481  17.283950617  0.0107089789  0.2654986382  350           0.0656599522 
0.8595679012  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8765432099  0.7765432099  0.8864197531  0.8987654321  19.753086419  0.0088971498  0.2654986382  400           0.0658616257 
0.8577160494  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8802469136  0.7888888889  0.8802469136  0.8814814815  22.222222222  0.0069142124  0.2654986382  450           0.0651452637 
0.8515432099  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8814814815  0.7827160494  0.8777777778  0.8641975309  24.691358024  0.0077829652  0.3475322723  500           0.0652373838 
0.8595679012  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8962962963  0.7925925926  0.8925925926  0.8567901235  27.160493827  0.0056023189  0.3475322723  550           0.0658112860 
0.8324074074  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8419753086  0.7740740741  0.8555555556  0.8580246914  29.629629629  0.0044554619  0.3475322723  600           0.0658917665 
0.8509259259  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8765432099  0.7851851852  0.8814814815  0.8604938272  32.098765432  0.0022188094  0.3475322723  650           0.0656654167 
0.8493827160  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8814814815  0.7975308642  0.8728395062  0.8456790123  34.567901234  0.0038358891  0.3475322723  700           0.0663487673 
0.8358024691  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8432098765  0.7777777778  0.8666666667  0.8555555556  37.037037037  0.0047585129  0.3475537300  750           0.0654700804 
0.8160493827  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8555555556  0.7740740741  0.8308641975  0.8037037037  39.506172839  0.0056678735  0.3475537300  800           0.0645153999 
0.8564814815  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8790123457  0.7888888889  0.8827160494  0.8753086420  41.975308642  0.0019709048  0.3475537300  850           0.0647853518 
0.8654320988  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.9037037037  0.7888888889  0.8938271605  0.8753086420  44.444444444  0.0032246421  0.3475537300  900           0.0821966839 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3629629630  0.3487654321  0.3765432099  0.3842592593  0.3765432099  0.3734567901  0.3703703704  0.3919753086  0.3703703704  0.3691358025  0.3543209877  0.3604938272  0.3679012346  0.0000000000  3.7757043839  0.1006770134  0             0.3560476303 
0.7601851852  0.9259259259  0.8765432099  0.9259259259  0.9012345679  0.9012345679  0.8580246914  0.9243827160  0.8888888889  0.7592592593  0.7271604938  0.7691358025  0.7851851852  2.4691358025  1.7468687582  0.2654094696  50            0.0618371677 
0.8089506173  0.9969135802  0.9938271605  1.0000000000  1.0000000000  0.9891975309  0.9938271605  0.9907407407  0.9444444444  0.7876543210  0.7814814815  0.8308641975  0.8358024691  4.9382716049  0.4798831749  0.2654094696  100           0.0627109528 
0.8567901235  1.0000000000  0.9938271605  0.9984567901  0.9938271605  1.0000000000  1.0000000000  0.9969135802  0.9567901235  0.8691358025  0.7938271605  0.8851851852  0.8790123457  7.4074074074  0.1384368616  0.2654309273  150           0.0637933874 
0.8231481481  0.9984567901  0.9938271605  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9567901235  0.8234567901  0.7617283951  0.8580246914  0.8493827160  9.8765432099  0.0667578773  0.2654309273  200           0.0634888744 
0.8472222222  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9753086420  0.8580246914  0.7814814815  0.8753086420  0.8740740741  12.345679012  0.0431174749  0.2654309273  250           0.0633897543 
0.8583333333  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8790123457  0.8000000000  0.8802469136  0.8740740741  14.814814814  0.0222234663  0.2654309273  300           0.0622539854 
0.8555555556  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8703703704  0.7975308642  0.8814814815  0.8728395062  17.283950617  0.0147898883  0.2654309273  350           0.0633472729 
0.8537037037  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8740740741  0.7765432099  0.8851851852  0.8790123457  19.753086419  0.0093331658  0.2654309273  400           0.0622612095 
0.8540123457  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8703703704  0.7975308642  0.8802469136  0.8679012346  22.222222222  0.0061380641  0.2654309273  450           0.0626412058 
0.8453703704  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8753086420  0.7913580247  0.8703703704  0.8444444444  24.691358024  0.0113736037  0.2654309273  500           0.0634449673 
0.8540123457  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8777777778  0.7950617284  0.8814814815  0.8617283951  27.160493827  0.0043678562  0.2654309273  550           0.0649415541 
0.8388888889  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8506172840  0.7864197531  0.8641975309  0.8543209877  29.629629629  0.0047926976  0.3475322723  600           0.0620432472 
0.8481481481  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8753086420  0.7987654321  0.8703703704  0.8481481481  32.098765432  0.0026294725  0.3476152420  650           0.0640194178 
0.8506172840  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8814814815  0.8061728395  0.8876543210  0.8271604938  34.567901234  0.0025134618  0.3476152420  700           0.0638748503 
0.8512345679  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8740740741  0.7777777778  0.8839506173  0.8691358025  37.037037037  0.0028285815  0.3476152420  750           0.0656832123 
0.8398148148  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8679012346  0.7753086420  0.8728395062  0.8432098765  39.506172839  0.0117952536  0.3476152420  800           0.0652308607 
0.8401234568  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8666666667  0.8000000000  0.8567901235  0.8370370370  41.975308642  0.0084150029  0.3476152420  850           0.0641421223 
0.8475308642  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8777777778  0.7814814815  0.8740740741  0.8567901235  44.444444444  0.0027264455  0.3476152420  900           0.0665234709 
0.8320987654  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8753086420  0.7888888889  0.8567901235  0.8074074074  46.913580246  0.0031331750  0.3476152420  950           0.0643030167 
0.8407407407  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8814814815  0.7827160494  0.8654320988  0.8333333333  49.382716049  0.0018822356  0.3476152420  1000          0.0658134413 
0.8543209877  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8839506173  0.7864197531  0.8839506173  0.8629629630  51.851851851  0.0019268928  0.3476152420  1050          0.0641451263 
0.8425925926  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8802469136  0.7888888889  0.8617283951  0.8395061728  54.320987654  0.0009862104  0.3476152420  1100          0.0662628603 
0.8475308642  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8888888889  0.7987654321  0.8728395062  0.8296296296  56.790123456  0.0014172696  0.3476152420  1150          0.0637451410 
0.8246913580  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8740740741  0.7827160494  0.8419753086  0.8000000000  59.259259259  0.0010316935  0.3476152420  1200          0.0701600599 
0.8219135802  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8679012346  0.7765432099  0.8432098765  0.8000000000  61.728395061  0.0034438472  0.3476152420  1250          0.0652472210 
0.8382716049  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8604938272  0.7839506173  0.8666666667  0.8419753086  64.197530864  0.0011825161  0.3476152420  1300          0.0682858706 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 104, in start
    _cleanup()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 55, in _cleanup
    if p._popen.poll() is not None:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 24, in poll
    def poll(self, flag=os.WNOHANG):
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3629629630  0.3487654321  0.3765432099  0.3842592593  0.3765432099  0.3734567901  0.3703703704  0.3919753086  0.3703703704  0.3691358025  0.3543209877  0.3604938272  0.3679012346  0.0000000000  3.7757043839  0.1006770134  0             0.4059839249 
0.7601851852  0.9259259259  0.8765432099  0.9259259259  0.9012345679  0.9012345679  0.8580246914  0.9243827160  0.8888888889  0.7592592593  0.7271604938  0.7691358025  0.7851851852  2.4691358025  1.7468687582  0.2654094696  50            0.0644576836 
0.8089506173  0.9969135802  0.9938271605  1.0000000000  1.0000000000  0.9891975309  0.9938271605  0.9907407407  0.9444444444  0.7876543210  0.7814814815  0.8308641975  0.8358024691  4.9382716049  0.4798831749  0.3476366997  100           0.0682224798 
0.8567901235  1.0000000000  0.9938271605  0.9984567901  0.9938271605  1.0000000000  1.0000000000  0.9969135802  0.9567901235  0.8691358025  0.7938271605  0.8851851852  0.8790123457  7.4074074074  0.1384368616  0.3476366997  150           0.0672548819 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2337, in update
    for j in meta_train_idx:
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3629629630  0.3487654321  0.3765432099  0.3842592593  0.3765432099  0.3734567901  0.3703703704  0.3919753086  0.3703703704  0.3691358025  0.3543209877  0.3604938272  0.3679012346  0.0000000000  3.7757043839  0.1006770134  0             0.4575035572 
0.7601851852  0.9259259259  0.8765432099  0.9259259259  0.9012345679  0.9012345679  0.8580246914  0.9243827160  0.8888888889  0.7592592593  0.7271604938  0.7691358025  0.7851851852  2.4691358025  1.7468687582  0.2654156685  50            0.0666542149 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2326, in update
    for i in meta_test_idx:
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3629629630  0.3487654321  0.3765432099  0.3842592593  0.3765432099  0.3734567901  0.3703703704  0.3919753086  0.3703703704  0.3691358025  0.3543209877  0.3604938272  0.3679012346  0.0000000000  3.7757043839  0.1006770134  0             0.3571534157 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2349, in update
    meta_loss_held_out += loss_held_out * self.hparams['heldout_p']  # meta-test lossfeature-critic
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3629629630  0.3487654321  0.3765432099  0.3842592593  0.3765432099  0.3734567901  0.3703703704  0.3919753086  0.3703703704  0.3691358025  0.3543209877  0.3604938272  0.3679012346  0.0000000000  3.7757043839  0.1006770134  0             0.3601877689 
0.7601851852  0.9259259259  0.8765432099  0.9259259259  0.9012345679  0.9012345679  0.8580246914  0.9243827160  0.8888888889  0.7592592593  0.7271604938  0.7691358025  0.7851851852  2.4691358025  1.7468687582  0.2654094696  50            0.0646608257 
Process Process-331:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 251, in _bootstrap
    util._run_after_forkers()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 132, in _run_after_forkers
    func(obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/multiprocessing/_atfork.py", line 10, in wrapper
    func()
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2349, in update
    meta_loss_held_out += loss_held_out * self.hparams['heldout_p']  # meta-test lossfeature-critic
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3629629630  0.3487654321  0.3765432099  0.3842592593  0.3765432099  0.3734567901  0.3703703704  0.3919753086  0.3703703704  0.3691358025  0.3543209877  0.3604938272  0.3679012346  0.0000000000  3.7757043839  0.1006770134  0             0.3592569828 
0.7601851852  0.9259259259  0.8765432099  0.9259259259  0.9012345679  0.9012345679  0.8580246914  0.9243827160  0.8888888889  0.7592592593  0.7271604938  0.7691358025  0.7851851852  2.4691358025  1.7468687582  0.2654094696  50            0.0623496151 
0.8089506173  0.9969135802  0.9938271605  1.0000000000  1.0000000000  0.9891975309  0.9938271605  0.9907407407  0.9444444444  0.7876543210  0.7814814815  0.8308641975  0.8358024691  4.9382716049  0.4798831749  0.2654156685  100           0.0628004932 
0.8567901235  1.0000000000  0.9938271605  0.9984567901  0.9938271605  1.0000000000  1.0000000000  0.9969135802  0.9567901235  0.8691358025  0.7938271605  0.8851851852  0.8790123457  7.4074074074  0.1384368616  0.2654156685  150           0.0614406395 
0.8231481481  0.9984567901  0.9938271605  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9567901235  0.8234567901  0.7617283951  0.8580246914  0.8493827160  9.8765432099  0.0667578773  0.3475384712  200           0.0626014519 
0.8472222222  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9753086420  0.8580246914  0.7814814815  0.8753086420  0.8740740741  12.345679012  0.0431174749  0.3475384712  250           0.0607085419 
0.8583333333  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8790123457  0.8000000000  0.8802469136  0.8740740741  14.814814814  0.0222234663  0.3475384712  300           0.0625741816 
0.8555555556  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8703703704  0.7975308642  0.8814814815  0.8728395062  17.283950617  0.0147898883  0.3475384712  350           0.0618885946 
0.8537037037  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8740740741  0.7765432099  0.8851851852  0.8790123457  19.753086419  0.0093331658  0.3475384712  400           0.0617837334 
0.8540123457  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8703703704  0.7975308642  0.8802469136  0.8679012346  22.222222222  0.0061380641  0.3475561142  450           0.0643838453 
0.8453703704  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8753086420  0.7913580247  0.8703703704  0.8444444444  24.691358024  0.0113736037  0.3475561142  500           0.0628240299 
0.8540123457  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8777777778  0.7950617284  0.8814814815  0.8617283951  27.160493827  0.0043678562  0.3475561142  550           0.0659430647 
0.8388888889  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8506172840  0.7864197531  0.8641975309  0.8543209877  29.629629629  0.0047926976  0.3475561142  600           0.0640202951 
0.8481481481  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8753086420  0.7987654321  0.8703703704  0.8481481481  32.098765432  0.0026294725  0.3475561142  650           0.0626691723 
0.8506172840  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8814814815  0.8061728395  0.8876543210  0.8271604938  34.567901234  0.0025134618  0.3475561142  700           0.0634463501 
0.8512345679  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8740740741  0.7777777778  0.8839506173  0.8691358025  37.037037037  0.0028285815  0.3475561142  750           0.0639080143 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 176, in accuracy
    p = network.predict(x)   # ARMpredict
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2365, in predict
    return self.phi(self.featurizer(x))
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/networks.py", line 286, in forward
    x = self.layer3(x)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 263, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 260, in _conv_forward
    self.padding, self.dilation, self.groups)
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3629629630  0.3487654321  0.3765432099  0.3842592593  0.3765432099  0.3734567901  0.3703703704  0.3919753086  0.3703703704  0.3691358025  0.3543209877  0.3604938272  0.3679012346  0.0000000000  3.7757043839  0.1006770134  0             0.3681795597 
0.7601851852  0.9259259259  0.8765432099  0.9259259259  0.9012345679  0.9012345679  0.8580246914  0.9243827160  0.8888888889  0.7592592593  0.7271604938  0.7691358025  0.7851851852  2.4691358025  1.7468687582  0.2654094696  50            0.0651320505 
0.8089506173  0.9969135802  0.9938271605  1.0000000000  1.0000000000  0.9891975309  0.9938271605  0.9907407407  0.9444444444  0.7876543210  0.7814814815  0.8308641975  0.8358024691  4.9382716049  0.4798831749  0.2654585838  100           0.0617894983 
0.8567901235  1.0000000000  0.9938271605  0.9984567901  0.9938271605  1.0000000000  1.0000000000  0.9969135802  0.9567901235  0.8691358025  0.7938271605  0.8851851852  0.8790123457  7.4074074074  0.1384368616  0.2654585838  150           0.0627063227 
0.8231481481  0.9984567901  0.9938271605  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9567901235  0.8234567901  0.7617283951  0.8580246914  0.8493827160  9.8765432099  0.0667578773  0.2679381371  200           0.0652605534 
0.8472222222  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9753086420  0.8580246914  0.7814814815  0.8753086420  0.8740740741  12.345679012  0.0431174749  0.2679381371  250           0.0691497469 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2280, in update
    meta_train_loss_main.backward(retain_graph=True)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3586419753  0.3518518519  0.3518518519  0.3503086420  0.3518518519  0.3425925926  0.3518518519  0.3564814815  0.3456790123  0.3469135802  0.3716049383  0.3567901235  0.3592592593  0.0000000000  3.7277758121  0.1006770134  0             0.3654465675 
0.7685185185  0.8950617284  0.8703703704  0.7932098765  0.7530864198  0.8533950617  0.8827160494  0.8580246914  0.8950617284  0.7456790123  0.7728395062  0.7604938272  0.7950617284  2.4691358025  1.7390949178  0.2654094696  50            0.0632086563 
0.8277777778  0.9537037037  0.9444444444  0.9444444444  0.8703703704  0.9737654321  0.9135802469  0.9521604938  0.9259259259  0.8185185185  0.8308641975  0.8160493827  0.8456790123  4.9382716049  0.8314244676  0.3475384712  100           0.0626390409 
0.7981481481  0.9814814815  0.9814814815  0.9475308642  0.8888888889  0.9706790123  0.9197530864  0.9675925926  0.9506172840  0.7950617284  0.8024691358  0.7913580247  0.8037037037  7.4074074074  0.4800687206  0.3475384712  150           0.0630498266 
0.8537037037  0.9969135802  0.9876543210  0.9938271605  0.9506172840  0.9953703704  0.9567901235  0.9861111111  0.9506172840  0.8407407407  0.8580246914  0.8407407407  0.8753086420  9.8765432099  0.3184500048  0.3475384712  200           0.0619177246 
0.8237654321  0.9938271605  0.9876543210  0.9984567901  0.9629629630  0.9969135802  0.9567901235  0.9953703704  0.9691358025  0.8037037037  0.8370370370  0.8172839506  0.8370370370  12.345679012  0.1613924228  0.3475384712  250           0.0616584730 
0.8432098765  0.9984567901  0.9876543210  1.0000000000  0.9814814815  0.9984567901  0.9629629630  0.9969135802  0.9753086420  0.8333333333  0.8506172840  0.8259259259  0.8629629630  14.814814814  0.1299010160  0.3475384712  300           0.0612986898 
0.8410493827  0.9953703704  0.9753086420  1.0000000000  0.9876543210  1.0000000000  0.9567901235  1.0000000000  0.9814814815  0.8296296296  0.8456790123  0.8271604938  0.8617283951  17.283950617  0.0960704940  0.3475384712  350           0.0601928139 
0.8296296296  0.9984567901  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9691358025  1.0000000000  0.9814814815  0.8209876543  0.8345679012  0.8172839506  0.8456790123  19.753086419  0.0476476883  0.3475384712  400           0.0606381321 
0.8453703704  1.0000000000  0.9876543210  1.0000000000  0.9444444444  0.9984567901  0.9382716049  0.9953703704  0.9629629630  0.8296296296  0.8419753086  0.8345679012  0.8753086420  22.222222222  0.0387441301  0.3475384712  450           0.0635432959 
0.8515432099  1.0000000000  0.9876543210  1.0000000000  0.9444444444  0.9969135802  0.9444444444  0.9953703704  0.9567901235  0.8370370370  0.8506172840  0.8395061728  0.8790123457  24.691358024  0.0382721634  0.3475384712  500           0.0627588987 
0.8314814815  0.9984567901  0.9876543210  1.0000000000  0.9876543210  1.0000000000  0.9753086420  1.0000000000  0.9814814815  0.8123456790  0.8382716049  0.8209876543  0.8543209877  27.160493827  0.0319175001  0.3475384712  550           0.0620922327 
0.8320987654  0.9984567901  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9814814815  0.8148148148  0.8382716049  0.8234567901  0.8518518519  29.629629629  0.0255387518  0.3475384712  600           0.0626450205 
0.8219135802  0.9984567901  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9876543210  0.8061728395  0.8259259259  0.8111111111  0.8444444444  32.098765432  0.0126843734  0.3475384712  650           0.0723409510 
0.8682098765  1.0000000000  0.9876543210  1.0000000000  0.9814814815  1.0000000000  0.9629629630  1.0000000000  0.9753086420  0.8567901235  0.8679012346  0.8555555556  0.8925925926  34.567901234  0.0132219762  0.3475384712  700           0.0716184807 
0.8095679012  0.9984567901  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9814814815  1.0000000000  0.9938271605  0.7839506173  0.8074074074  0.8098765432  0.8370370370  37.037037037  0.0292978737  0.3475384712  750           0.0639801598 
0.8361111111  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9753086420  1.0000000000  0.9814814815  0.8222222222  0.8320987654  0.8320987654  0.8580246914  39.506172839  0.0206174996  0.3475384712  800           0.0621866989 
0.8672839506  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9814814815  0.8604938272  0.8604938272  0.8592592593  0.8888888889  41.975308642  0.0116326938  0.3475384712  850           0.0642749023 

KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3929012346  0.3858024691  0.4259259259  0.3333333333  0.3518518519  0.3611111111  0.3456790123  0.3765432099  0.4135802469  0.3666666667  0.4000000000  0.4197530864  0.3851851852  0.0000000000  4.2838053703  0.1362838745  0             0.3900427818 
0.7496913580  0.8395061728  0.8086419753  0.7438271605  0.7160493827  0.7854938272  0.8209876543  0.8117283951  0.8209876543  0.7222222222  0.7629629630  0.7518518519  0.7617283951  2.4691358025  2.5598927450  0.3010482788  50            0.0722708797 
0.7376543210  0.8796296296  0.8641975309  0.7824074074  0.7469135802  0.8225308642  0.8641975309  0.8441358025  0.8703703704  0.7283950617  0.7407407407  0.7296296296  0.7518518519  4.9382716049  1.8885966325  0.3010482788  100           0.0705757427 
0.7043209877  0.8719135802  0.9012345679  0.7685185185  0.7407407407  0.7993827160  0.8456790123  0.8225308642  0.8456790123  0.6925925926  0.6938271605  0.7098765432  0.7209876543  7.4074074074  1.7154878712  0.3010482788  150           0.0702948999 
0.7901234568  0.9197530864  0.8888888889  0.7793209877  0.7654320988  0.8379629630  0.8703703704  0.8425925926  0.8765432099  0.7814814815  0.7950617284  0.7765432099  0.8074074074  9.8765432099  1.5537002110  0.3010482788  200           0.0694800997 
0.6771604938  0.8765432099  0.8580246914  0.8256172840  0.7901234568  0.8364197531  0.8333333333  0.8441358025  0.8456790123  0.6641975309  0.6543209877  0.6925925926  0.6975308642  12.345679012  1.3887181568  0.3010482788  250           0.0727544403 
0.7919753086  0.9799382716  0.9814814815  0.9413580247  0.9074074074  0.9475308642  0.9074074074  0.9429012346  0.9567901235  0.7827160494  0.7925925926  0.7901234568  0.8024691358  14.814814814  1.2421945071  0.3010482788  300           0.0703106499 
0.8191358025  0.9845679012  0.9567901235  0.8935185185  0.8580246914  0.9212962963  0.9135802469  0.9367283951  0.9567901235  0.8111111111  0.8197530864  0.8160493827  0.8296296296  17.283950617  0.9720236933  0.3010544777  350           0.0736056757 
0.7956790123  0.9830246914  0.9814814815  0.9645061728  0.9506172840  0.9629629630  0.9197530864  0.9691358025  0.9567901235  0.7925925926  0.8000000000  0.7925925926  0.7975308642  19.753086419  0.9184761000  0.3010544777  400           0.0769607687 
0.7987654321  0.9891975309  0.9876543210  0.9830246914  0.9567901235  0.9753086420  0.9382716049  0.9691358025  0.9567901235  0.7962962963  0.8061728395  0.7950617284  0.7975308642  22.222222222  0.8554616451  0.3010544777  450           0.0742915249 
0.8172839506  0.9953703704  0.9753086420  0.9645061728  0.9320987654  0.9691358025  0.9444444444  0.9706790123  0.9629629630  0.8148148148  0.8283950617  0.8160493827  0.8098765432  24.691358024  0.7463840246  0.3010544777  500           0.0736550617 
0.7756172840  0.9722222222  0.9691358025  0.9953703704  0.9691358025  0.9876543210  0.9259259259  0.9861111111  0.9691358025  0.7716049383  0.7827160494  0.7629629630  0.7851851852  27.160493827  0.6806949955  0.3010544777  550           0.0718782949 
0.7867283951  0.9907407407  0.9876543210  0.9969135802  0.9876543210  0.9876543210  0.9320987654  0.9845679012  0.9691358025  0.7827160494  0.8000000000  0.7765432099  0.7876543210  29.629629629  0.6367685467  0.3010544777  600           0.0778221083 
0.7922839506  0.9938271605  0.9876543210  0.9984567901  0.9753086420  0.9907407407  0.9259259259  0.9922839506  0.9753086420  0.7901234568  0.7987654321  0.7827160494  0.7975308642  32.098765432  0.6375306123  0.3010544777  650           0.0744540024 
0.7722222222  0.9768518519  0.9629629630  1.0000000000  0.9814814815  0.9907407407  0.9259259259  0.9938271605  0.9629629630  0.7641975309  0.7827160494  0.7641975309  0.7777777778  34.567901234  0.5564187145  0.3010544777  700           0.0713421392 
0.7904320988  0.9953703704  0.9753086420  0.9984567901  0.9876543210  0.9891975309  0.9320987654  0.9953703704  0.9567901235  0.7827160494  0.8024691358  0.7851851852  0.7913580247  37.037037037  0.5545576096  0.3010544777  750           0.0716693592 
0.8024691358  1.0000000000  0.9876543210  0.9922839506  0.9753086420  0.9907407407  0.9444444444  0.9938271605  0.9691358025  0.8000000000  0.8098765432  0.7975308642  0.8024691358  39.506172839  0.5263436514  0.3010544777  800           0.0715966415 
0.8018518519  1.0000000000  0.9814814815  0.9953703704  0.9876543210  0.9938271605  0.9382716049  0.9938271605  0.9629629630  0.8012345679  0.8123456790  0.7950617284  0.7987654321  41.975308642  0.4955841202  0.3010544777  850           0.0760315275 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3580246914  0.3410493827  0.3271604938  0.4104938272  0.3518518519  0.4027777778  0.3888888889  0.4043209877  0.3456790123  0.3543209877  0.3333333333  0.3518518519  0.3925925926  0.0000000000  4.2582302094  0.1362838745  0             0.3673870564 
0.7496913580  0.8564814815  0.8148148148  0.8580246914  0.8148148148  0.8425925926  0.7901234568  0.8395061728  0.7962962963  0.7716049383  0.7135802469  0.7493827160  0.7641975309  2.4691358025  2.5950414920  0.3010482788  50            0.0707210112 
0.7929012346  0.8703703704  0.8333333333  0.8595679012  0.8271604938  0.8595679012  0.8271604938  0.8796296296  0.8580246914  0.8444444444  0.7358024691  0.7851851852  0.8061728395  4.9382716049  1.8391623187  0.3010482788  100           0.0726245594 
0.8188271605  0.9768518519  0.9444444444  0.9675925926  0.9382716049  0.9521604938  0.9320987654  0.9737654321  0.9320987654  0.8370370370  0.7518518519  0.8234567901  0.8629629630  7.4074074074  1.5279689312  0.3010482788  150           0.0753320932 
0.7774691358  0.9429012346  0.9012345679  0.9475308642  0.8827160494  0.9274691358  0.9074074074  0.9506172840  0.9135802469  0.8246913580  0.7049382716  0.7765432099  0.8037037037  9.8765432099  1.3059186983  0.3831710815  200           0.0747613287 
0.8493827160  0.9922839506  0.9938271605  0.9969135802  0.9814814815  0.9814814815  1.0000000000  0.9922839506  0.9629629630  0.8493827160  0.7938271605  0.8728395062  0.8814814815  12.345679012  0.9667946494  0.3831710815  250           0.0730162239 
0.8506172840  0.9953703704  0.9814814815  0.9984567901  0.9876543210  0.9783950617  0.9876543210  0.9922839506  0.9567901235  0.8580246914  0.7703703704  0.8753086420  0.8987654321  14.814814814  0.7706454015  0.3831710815  300           0.0707761621 
0.8212962963  0.9861111111  0.9938271605  1.0000000000  1.0000000000  0.9861111111  0.9938271605  0.9922839506  0.9567901235  0.8148148148  0.7469135802  0.8469135802  0.8765432099  17.283950617  0.6287836808  0.3831710815  350           0.0710966492 
0.8469135802  0.9953703704  0.9938271605  1.0000000000  0.9938271605  0.9984567901  0.9938271605  0.9938271605  0.9567901235  0.8370370370  0.7777777778  0.8790123457  0.8938271605  19.753086419  0.6041003609  0.3831710815  400           0.0704561186 
0.8416666667  0.9907407407  0.9938271605  1.0000000000  1.0000000000  0.9953703704  0.9938271605  0.9938271605  0.9691358025  0.8320987654  0.7703703704  0.8703703704  0.8938271605  22.222222222  0.5418576592  0.3831710815  450           0.0712457418 
0.8444444444  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9984567901  1.0000000000  0.9969135802  0.9691358025  0.8370370370  0.7691358025  0.8802469136  0.8913580247  24.691358024  0.5093142474  0.3831710815  500           0.0728299284 
0.8395061728  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9938271605  0.9969135802  0.9629629630  0.8432098765  0.7493827160  0.8728395062  0.8925925926  27.160493827  0.4447830731  0.3831710815  550           0.0726641750 
0.8503086420  0.9984567901  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9969135802  0.9753086420  0.8506172840  0.7654320988  0.8851851852  0.9000000000  29.629629629  0.4402065143  0.3831710815  600           0.0724628782 
0.8490740741  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9984567901  1.0000000000  1.0000000000  0.9814814815  0.8419753086  0.7827160494  0.8888888889  0.8827160494  32.098765432  0.4322500068  0.3831710815  650           0.0731730127 
0.8407407407  0.9984567901  1.0000000000  1.0000000000  1.0000000000  0.9953703704  0.9938271605  0.9969135802  0.9876543210  0.8432098765  0.7888888889  0.8716049383  0.8592592593  34.567901234  0.3904083222  0.3831710815  700           0.6367996168 
0.8506172840  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.9984567901  1.0000000000  1.0000000000  0.9876543210  0.8407407407  0.7765432099  0.8901234568  0.8950617284  37.037037037  0.3702793694  0.3831710815  750           0.0775246763 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3780864198  0.3549382716  0.3641975309  0.3981481481  0.3703703704  0.3858024691  0.3518518519  0.4027777778  0.4135802469  0.3814814815  0.3629629630  0.3740740741  0.3938271605  0.0000000000  4.0252451897  0.1362838745  0             0.5213212967 
0.7496913580  0.8750000000  0.8395061728  0.8734567901  0.8148148148  0.8595679012  0.8024691358  0.8719135802  0.8271604938  0.7753086420  0.7135802469  0.7444444444  0.7654320988  2.4691358025  2.2974087358  0.3010544777  50            0.0717283916 
0.8033950617  0.9675925926  0.9320987654  0.9475308642  0.9135802469  0.9305555556  0.9197530864  0.9660493827  0.9259259259  0.8185185185  0.7395061728  0.8123456790  0.8432098765  4.9382716049  1.5809627223  0.3010544777  100           0.0728685284 
0.8030864198  0.9907407407  1.0000000000  0.9969135802  0.9814814815  0.9737654321  0.9938271605  0.9814814815  0.9444444444  0.7827160494  0.7802469136  0.8111111111  0.8382716049  7.4074074074  1.1780039454  0.3010544777  150           0.0722677803 
0.8376543210  0.9969135802  0.9938271605  0.9953703704  0.9876543210  0.9814814815  0.9938271605  0.9938271605  0.9567901235  0.8493827160  0.7543209877  0.8506172840  0.8962962963  9.8765432099  0.8536181760  0.3010544777  200           0.0717143679 
0.8500000000  0.9953703704  1.0000000000  0.9984567901  0.9938271605  0.9830246914  1.0000000000  0.9922839506  0.9691358025  0.8432098765  0.7975308642  0.8814814815  0.8777777778  12.345679012  0.6696751165  0.3010544777  250           0.0717584515 
0.8574074074  0.9984567901  0.9938271605  0.9969135802  0.9876543210  0.9830246914  1.0000000000  0.9938271605  0.9691358025  0.8666666667  0.7703703704  0.8839506173  0.9086419753  14.814814814  0.5537734312  0.3831772804  300           0.0718864298 
0.8314814815  0.9953703704  0.9938271605  1.0000000000  1.0000000000  0.9922839506  0.9938271605  0.9953703704  0.9567901235  0.8370370370  0.7481481481  0.8555555556  0.8851851852  17.283950617  0.4586117369  0.3831772804  350           0.0721007395 
0.8376543210  0.9953703704  0.9938271605  1.0000000000  0.9938271605  0.9953703704  1.0000000000  0.9984567901  0.9691358025  0.8296296296  0.7703703704  0.8679012346  0.8827160494  19.753086419  0.4325751996  0.3831772804  400           0.0720225477 
0.8484567901  0.9984567901  1.0000000000  1.0000000000  0.9938271605  0.9969135802  1.0000000000  0.9984567901  0.9691358025  0.8419753086  0.7802469136  0.8802469136  0.8913580247  22.222222222  0.3945319536  0.3831772804  450           0.0729456425 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3697530864  0.3688271605  0.3703703704  0.3441358025  0.3641975309  0.3549382716  0.3580246914  0.3796296296  0.3765432099  0.3641975309  0.3777777778  0.3716049383  0.3654320988  0.0000000000  3.9928092957  0.1363158226  0             0.3844993114 
0.7453703704  0.8611111111  0.8395061728  0.7608024691  0.7345679012  0.8040123457  0.8456790123  0.8271604938  0.8456790123  0.7209876543  0.7481481481  0.7456790123  0.7666666667  2.4691358025  2.3296666026  0.3010482788  50            0.0755344296 
0.7339506173  0.8950617284  0.8765432099  0.7978395062  0.7654320988  0.8410493827  0.8580246914  0.8503086420  0.8827160494  0.7259259259  0.7308641975  0.7296296296  0.7493827160  4.9382716049  1.6710466361  0.3010482788  100           0.0763098335 
0.7037037037  0.9151234568  0.9012345679  0.8055555556  0.7839506173  0.8302469136  0.8456790123  0.8456790123  0.8827160494  0.6913580247  0.6938271605  0.7123456790  0.7172839506  7.4074074074  1.4274956775  0.3010544777  150           0.0753483677 
0.7376543210  0.9783950617  0.9567901235  0.8827160494  0.8456790123  0.9043209877  0.8888888889  0.8966049383  0.8950617284  0.7197530864  0.7308641975  0.7469135802  0.7530864198  9.8765432099  1.2211565113  0.3010544777  200           0.0751625061 
0.7691358025  0.9552469136  0.9259259259  0.9706790123  0.9444444444  0.9629629630  0.9197530864  0.9583333333  0.9444444444  0.7604938272  0.7654320988  0.7716049383  0.7790123457  12.345679012  0.9980688190  0.3010544777  250           0.0777874374 
0.7601851852  0.9552469136  0.9567901235  0.9845679012  0.9814814815  0.9660493827  0.9320987654  0.9614197531  0.9259259259  0.7456790123  0.7617283951  0.7592592593  0.7740740741  14.814814814  0.8264249027  0.3010544777  300           0.0774633312 
0.7984567901  0.9922839506  0.9876543210  0.9645061728  0.9382716049  0.9722222222  0.9444444444  0.9753086420  0.9506172840  0.7950617284  0.8037037037  0.7975308642  0.7975308642  17.283950617  0.6658066809  0.3010544777  350           0.0760791540 
0.7882716049  0.9799382716  0.9691358025  0.9861111111  0.9567901235  0.9907407407  0.9382716049  0.9922839506  0.9506172840  0.7765432099  0.7962962963  0.7876543210  0.7925925926  19.753086419  0.6121173751  0.3010544777  400           0.0764314079 
0.8141975309  0.9953703704  0.9938271605  0.9861111111  0.9506172840  0.9861111111  0.9444444444  0.9876543210  0.9629629630  0.7987654321  0.8222222222  0.8234567901  0.8123456790  22.222222222  0.5423478740  0.3010544777  450           0.0786165190 
0.8185185185  1.0000000000  0.9814814815  0.9814814815  0.9444444444  0.9861111111  0.9444444444  0.9783950617  0.9567901235  0.8086419753  0.8209876543  0.8246913580  0.8197530864  24.691358024  0.5013874751  0.3010544777  500           0.0826292229 
0.7922839506  0.9938271605  0.9814814815  1.0000000000  0.9753086420  0.9938271605  0.9382716049  0.9953703704  0.9629629630  0.7839506173  0.7938271605  0.7925925926  0.7987654321  27.160493827  0.4600792682  0.3010544777  550           0.0768875360 
0.7938271605  1.0000000000  0.9938271605  1.0000000000  0.9876543210  0.9984567901  0.9444444444  0.9969135802  0.9629629630  0.7839506173  0.8037037037  0.7864197531  0.8012345679  29.629629629  0.4416395825  0.3010697365  600           0.0834685612 
0.7978395062  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.9969135802  0.9506172840  0.9984567901  0.9691358025  0.7901234568  0.8037037037  0.7938271605  0.8037037037  32.098765432  0.4139550939  0.3010697365  650           0.0785877514 
0.8003086420  1.0000000000  0.9876543210  1.0000000000  0.9938271605  0.9984567901  0.9506172840  0.9969135802  0.9629629630  0.7913580247  0.8098765432  0.7950617284  0.8049382716  34.567901234  0.3617313322  0.3010697365  700           0.0789394522 
0.7657407407  0.9907407407  0.9444444444  1.0000000000  0.9753086420  0.9922839506  0.9444444444  0.9938271605  0.9567901235  0.7555555556  0.7716049383  0.7567901235  0.7790123457  37.037037037  0.3593065175  0.3010697365  750           0.0855702972 
0.8058641975  1.0000000000  0.9938271605  0.9969135802  0.9691358025  0.9969135802  0.9506172840  0.9969135802  0.9691358025  0.8000000000  0.8086419753  0.8061728395  0.8086419753  39.506172839  0.3673307222  0.3010873795  800           0.0795708227 
0.8012345679  1.0000000000  0.9938271605  0.9969135802  0.9938271605  1.0000000000  0.9567901235  0.9984567901  0.9691358025  0.7925925926  0.8098765432  0.8000000000  0.8024691358  41.975308642  0.3327607870  0.3010935783  850           0.0865615225 

KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3629629630  0.3487654321  0.3765432099  0.3842592593  0.3765432099  0.3734567901  0.3703703704  0.3919753086  0.3703703704  0.3691358025  0.3543209877  0.3604938272  0.3679012346  0.0000000000  3.7757043839  0.1006450653  0             0.3754043579 
0.7305555556  0.8827160494  0.8518518519  0.8950617284  0.8580246914  0.8688271605  0.8395061728  0.8935185185  0.8518518519  0.7432098765  0.7024691358  0.7283950617  0.7481481481  2.4691358025  1.7720579028  0.2654156685  50            0.0587966156 
0.7827160494  0.9891975309  0.9938271605  1.0000000000  1.0000000000  0.9845679012  0.9938271605  0.9876543210  0.9382716049  0.7444444444  0.7728395062  0.8000000000  0.8135802469  4.9382716049  0.5046393618  0.2654156685  100           0.0613164568 
0.8388888889  1.0000000000  0.9938271605  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9969135802  0.9567901235  0.8530864198  0.7679012346  0.8530864198  0.8814814815  7.4074074074  0.1373130610  0.2654156685  150           0.0626527262 
0.8049382716  0.9984567901  0.9938271605  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9629629630  0.7962962963  0.7530864198  0.8308641975  0.8395061728  9.8765432099  0.0589902468  0.2654156685  200           0.0655121231 
0.8574074074  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9691358025  0.8753086420  0.7851851852  0.8864197531  0.8827160494  12.345679012  0.0431555521  0.3475384712  250           0.0601765585 
0.8524691358  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9753086420  0.8641975309  0.8012345679  0.8740740741  0.8703703704  14.814814814  0.0196002762  0.3475384712  300           0.0582195807 
0.8524691358  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8691358025  0.7962962963  0.8777777778  0.8666666667  17.283950617  0.0125830314  0.3475384712  350           0.0608947182 
0.8509259259  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8753086420  0.8000000000  0.8790123457  0.8493827160  19.753086419  0.0083914579  0.3475661278  400           0.0598704052 
0.8552469136  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8790123457  0.7901234568  0.8839506173  0.8679012346  22.222222222  0.0080581978  0.3475661278  450           0.0644173098 
0.7984567901  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9691358025  0.8049382716  0.7234567901  0.8209876543  0.8444444444  24.691358024  0.0161683135  0.3475661278  500           0.0620908356 
0.8518518519  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8679012346  0.7876543210  0.8814814815  0.8703703704  27.160493827  0.0056777415  0.3475661278  550           0.0588800716 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3629629630  0.3487654321  0.3765432099  0.3842592593  0.3765432099  0.3734567901  0.3703703704  0.3919753086  0.3703703704  0.3691358025  0.3543209877  0.3604938272  0.3679012346  0.0000000000  3.7757043839  0.1006450653  0             0.3547725677 
0.7305555556  0.8827160494  0.8518518519  0.8950617284  0.8580246914  0.8688271605  0.8395061728  0.8935185185  0.8518518519  0.7432098765  0.7024691358  0.7283950617  0.7481481481  2.4691358025  1.7720579028  0.2654094696  50            0.0604545879 
0.7827160494  0.9891975309  0.9938271605  1.0000000000  1.0000000000  0.9845679012  0.9938271605  0.9876543210  0.9382716049  0.7444444444  0.7728395062  0.8000000000  0.8135802469  4.9382716049  0.5046393618  0.2654771805  100           0.0580929899 
0.8388888889  1.0000000000  0.9938271605  0.9984567901  0.9938271605  1.0000000000  0.9938271605  0.9969135802  0.9567901235  0.8530864198  0.7679012346  0.8530864198  0.8814814815  7.4074074074  0.1373130610  0.3475999832  150           0.0595910883 
0.8049382716  0.9984567901  0.9938271605  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9629629630  0.7962962963  0.7530864198  0.8308641975  0.8395061728  9.8765432099  0.0589902468  0.3475999832  200           0.0607463264 
0.8574074074  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9691358025  0.8753086420  0.7851851852  0.8864197531  0.8827160494  12.345679012  0.0431555521  0.3475999832  250           0.0593965626 
0.8524691358  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9753086420  0.8641975309  0.8012345679  0.8740740741  0.8703703704  14.814814814  0.0196002762  0.3475999832  300           0.0598769522 
0.8524691358  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8691358025  0.7962962963  0.8777777778  0.8666666667  17.283950617  0.0125830314  0.3475999832  350           0.0601625776 
0.8509259259  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8753086420  0.8000000000  0.8790123457  0.8493827160  19.753086419  0.0083914579  0.3475999832  400           0.0579942369 
0.8552469136  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8790123457  0.7901234568  0.8839506173  0.8679012346  22.222222222  0.0080581978  0.3475999832  450           0.0613424301 
0.7984567901  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9691358025  0.8049382716  0.7234567901  0.8209876543  0.8444444444  24.691358024  0.0161683135  0.3475999832  500           0.0587227392 
0.8518518519  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8679012346  0.7876543210  0.8814814815  0.8703703704  27.160493827  0.0056777415  0.3475999832  550           0.0611632633 
0.8555555556  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8790123457  0.7851851852  0.8839506173  0.8740740741  29.629629629  0.0054706105  0.3475999832  600           0.0594830704 
0.8570987654  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8728395062  0.7876543210  0.8839506173  0.8839506173  32.098765432  0.0037649502  0.3475999832  650           0.0610284472 
0.8524691358  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8839506173  0.7950617284  0.8790123457  0.8518518519  34.567901234  0.0033704838  0.3475999832  700           0.0582784414 
0.8367283951  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8629629630  0.7839506173  0.8728395062  0.8271604938  37.037037037  0.0043732779  0.3475999832  750           0.0601007652 
0.8503086420  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8839506173  0.7864197531  0.8839506173  0.8469135802  39.506172839  0.0035548325  0.3475999832  800           0.0646014929 
0.8580246914  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8888888889  0.7888888889  0.8888888889  0.8654320988  41.975308642  0.0022194333  0.3475999832  850           0.0599368429 
0.8645061728  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8950617284  0.7913580247  0.8987654321  0.8728395062  44.444444444  0.0023890172  0.3475999832  900           0.0613472366 
0.8447530864  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8703703704  0.7839506173  0.8703703704  0.8543209877  46.913580246  0.0016978785  0.3475999832  950           0.0641116285 
0.8496913580  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.8827160494  0.7901234568  0.8839506173  0.8419753086  49.382716049  0.0017868915  0.3475999832  1000          0.0591104317 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3317374579  0.3328313253  0.3313253012  0.3317307692  0.3293556086  0.3328313253  0.3313253012  0.3323308271  0.3293413174  0.3323308271  0.3293413174  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4467766117  0.4431137725  0.3325301205  0.3333333333  0.0000000000  8.0671997070  0.2074122429  0             0.4360723495 
0.7249483473  0.9367469880  0.9277108434  0.7355769231  0.6515513126  0.8629518072  0.8253012048  0.9609022556  0.9281437126  0.8992481203  0.8862275449  0.7590361446  0.7710843373  0.8102409639  0.8554216867  0.7831325301  0.7228915663  0.8605697151  0.8622754491  0.7566265060  0.7560386473  2.4096385542  4.7670046091  0.3946189880  50            0.1227522898 
0.7585919598  0.9759036145  0.9518072289  0.7331730769  0.7136038186  0.8870481928  0.8253012048  0.9684210526  0.9341317365  0.9939849624  0.9760479042  0.9307228916  0.9397590361  0.7620481928  0.8192771084  0.9216867470  0.9036144578  0.9580209895  0.9401197605  0.7373493976  0.8502415459  4.8192771084  1.9616367292  0.3946189880  100           0.1150665092 
0.7585575236  0.9698795181  0.9518072289  0.7331730769  0.7303102625  0.9051204819  0.8493975904  0.9654135338  0.9341317365  0.9924812030  0.9700598802  0.9277108434  0.9397590361  0.7801204819  0.8313253012  0.9111445783  0.8855421687  0.9670164918  0.9401197605  0.7108433735  0.8599033816  7.2289156627  1.5136026621  0.3946189880  150           0.1242057228 
0.7561737920  0.9698795181  0.9518072289  0.7307692308  0.7207637232  0.8990963855  0.8373493976  0.9654135338  0.9341317365  0.9924812030  0.9700598802  0.9277108434  0.9397590361  0.7801204819  0.8313253012  0.9156626506  0.8915662651  0.9670164918  0.9401197605  0.7108433735  0.8623188406  9.6385542169  1.4915693212  0.5834388733  200           0.1109363794 
0.7567661549  0.9668674699  0.9518072289  0.7307692308  0.7255369928  0.9081325301  0.8493975904  0.9624060150  0.9341317365  0.9924812030  0.9700598802  0.9277108434  0.9397590361  0.7861445783  0.8313253012  0.9096385542  0.8795180723  0.9715142429  0.9461077844  0.7060240964  0.8647342995  12.048192771  1.4774513865  0.5834388733  250           0.1114764309 
0.7543419724  0.9728915663  0.9518072289  0.7331730769  0.7255369928  0.8960843373  0.8373493976  0.9774436090  0.9580838323  0.9969924812  0.9760479042  0.9307228916  0.9397590361  0.7680722892  0.8313253012  0.9246987952  0.9156626506  0.9715142429  0.9461077844  0.7156626506  0.8429951691  14.457831325  1.4285216403  0.5834388733  300           0.1186867905 
0.7579723668  0.9728915663  0.9518072289  0.7331730769  0.7231503580  0.9021084337  0.8493975904  0.9744360902  0.9580838323  0.9939849624  0.9760479042  0.9307228916  0.9397590361  0.7801204819  0.8313253012  0.9201807229  0.9096385542  0.9715142429  0.9461077844  0.7156626506  0.8599033816  16.867469879  1.4926013100  0.5834388733  350           0.1111810255 
0.7561463605  0.9713855422  0.9457831325  0.7331730769  0.7279236277  0.9006024096  0.8433734940  0.9774436090  0.9580838323  0.9969924812  0.9760479042  0.9307228916  0.9397590361  0.7771084337  0.8313253012  0.9246987952  0.9156626506  0.9745127436  0.9461077844  0.7156626506  0.8478260870  19.277108433  1.4487268507  0.5834388733  400           0.1119194698 
0.7567546530  0.9653614458  0.9457831325  0.7307692308  0.7303102625  0.9171686747  0.8614457831  0.9624060150  0.9341317365  0.9939849624  0.9760479042  0.9246987952  0.9397590361  0.7996987952  0.8373493976  0.9096385542  0.8795180723  0.9730134933  0.9520958084  0.7012048193  0.8647342995  21.686746988  1.4135271859  0.5834388733  450           0.1116140032 
0.7549530431  0.9713855422  0.9457831325  0.7331730769  0.7231503580  0.9006024096  0.8433734940  0.9744360902  0.9580838323  0.9939849624  0.9760479042  0.9307228916  0.9397590361  0.7771084337  0.8313253012  0.9216867470  0.9156626506  0.9715142429  0.9461077844  0.7156626506  0.8478260870  24.096385542  1.4471870303  0.5834388733  500           0.1189669418 
0.7537525197  0.9713855422  0.9457831325  0.7331730769  0.7207637232  0.9006024096  0.8433734940  0.9744360902  0.9580838323  0.9939849624  0.9760479042  0.9307228916  0.9397590361  0.7771084337  0.8313253012  0.9216867470  0.9156626506  0.9715142429  0.9461077844  0.7156626506  0.8454106280  26.506024096  1.4583171034  0.5834388733  550           0.1166819859 
0.7543563844  0.9713855422  0.9457831325  0.7331730769  0.7207637232  0.9126506024  0.8554216867  0.9744360902  0.9580838323  0.9939849624  0.9760479042  0.9307228916  0.9397590361  0.7771084337  0.8313253012  0.9216867470  0.9156626506  0.9715142429  0.9461077844  0.7156626506  0.8478260870  28.915662650  1.4090067673  0.5855388641  600           0.1106904411 
0.7549574014  0.9683734940  0.9457831325  0.7307692308  0.7231503580  0.9096385542  0.8554216867  0.9684210526  0.9461077844  0.9939849624  0.9760479042  0.9277108434  0.9397590361  0.7861445783  0.8313253012  0.9186746988  0.9036144578  0.9730134933  0.9520958084  0.7132530120  0.8526570048  31.325301204  1.4062898350  0.5855388641  650           0.1121994781 
0.7549617597  0.9683734940  0.9457831325  0.7283653846  0.7231503580  0.9096385542  0.8554216867  0.9669172932  0.9520958084  0.9939849624  0.9760479042  0.9277108434  0.9397590361  0.7861445783  0.8313253012  0.9171686747  0.9096385542  0.9730134933  0.9520958084  0.7108433735  0.8574879227  33.734939759  1.4169423103  0.5855388641  700           0.1112174606 
0.7519294166  0.9713855422  0.9457831325  0.7307692308  0.7255369928  0.9021084337  0.8493975904  0.9744360902  0.9580838323  0.9939849624  0.9760479042  0.9307228916  0.9397590361  0.7771084337  0.8313253012  0.9216867470  0.9156626506  0.9715142429  0.9461077844  0.7156626506  0.8357487923  36.144578313  1.4154003549  0.5855388641  750           0.1117874146 
0.7549241566  0.9728915663  0.9518072289  0.7355769231  0.7303102625  0.9066265060  0.8433734940  0.9774436090  0.9580838323  0.9969924812  0.9760479042  0.9307228916  0.9397590361  0.7635542169  0.8253012048  0.9231927711  0.9216867470  0.9745127436  0.9341317365  0.7228915663  0.8309178744  38.554216867  1.3947072411  0.5855388641  800           0.1113418484 
0.7513399639  0.9728915663  0.9518072289  0.7307692308  0.7207637232  0.9126506024  0.8554216867  0.9774436090  0.9580838323  0.9969924812  0.9760479042  0.9307228916  0.9397590361  0.7801204819  0.8313253012  0.9292168675  0.9216867470  0.9745127436  0.9461077844  0.7156626506  0.8381642512  40.963855421  1.3729673958  0.5855388641  850           0.1118806744 
0.7531515581  0.9683734940  0.9457831325  0.7307692308  0.7207637232  0.9156626506  0.8554216867  0.9744360902  0.9580838323  0.9939849624  0.9760479042  0.9277108434  0.9397590361  0.7801204819  0.8313253012  0.9262048193  0.9216867470  0.9730134933  0.9520958084  0.7156626506  0.8454106280  43.373493975  1.3281194723  0.5855388641  900           0.1111630297 
0.7525433281  0.9728915663  0.9518072289  0.7307692308  0.7207637232  0.9096385542  0.8433734940  0.9774436090  0.9580838323  0.9969924812  0.9760479042  0.9337349398  0.9397590361  0.7756024096  0.8253012048  0.9292168675  0.9216867470  0.9745127436  0.9461077844  0.7228915663  0.8357487923  45.783132530  1.3543339539  0.5855388641  950           0.1198366070 
0.7519409185  0.9728915663  0.9518072289  0.7307692308  0.7207637232  0.9096385542  0.8433734940  0.9744360902  0.9580838323  0.9939849624  0.9760479042  0.9337349398  0.9397590361  0.7801204819  0.8313253012  0.9292168675  0.9216867470  0.9730134933  0.9520958084  0.7204819277  0.8357487923  48.192771084  1.4040803587  0.5855388641  1000          0.1125665331 
0.7531544683  0.9683734940  0.9457831325  0.7307692308  0.7207637232  0.9186746988  0.8554216867  0.9744360902  0.9580838323  0.9939849624  0.9760479042  0.9337349398  0.9397590361  0.7861445783  0.8313253012  0.9277108434  0.9156626506  0.9730134933  0.9520958084  0.7108433735  0.8502415459  50.602409638  1.3679922795  0.5855388641  1050          0.1128954363 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3629629630  0.3487654321  0.3765432099  0.3842592593  0.3765432099  0.3734567901  0.3703703704  0.3919753086  0.3703703704  0.3691358025  0.3543209877  0.3604938272  0.3679012346  0.0000000000  3.7757043839  0.1006770134  0             0.3851034641 
0.7601851852  0.9259259259  0.8765432099  0.9259259259  0.9012345679  0.9012345679  0.8580246914  0.9243827160  0.8888888889  0.7592592593  0.7271604938  0.7691358025  0.7851851852  2.4691358025  1.7468687582  0.2654094696  50            0.0704738331 
0.8089506173  0.9969135802  0.9938271605  1.0000000000  1.0000000000  0.9891975309  0.9938271605  0.9907407407  0.9444444444  0.7876543210  0.7814814815  0.8308641975  0.8358024691  4.9382716049  0.4798831749  0.3475322723  100           0.0676093054 
0.8567901235  1.0000000000  0.9938271605  0.9984567901  0.9938271605  1.0000000000  1.0000000000  0.9969135802  0.9567901235  0.8691358025  0.7938271605  0.8851851852  0.8790123457  7.4074074074  0.1384368616  0.3475322723  150           0.0677402496 
0.8231481481  0.9984567901  0.9938271605  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9567901235  0.8234567901  0.7617283951  0.8580246914  0.8493827160  9.8765432099  0.0667578773  0.3475322723  200           0.0677416277 
0.8472222222  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9753086420  0.8580246914  0.7814814815  0.8753086420  0.8740740741  12.345679012  0.0431174749  0.3475322723  250           0.0675920868 
0.8583333333  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8790123457  0.8000000000  0.8802469136  0.8740740741  14.814814814  0.0222234663  0.3475322723  300           0.0653330135 
0.8555555556  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8703703704  0.7975308642  0.8814814815  0.8728395062  17.283950617  0.0147898883  0.3475322723  350           0.0637759066 
0.8537037037  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8740740741  0.7765432099  0.8851851852  0.8790123457  19.753086419  0.0093331658  0.3475322723  400           0.0722253752 
0.8540123457  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8703703704  0.7975308642  0.8802469136  0.8679012346  22.222222222  0.0061380641  0.3475322723  450           0.0640193701 
0.8453703704  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8753086420  0.7913580247  0.8703703704  0.8444444444  24.691358024  0.0113736037  0.3475322723  500           0.0711192989 
0.8540123457  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8777777778  0.7950617284  0.8814814815  0.8617283951  27.160493827  0.0043678562  0.3475322723  550           0.0665783310 
0.8388888889  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8506172840  0.7864197531  0.8641975309  0.8543209877  29.629629629  0.0047926976  0.3475322723  600           0.0746160793 
0.8481481481  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8753086420  0.7987654321  0.8703703704  0.8481481481  32.098765432  0.0026294725  0.3475322723  650           0.0730270910 
0.8506172840  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8814814815  0.8061728395  0.8876543210  0.8271604938  34.567901234  0.0025134618  0.3475322723  700           0.0705805063 
0.8512345679  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8740740741  0.7777777778  0.8839506173  0.8691358025  37.037037037  0.0028285815  0.3475322723  750           0.0660425568 
0.8398148148  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8679012346  0.7753086420  0.8728395062  0.8432098765  39.506172839  0.0117952536  0.3475322723  800           0.0695109653 
0.8401234568  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8666666667  0.8000000000  0.8567901235  0.8370370370  41.975308642  0.0084150029  0.3475322723  850           0.0671667719 
0.8475308642  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8777777778  0.7814814815  0.8740740741  0.8567901235  44.444444444  0.0027264455  0.3475322723  900           0.0691702890 
0.8320987654  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8753086420  0.7888888889  0.8567901235  0.8074074074  46.913580246  0.0031331750  0.3475322723  950           0.0720727491 
0.8407407407  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8814814815  0.7827160494  0.8654320988  0.8333333333  49.382716049  0.0018822356  0.3475322723  1000          0.0668571758 
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3629629630  0.3487654321  0.3765432099  0.3842592593  0.3765432099  0.3734567901  0.3703703704  0.3919753086  0.3703703704  0.3691358025  0.3543209877  0.3604938272  0.3679012346  0.0000000000  3.7757043839  0.1006770134  0             0.3775207996 
0.7601851852  0.9259259259  0.8765432099  0.9259259259  0.9012345679  0.9012345679  0.8580246914  0.9243827160  0.8888888889  0.7592592593  0.7271604938  0.7691358025  0.7851851852  2.4691358025  1.7468687582  0.2654094696  50            0.0659737492 
0.8089506173  0.9969135802  0.9938271605  1.0000000000  1.0000000000  0.9891975309  0.9938271605  0.9907407407  0.9444444444  0.7876543210  0.7814814815  0.8308641975  0.8358024691  4.9382716049  0.4798831749  0.2654371262  100           0.0656059265 
0.8567901235  1.0000000000  0.9938271605  0.9984567901  0.9938271605  1.0000000000  1.0000000000  0.9969135802  0.9567901235  0.8691358025  0.7938271605  0.8851851852  0.8790123457  7.4074074074  0.1384368616  0.2654371262  150           0.0654387617 
0.8231481481  0.9984567901  0.9938271605  1.0000000000  0.9938271605  1.0000000000  1.0000000000  0.9984567901  0.9567901235  0.8234567901  0.7617283951  0.8580246914  0.8493827160  9.8765432099  0.0667578773  0.2654371262  200           0.0644485998 
0.8472222222  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9753086420  0.8580246914  0.7814814815  0.8753086420  0.8740740741  12.345679012  0.0431174749  0.2654371262  250           0.0643330956 
0.8583333333  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8790123457  0.8000000000  0.8802469136  0.8740740741  14.814814814  0.0222234663  0.3475599289  300           0.0645010757 
0.8555555556  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8703703704  0.7975308642  0.8814814815  0.8728395062  17.283950617  0.0147898883  0.3475599289  350           0.0664832592 
0.8537037037  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.8740740741  0.7765432099  0.8851851852  0.8790123457  19.753086419  0.0093331658  0.3475599289  400           0.0665063095 
0.8540123457  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8703703704  0.7975308642  0.8802469136  0.8679012346  22.222222222  0.0061380641  0.3475599289  450           0.0653369522 
0.8493827160  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8691358025  0.7950617284  0.8728395062  0.8604938272  24.691358024  0.0078640108  0.3475599289  500           0.0658662844 
0.8611111111  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8802469136  0.7950617284  0.8901234568  0.8790123457  27.160493827  0.0047060632  0.3475599289  550           0.0721410751 
0.8555555556  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8827160494  0.8037037037  0.8814814815  0.8543209877  29.629629629  0.0066394655  0.3475599289  600           0.0661106825 
0.8595679012  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8839506173  0.8061728395  0.8864197531  0.8617283951  32.098765432  0.0048207472  0.3475599289  650           0.0663210630 
0.8620370370  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8839506173  0.8037037037  0.8901234568  0.8703703704  34.567901234  0.0052306566  0.3475599289  700           0.0631746674 
0.8592592593  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8802469136  0.7876543210  0.8925925926  0.8765432099  37.037037037  0.0059337483  0.3475599289  750           0.0662009764 
0.8620370370  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8851851852  0.7987654321  0.8925925926  0.8716049383  39.506172839  0.0071625207  0.3475599289  800           0.0666002941 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3648148148  0.4211469534  0.4071428571  0.3703703704  0.3580246914  0.3641975309  0.3580246914  0.3672839506  0.3456790123  0.3802469136  0.3580246914  0.3617283951  0.3592592593  0.0000000000  3.6515154839  0.1006770134  0             0.4134094715 
0.8432098765  0.9964157706  0.9571428571  0.9228395062  0.8456790123  0.9197530864  0.8580246914  0.9074074074  0.8209876543  0.8234567901  0.8037037037  0.8580246914  0.8876543210  2.8673835125  1.6637109447  0.2654094696  50            0.0642381811 
0.8027777778  0.9982078853  0.9714285714  0.9969135802  0.9753086420  0.9922839506  0.9938271605  0.9783950617  0.9506172840  0.8037037037  0.7827160494  0.7901234568  0.8345679012  5.7347670251  0.4051722279  0.2654094696  100           0.0606488085 
0.7564814815  0.9964157706  0.9500000000  0.9984567901  0.9814814815  0.9938271605  0.9876543210  0.9953703704  0.9629629630  0.7703703704  0.7370370370  0.7456790123  0.7728395062  8.6021505376  0.2013785847  0.2654094696  150           0.0648837519 
0.7706790123  0.9982078853  0.9714285714  0.9984567901  0.9876543210  0.9984567901  0.9876543210  0.9984567901  0.9876543210  0.7901234568  0.7419753086  0.7654320988  0.7851851852  11.469534050  0.0726428253  0.2654094696  200           0.0645836782 
0.7728395062  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9876543210  0.8135802469  0.7358024691  0.7530864198  0.7888888889  14.336917562  0.0335739277  0.2654094696  250           0.0640980434 
0.7709876543  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9938271605  0.7962962963  0.7370370370  0.7592592593  0.7913580247  17.204301075  0.0231093946  0.2654094696  300           0.0627140331 
0.7836419753  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8296296296  0.7432098765  0.7617283951  0.8000000000  20.071684587  0.0212456348  0.2654094696  350           0.0622671270 
0.7858024691  1.0000000000  0.9785714286  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8506172840  0.7419753086  0.7506172840  0.8000000000  22.939068100  0.0168003575  0.2654986382  400           0.0647171688 
0.7811728395  1.0000000000  0.9928571429  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8172839506  0.7469135802  0.7592592593  0.8012345679  25.806451612  0.0084128070  0.2654986382  450           0.0657464314 
0.7654320988  1.0000000000  0.9714285714  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9938271605  0.7901234568  0.7320987654  0.7617283951  0.7777777778  28.673835125  0.0110556732  0.3475322723  500           0.0629519653 
0.7697530864  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9938271605  0.8098765432  0.7370370370  0.7469135802  0.7851851852  31.541218638  0.0064334239  0.3475322723  550           0.0633665943 
0.7716049383  1.0000000000  0.9928571429  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9938271605  0.8209876543  0.7407407407  0.7444444444  0.7802469136  34.408602150  0.0047877658  0.3475322723  600           0.0633390808 
0.7601851852  1.0000000000  0.9785714286  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9938271605  0.8061728395  0.7271604938  0.7419753086  0.7654320988  37.275985663  0.0169784020  0.3475322723  650           0.0632317209 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2314, in update
    theta_updated_new[k] = v - self.hparams['lr'] * grad_theta[k]
KeyboardInterrupt
trails: 0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.00025
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3623456790  0.4408602151  0.4214285714  0.3719135802  0.3703703704  0.3780864198  0.3703703704  0.3703703704  0.3580246914  0.3716049383  0.3555555556  0.3592592593  0.3629629630  0.0000000000  1.1810278893  0.0539774895  0             0.4246201515 
0.7688271605  0.9892473118  0.9642857143  0.9521604938  0.9074074074  0.9645061728  0.9691358025  0.9537037037  0.8950617284  0.7641975309  0.7604938272  0.7604938272  0.7901234568  2.8673835125  0.4799948263  0.0604104996  50            0.0131172085 
0.7712962963  0.9946236559  0.9857142857  1.0000000000  0.9876543210  1.0000000000  1.0000000000  0.9922839506  0.9320987654  0.8222222222  0.7296296296  0.7395061728  0.7938271605  5.7347670251  0.0813998181  0.0604534149  100           0.0127704287 
0.7663580247  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9753086420  0.8222222222  0.7222222222  0.7320987654  0.7888888889  8.6021505376  0.0248267550  0.0604534149  150           0.0132548046 
0.7567901235  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.8061728395  0.7222222222  0.7172839506  0.7814814815  11.469534050  0.0094541812  0.0604534149  200           0.0123128939 
0.7601851852  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.8000000000  0.7283950617  0.7234567901  0.7888888889  14.336917562  0.0059331586  0.0604534149  250           0.0127003860 
0.7484567901  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9938271605  0.7851851852  0.7234567901  0.7197530864  0.7654320988  17.204301075  0.0067940809  0.0604772568  300           0.0125193739 
0.7604938272  1.0000000000  0.9785714286  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9876543210  0.8234567901  0.7111111111  0.7259259259  0.7814814815  20.071684587  0.0029959223  0.0604772568  350           0.0132947206 
0.7743827160  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8395061728  0.7358024691  0.7320987654  0.7901234568  22.939068100  0.0041402032  0.0604772568  400           0.0131447315 
0.7706790123  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8308641975  0.7296296296  0.7358024691  0.7864197531  25.806451612  0.0012838384  0.0604772568  450           0.0125809765 
0.7623456790  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9938271605  0.8172839506  0.7259259259  0.7246913580  0.7814814815  28.673835125  0.0010208350  0.0619816780  500           0.0128359604 
0.7808641975  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8382716049  0.7407407407  0.7407407407  0.8037037037  31.541218638  0.0008936316  0.0619816780  550           0.0122748804 
0.7882716049  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8567901235  0.7444444444  0.7395061728  0.8123456790  34.408602150  0.0017212098  0.0624547005  600           0.0123704720 
0.7762345679  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.8395061728  0.7283950617  0.7358024691  0.8012345679  37.275985663  0.0007453186  0.0624547005  650           0.0126307774 
0.7660493827  1.0000000000  0.9928571429  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8172839506  0.7283950617  0.7259259259  0.7925925926  40.143369175  0.0007378012  0.0624547005  700           0.0125029659 
0.7641975309  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8172839506  0.7246913580  0.7222222222  0.7925925926  43.010752688  0.0004635191  0.0624547005  750           0.0128996658 
0.7487654321  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9938271605  0.7975308642  0.7135802469  0.7259259259  0.7580246914  45.878136200  0.0007914828  0.0644817352  800           0.0125759029 
0.7734567901  1.0000000000  0.9928571429  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8271604938  0.7345679012  0.7296296296  0.8024691358  48.745519713  0.0005419355  0.0644817352  850           0.0141000652 
0.7466049383  1.0000000000  0.9785714286  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9938271605  0.7938271605  0.7098765432  0.7234567901  0.7592592593  51.612903225  0.0005055564  0.0644817352  900           0.0134714127 
0.7672839506  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8209876543  0.7296296296  0.7209876543  0.7975308642  54.480286738  0.0003676626  0.0644817352  950           0.0179695702 
0.7521604938  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9938271605  0.8037037037  0.7160493827  0.7234567901  0.7654320988  57.347670250  0.0005001596  0.0644817352  1000          0.0187694979 
0.7820987654  1.0000000000  1.0000000000  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.8333333333  0.7481481481  0.7370370370  0.8098765432  60.215053763  0.0005388203  0.0644817352  1050          0.0123422003 
0.7728395062  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8259259259  0.7296296296  0.7308641975  0.8049382716  63.082437276  0.0004024003  0.0644817352  1100          0.0124615860 
0.7796296296  1.0000000000  0.9928571429  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.8345679012  0.7444444444  0.7308641975  0.8086419753  65.949820788  0.0002837100  0.0644817352  1150          0.0129840517 
0.7589506173  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9938271605  0.8111111111  0.7222222222  0.7246913580  0.7777777778  68.817204301  0.0005342822  0.0644817352  1200          0.0126337624 
0.7620370370  1.0000000000  0.9857142857  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9876543210  0.8222222222  0.7160493827  0.7234567901  0.7864197531  71.684587813  0.0010711457  0.0644817352  1250          0.0138682222 
0.7604938272  1.0000000000  0.9928571429  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9814814815  0.8061728395  0.7271604938  0.7234567901  0.7851851852  74.551971326  0.0022304330  0.0644817352  1300          0.0136498165 
Exception ignored in: <function WeakSet.__init__.<locals>._remove at 0x7fd5277c4950>
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/_weakrefset.py", line 38, in _remove
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/util.py", line 319, in _exit_function
    p.join()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 122, in join
    assert self._parent_pid == os.getpid(), 'can only join a child process'
AssertionError: can only join a child process
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2164, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1476, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1483, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 267, in in_project_roots
    return filename_to_in_scope_cache[filename]
KeyError: '/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 557, in get_abs_path_real_path_and_base_from_file
KeyError: '/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/typeshed/stubs/pycurl'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 230, in _NormPaths
    return NORM_PATHS_CONTAINER[filename]
KeyError: '/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/typeshed/stubs/pycurl'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2166, in main
    handle_keyboard_interrupt()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1959, in handle_keyboard_interrupt
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 607, in in_project_scope
    return pydevd_utils.in_project_roots(filename)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 280, in in_project_roots
    library_roots = _get_library_roots()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 261, in _get_library_roots
    return _get_roots(library_roots_cache, 'LIBRARY_ROOTS', set_library_roots, _get_default_library_roots)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 208, in _get_roots
    set_when_not_cached(roots)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 256, in set_library_roots
    roots = _set_roots(roots, _LIBRARY_ROOTS_CACHE)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 188, in _set_roots
    new_roots.append(_normpath(root))
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_utils.py", line 27, in _normpath
    return pydevd_file_utils.get_abs_path_real_path_and_base_from_file(filename)[0]
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 571, in get_abs_path_real_path_and_base_from_file
    abs_path, real_path = _NormPaths(f)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 237, in _NormPaths
    abs_path = _NormPath(filename, os.path.abspath)
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd_file_utils.py", line 246, in _NormPath
    r = normpath(filename)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/posixpath.py", line 376, in abspath
    def abspath(path):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
RuntimeError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2173, in <module>
    main()
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 2166, in main
    handle_keyboard_interrupt()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 30702) is killed by signal: Terminated. 
trails: 0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.00025
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1, 2, 3, 4]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 176, in accuracy
    p = network.predict(x)   # ARMpredict
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1, 2, 3, 4]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_test_ac  env2_test_ac  env3_test_ac  env4_test_ac  epoch         loss          mem_gb        step          step_time    
0.3253086420  0.3244598765  0.3410493827  0.3148148148  0.3098765432  0.3444444444  0.3320987654  0.0000000000  1.1505750418  0.0472397804  0             0.3171467781 
0.7481481481  0.8699845679  0.8364197531  0.7827160494  0.7395061728  0.7271604938  0.7432098765  0.6172839506  0.7272898424  0.1052832603  50            0.0335891819 
0.7740740741  0.9810956790  0.9691358025  0.7753086420  0.7444444444  0.7765432099  0.8000000000  1.2345679012  0.2735281099  0.1052832603  100           0.0337055254 
0.8280864198  0.9949845679  0.9876543210  0.8333333333  0.7790123457  0.8444444444  0.8555555556  1.8518518519  0.1328883277  0.1052832603  150           0.0364984608 
0.7981481481  0.9957561728  0.9845679012  0.8049382716  0.7654320988  0.8086419753  0.8135802469  2.4691358025  0.0561554940  0.1052832603  200           0.0380595827 
0.8000000000  0.9976851852  0.9861111111  0.8074074074  0.7728395062  0.8086419753  0.8111111111  3.0864197531  0.0397662835  0.1052832603  250           0.0350378370 
0.8055555556  0.9969135802  0.9861111111  0.8172839506  0.7629629630  0.8185185185  0.8234567901  3.7037037037  0.0168366972  0.1052832603  300           0.0351450062 
0.8398148148  0.9972993827  0.9891975309  0.8617283951  0.7580246914  0.8629629630  0.8765432099  4.3209876543  0.0212583187  0.1052832603  350           0.0338712931 
0.7756172840  0.9984567901  0.9891975309  0.7802469136  0.7456790123  0.7950617284  0.7814814815  4.9382716049  0.0182811207  0.1052832603  400           0.0358534813 
0.7790123457  0.9992283951  0.9922839506  0.7864197531  0.7444444444  0.8000000000  0.7851851852  5.5555555556  0.0167222796  0.1052832603  450           0.0388051462 
0.7975308642  0.9988425926  0.9938271605  0.7975308642  0.7567901235  0.8271604938  0.8086419753  6.1728395062  0.0103928396  0.1052832603  500           0.0394721746 
0.7657407407  1.0000000000  0.9938271605  0.7864197531  0.7296296296  0.7814814815  0.7654320988  6.7901234568  0.0124797705  0.1052832603  550           0.0377271891 
0.7549382716  0.9996141975  0.9922839506  0.7728395062  0.7259259259  0.7703703704  0.7506172840  7.4074074074  0.0060419167  0.1052832603  600           0.0403755808 
0.7993827160  0.9992283951  0.9891975309  0.8135802469  0.7518518519  0.8283950617  0.8037037037  8.0246913580  0.0078650925  0.1052832603  650           0.0397491360 
0.7824074074  1.0000000000  0.9922839506  0.7938271605  0.7481481481  0.8049382716  0.7827160494  8.6419753086  0.0021831221  0.1052832603  700           0.0415963030 
0.7740740741  0.9996141975  0.9938271605  0.7864197531  0.7271604938  0.7975308642  0.7851851852  9.2592592593  0.0042387277  0.1052832603  750           0.0397348309 
0.8237654321  1.0000000000  0.9953703704  0.8370370370  0.7604938272  0.8506172840  0.8469135802  9.8765432099  0.0037999764  0.1052832603  800           0.0390646076 
0.7981481481  0.9996141975  0.9938271605  0.8123456790  0.7555555556  0.8333333333  0.7913580247  10.493827160  0.0080451505  0.1052832603  850           0.0406059456 
0.8172839506  0.9996141975  0.9969135802  0.8370370370  0.7629629630  0.8493827160  0.8197530864  11.111111111  0.0014259186  0.1052832603  900           0.0385026026 
0.8274691358  0.9996141975  0.9953703704  0.8333333333  0.7641975309  0.8641975309  0.8481481481  11.728395061  0.0033418580  0.1052832603  950           0.0356772327 
0.7845679012  1.0000000000  0.9938271605  0.7987654321  0.7469135802  0.8074074074  0.7851851852  12.345679012  0.0080161508  0.1052832603  1000          0.0374316692 
0.7935185185  1.0000000000  0.9969135802  0.8049382716  0.7518518519  0.8259259259  0.7913580247  12.962962963  0.0023176458  0.1052832603  1050          0.0360496616 
0.8203703704  1.0000000000  0.9922839506  0.8271604938  0.7654320988  0.8691358025  0.8197530864  13.580246913  0.0035940860  0.1052832603  1100          0.0382619524 
0.8407407407  1.0000000000  0.9938271605  0.8567901235  0.7592592593  0.8654320988  0.8814814815  14.197530864  0.0015622632  0.1052832603  1150          0.0375819826 
0.7679012346  1.0000000000  0.9938271605  0.7851851852  0.7382716049  0.7864197531  0.7617283951  14.814814814  0.0017551947  0.1053047180  1200          0.0360562420 
0.8197530864  1.0000000000  0.9938271605  0.8308641975  0.7691358025  0.8432098765  0.8358024691  15.432098765  0.0018149268  0.1053047180  1250          0.0361101055 
0.8074074074  1.0000000000  0.9953703704  0.8234567901  0.7592592593  0.8432098765  0.8037037037  16.049382716  0.0010380374  0.1053047180  1300          0.0355924654 
0.8240740741  1.0000000000  0.9969135802  0.8345679012  0.7691358025  0.8456790123  0.8469135802  16.666666666  0.0034109657  0.1053047180  1350          0.0349876595 
0.7401234568  0.9996141975  0.9891975309  0.7654320988  0.7074074074  0.7555555556  0.7320987654  17.283950617  0.0027668485  0.1053047180  1400          0.0360092640 
0.8098765432  1.0000000000  0.9938271605  0.8345679012  0.7617283951  0.8259259259  0.8172839506  17.901234567  0.0262131581  0.1053047180  1450          0.0376113749 
0.7873456790  1.0000000000  0.9907407407  0.8086419753  0.7481481481  0.8098765432  0.7827160494  18.518518518  0.0014650398  0.1053047180  1500          0.0358157778 
0.7854938272  1.0000000000  0.9922839506  0.8098765432  0.7518518519  0.8086419753  0.7716049383  19.135802469  0.0054277000  0.1053047180  1550          0.0368860674 
0.7629629630  1.0000000000  0.9922839506  0.7913580247  0.7283950617  0.7827160494  0.7493827160  19.753086419  0.0017213118  0.1053047180  1600          0.0373398924 
0.7765432099  1.0000000000  0.9938271605  0.8000000000  0.7456790123  0.7962962963  0.7641975309  20.370370370  0.0020562500  0.1053047180  1650          0.0366529989 
0.7567901235  0.9984567901  0.9845679012  0.8111111111  0.6827160494  0.7604938272  0.7728395062  20.987654321  0.0117458095  0.1053047180  1700          0.0363783646 
0.8262345679  0.9996141975  0.9938271605  0.8567901235  0.7481481481  0.8543209877  0.8456790123  21.604938271  0.0104984993  0.1053047180  1750          0.0368233395 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 379, in select
    for fd, event in fd_event_list:
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1, 2, 3, 4]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_test_ac  env2_test_ac  env3_test_ac  env4_test_ac  epoch         loss          mem_gb        step          step_time    
0.3262345679  0.3391203704  0.3256172840  0.3061728395  0.3358024691  0.3382716049  0.3246913580  0.0000000000  1.1200503111  0.0472397804  0             0.3241839409 
0.7200617284  0.8321759259  0.8209876543  0.7148148148  0.7259259259  0.7172839506  0.7222222222  0.6172839506  0.6898069799  0.0744090080  50            0.0351663780 
0.8493827160  0.8834876543  0.8888888889  0.8481481481  0.8641975309  0.8308641975  0.8543209877  1.2345679012  0.3796423176  0.0744304657  100           0.0377753639 
0.8037037037  0.9432870370  0.9027777778  0.7962962963  0.8098765432  0.7925925926  0.8160493827  1.8518518519  0.3067936763  0.0744519234  150           0.0370027113 
0.7549382716  0.9120370370  0.8919753086  0.7543209877  0.7506172840  0.7543209877  0.7604938272  2.4691358025  0.2313530047  0.1053261757  200           0.0374495697 
0.8320987654  0.8993055556  0.8935185185  0.8358024691  0.8296296296  0.8123456790  0.8506172840  3.0864197531  0.2063316321  0.1053261757  250           0.0374591923 
0.8262345679  0.9409722222  0.9182098765  0.8271604938  0.8271604938  0.8098765432  0.8407407407  3.7037037037  0.1687745824  0.1053261757  300           0.0392531204 
0.8367283951  0.9737654321  0.9537037037  0.8333333333  0.8358024691  0.8283950617  0.8493827160  4.3209876543  0.1395720578  0.1053261757  350           0.0369551373 
0.8080246914  0.9907407407  0.9691358025  0.7925925926  0.8172839506  0.8049382716  0.8172839506  4.9382716049  0.1066360882  0.1053261757  400           0.0392231274 
0.8191358025  0.9888117284  0.9567901235  0.8185185185  0.8172839506  0.8160493827  0.8246913580  5.5555555556  0.0983246818  0.1053261757  450           0.0385387945 
0.7743827160  0.9884259259  0.9552469136  0.7493827160  0.7679012346  0.7827160494  0.7975308642  6.1728395062  0.0653598943  0.1053261757  500           0.0379938793 
0.8182098765  0.9864969136  0.9537037037  0.8061728395  0.8185185185  0.8185185185  0.8296296296  6.7901234568  0.0589176711  0.1053261757  550           0.0344478226 
0.8287037037  0.9961419753  0.9691358025  0.8148148148  0.8358024691  0.8320987654  0.8320987654  7.4074074074  0.0679438990  0.1053261757  600           0.0355657482 
0.8209876543  0.9965277778  0.9768518519  0.8074074074  0.8209876543  0.8246913580  0.8308641975  8.0246913580  0.0361295636  0.1053261757  650           0.0352615356 
0.7975308642  0.9980709877  0.9845679012  0.7790123457  0.7962962963  0.8037037037  0.8111111111  8.6419753086  0.0203423077  0.1053261757  700           0.0360894632 
0.7706790123  0.9969135802  0.9753086420  0.7493827160  0.7641975309  0.7777777778  0.7913580247  9.2592592593  0.0238645207  0.1053261757  750           0.0376674366 
0.8197530864  0.9992283951  0.9768518519  0.8061728395  0.8172839506  0.8197530864  0.8358024691  9.8765432099  0.0208512313  0.1053261757  800           0.0374231815 
0.7885802469  0.9992283951  0.9814814815  0.7555555556  0.7864197531  0.7938271605  0.8185185185  10.493827160  0.0146159077  0.1053261757  850           0.0374396324 
0.8092592593  1.0000000000  0.9845679012  0.7987654321  0.8086419753  0.8111111111  0.8185185185  11.111111111  0.0077494796  0.1053261757  900           0.0395988512 
0.8148148148  1.0000000000  0.9861111111  0.8061728395  0.8098765432  0.8123456790  0.8308641975  11.728395061  0.0092184576  0.1053261757  950           0.0411161566 
0.8206790123  0.9996141975  0.9830246914  0.8037037037  0.8209876543  0.8209876543  0.8370370370  12.345679012  0.0111538122  0.1053261757  1000          0.0373464298 
0.8043209877  1.0000000000  0.9799382716  0.7728395062  0.8000000000  0.8148148148  0.8296296296  12.962962963  0.0152916566  0.1053261757  1050          0.0400496435 
0.8305555556  1.0000000000  0.9845679012  0.8049382716  0.8320987654  0.8333333333  0.8518518519  13.580246913  0.0108636177  0.1053261757  1100          0.0396706867 
0.8432098765  1.0000000000  0.9861111111  0.8098765432  0.8444444444  0.8444444444  0.8740740741  14.197530864  0.0067946777  0.1053261757  1150          0.0384035587 
0.8037037037  1.0000000000  0.9861111111  0.7814814815  0.8049382716  0.8111111111  0.8172839506  14.814814814  0.0095986229  0.1053261757  1200          0.0387512016 
0.8151234568  1.0000000000  0.9907407407  0.8024691358  0.8086419753  0.8185185185  0.8308641975  15.432098765  0.0059639490  0.1053261757  1250          0.0376615763 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2308, in update
    for i, (k, v) in enumerate(self.featurizer.state_dict().items()):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1076, in state_dict
    module.state_dict(destination, prefix + name + '.', keep_vars=keep_vars)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1076, in state_dict
    module.state_dict(destination, prefix + name + '.', keep_vars=keep_vars)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1073, in state_dict
    self._save_to_state_dict(destination, prefix, keep_vars)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1034, in _save_to_state_dict
    destination[prefix + name] = param if keep_vars else param.detach()
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1, 2, 3, 4]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
kernel: coral
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1, 2, 3, 4]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_test_ac  env2_test_ac  env3_test_ac  env4_test_ac  epoch         loss          mem_gb        step          step_time    
0.4753825346  0.2625047010  0.2571428571  0.4963855422  0.4323671498  0.5312500000  0.4415274463  0.0000000000  1.1622893810  0.0472397804  0             0.3516986370 
0.7405576810  0.8672433246  0.8721804511  0.7638554217  0.7608695652  0.7716346154  0.6658711217  0.3008649868  0.6651511770  0.0744090080  50            0.0368456268 
0.7315855871  0.9180142911  0.9142857143  0.7108433735  0.8333333333  0.7091346154  0.6730310263  0.6017299737  0.3397090033  0.0744090080  100           0.0347671270 
0.7135421076  0.9398270026  0.9398496241  0.6843373494  0.8236714976  0.6802884615  0.6658711217  0.9025949605  0.2394119652  0.0745415688  150           0.0360008574 
0.6664884681  0.9323053780  0.9323308271  0.6409638554  0.6859903382  0.6802884615  0.6587112172  1.2034599473  0.2292315227  0.1053466797  200           0.0354311609 
0.7038915518  0.9512974803  0.9496240602  0.6987951807  0.7657004831  0.6995192308  0.6515513126  1.5043249342  0.2049631037  0.1053466797  250           0.0366072607 
0.6472870217  0.9170740880  0.9180451128  0.6530120482  0.6256038647  0.7043269231  0.6062052506  1.8051899210  0.1636690132  0.1053466797  300           0.0390954208 
0.6697413222  0.9845806694  0.9744360902  0.6530120482  0.7753623188  0.6658653846  0.5847255370  2.1060549079  0.1601055309  0.1053466797  350           0.0356542587 
0.6733039797  0.9778112072  0.9729323308  0.6746987952  0.7608695652  0.6514423077  0.6062052506  2.4069198947  0.1057008716  0.1053466797  400           0.0363219690 
0.6611405320  0.9838285070  0.9744360902  0.6337349398  0.7222222222  0.6418269231  0.6467780430  2.7077848815  0.0935372262  0.1053466797  450           0.0350078344 
0.6503920001  0.9827002633  0.9729323308  0.6457831325  0.7125603865  0.6370192308  0.6062052506  3.0086498684  0.0780271354  0.1053466797  500           0.0365688467 
0.5927163925  0.9877773599  0.9736842105  0.5325301205  0.7222222222  0.5504807692  0.5656324582  3.3095148552  0.0825511784  0.1053466797  550           0.0359672260 
0.6454457928  0.9763068823  0.9639097744  0.6409638554  0.6570048309  0.6394230769  0.6443914081  3.6103798420  0.0801689607  0.1053466797  600           0.0354163790 
0.6292745628  0.9924783753  0.9804511278  0.6144578313  0.6835748792  0.5961538462  0.6229116945  3.9112448289  0.0671577881  0.1054100990  650           0.0355970526 
0.6087884847  0.9966152689  0.9879699248  0.5349397590  0.6932367150  0.5745192308  0.6324582339  4.2121098157  0.0640341342  0.1054100990  700           0.0364064837 
0.6008796045  0.9951109440  0.9864661654  0.5036144578  0.6859903382  0.5432692308  0.6706443914  4.5129748026  0.0481524576  0.1054100990  750           0.0382915497 
0.6160377162  0.9956750658  0.9872180451  0.5325301205  0.7222222222  0.5793269231  0.6300715990  4.8138397894  0.0490022334  0.1054100990  800           0.0379175282 
0.5769590168  0.9958631064  0.9849624060  0.5132530120  0.7004830918  0.4759615385  0.6181384248  5.1147047762  0.0353299382  0.1054100990  850           0.0359625340 
0.6238369809  0.9958631064  0.9834586466  0.6144578313  0.6666666667  0.5865384615  0.6276849642  5.4155697631  0.0435173530  0.1054100990  900           0.0350808525 
0.5955886441  0.9956750658  0.9872180451  0.5301204819  0.6811594203  0.5600961538  0.6109785203  5.7164347499  0.0341122295  0.1054100990  950           0.0366677427 
0.6196980840  0.9984956751  0.9879699248  0.5975903614  0.7028985507  0.5697115385  0.6085918854  6.0172997367  0.0355111315  0.1054100990  1000          0.0353955603 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2303, in update
    meta_train_loss_dg.backward(create_graph=True)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1, 2, 3, 4]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_test_ac  env2_test_ac  env3_test_ac  env4_test_ac  epoch         loss          mem_gb        step          step_time    
0.4243128333  0.2408800301  0.2338345865  0.4216867470  0.4009661836  0.4855769231  0.3890214797  0.0000000000  1.1622893810  0.0472397804  0             0.3491690159 
0.6671394485  0.7783001128  0.7947368421  0.6819277108  0.6570048309  0.7043269231  0.6252983294  0.3008649868  0.8515839207  0.0744090080  50            0.0372166061 
0.7111534350  0.8715682587  0.8774436090  0.7301204819  0.7632850242  0.7187500000  0.6324582339  0.6017299737  0.4828666562  0.0744228363  100           0.0366440582 
0.7689793372  0.8879277924  0.8857142857  0.8048192771  0.8478260870  0.7836538462  0.6396181384  0.9025949605  0.3543411708  0.1052970886  150           0.0389096117 
0.7194578363  0.9225272659  0.9187969925  0.7204819277  0.7632850242  0.7043269231  0.6897374702  1.2034599473  0.3120804131  0.1052970886  200           0.0346331978 
0.7574833267  0.9298608499  0.9270676692  0.7542168675  0.8478260870  0.7620192308  0.6658711217  1.5043249342  0.2954356852  0.1052970886  250           0.0355456495 
0.6970602145  0.9232794284  0.9203007519  0.6746987952  0.6835748792  0.7187500000  0.7112171838  1.8051899210  0.2454497904  0.1052970886  300           0.0348759985 
0.7021949767  0.9552463332  0.9503759398  0.7108433735  0.7729468599  0.7235576923  0.6014319809  2.1060549079  0.2215418431  0.1052970886  350           0.0355810547 
0.7081688597  0.9567506581  0.9488721805  0.7204819277  0.7777777778  0.7067307692  0.6276849642  2.4069198947  0.1834651624  0.1052970886  400           0.0352793884 
0.6899437470  0.9586310643  0.9578947368  0.6795180723  0.7101449275  0.6923076923  0.6778042959  2.7077848815  0.1613719136  0.1052970886  450           0.0355158091 
0.6930489948  0.9432117337  0.9368421053  0.7349397590  0.6908212560  0.7187500000  0.6276849642  3.0086498684  0.1526855106  0.1052970886  500           0.0340387964 
0.6732678111  0.9731101918  0.9601503759  0.6626506024  0.7439613527  0.6754807692  0.6109785203  3.3095148552  0.1465804276  0.1052970886  550           0.0353663111 
0.6593716608  0.9652124859  0.9571428571  0.6650602410  0.6811594203  0.6802884615  0.6109785203  3.6103798420  0.1274551700  0.1052970886  600           0.0353738403 
0.6515661105  0.9676570139  0.9533834586  0.6626506024  0.6666666667  0.6802884615  0.5966587112  3.9112448289  0.1229465310  0.1052970886  650           0.0369913673 
0.6214877209  0.9759308011  0.9609022556  0.5807228916  0.6859903382  0.6201923077  0.5990453461  4.2121098157  0.1276669396  0.1052970886  700           0.0360974693 
0.6412550337  0.9836404663  0.9721804511  0.6024096386  0.6859903382  0.6346153846  0.6420047733  4.5129748026  0.1171402461  0.1052970886  750           0.0362135172 
0.6528084359  0.9763068823  0.9624060150  0.6506024096  0.6884057971  0.6875000000  0.5847255370  4.8138397894  0.1017632461  0.1052970886  800           0.0351826000 
0.6226106690  0.9875893193  0.9759398496  0.5590361446  0.6884057971  0.6057692308  0.6372315036  5.1147047762  0.0822510646  0.1052970886  850           0.0358051205 
0.6353341142  0.9845806694  0.9699248120  0.6409638554  0.6545893720  0.6610576923  0.5847255370  5.4155697631  0.0825787538  0.1052970886  900           0.0369234943 
0.6262407237  0.9932305378  0.9849624060  0.5783132530  0.6739130435  0.6322115385  0.6205250597  5.7164347499  0.0744529137  0.1052970886  950           0.0360020971 
0.6460855283  0.9798796540  0.9691729323  0.6433734940  0.6594202899  0.6562500000  0.6252983294  6.0172997367  0.0717299934  0.1053757668  1000          0.0342514420 
0.6280709423  0.9941707409  0.9887218045  0.6048192771  0.6690821256  0.6274038462  0.6109785203  6.3181647236  0.0885228905  0.1053757668  1050          0.0352178812 
0.6501888253  0.9949229033  0.9857142857  0.6144578313  0.6594202899  0.6538461538  0.6730310263  6.6190297104  0.0736312597  0.1053757668  1100          0.0372002506 
0.6225042071  0.9936066190  0.9834586466  0.5445783133  0.6714975845  0.5937500000  0.6801909308  6.9198946973  0.0576614967  0.1053757668  1150          0.0361131907 
0.6352396883  0.9954870252  0.9849624060  0.5855421687  0.6932367150  0.6177884615  0.6443914081  7.2207596841  0.0467700208  0.1053757668  1200          0.0349802494 
0.6262609766  0.9968033095  0.9857142857  0.5903614458  0.6811594203  0.6153846154  0.6181384248  7.5216246709  0.0694676360  0.1053757668  1250          0.0368123293 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2319, in update
    fix_nn(temp_new_featurizer, theta_updated_new)  # new
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 283, in fix_nn
    k_param_fn(model)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 269, in k_param_fn
    k_param_fn(v, name=str(k))
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 271, in k_param_fn
    k_param_fn(v, name=str(name + '.' + k))
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 273, in k_param_fn
    for (k, v) in tmp_model._parameters.items():
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
kernel: coral
Traceback (most recent call last):
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/pydevd.py", line 1483, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "/usr/pycharm/pycharm-2021.3.1/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 337, in <module>
    results['average_acc'] /= count
ZeroDivisionError: division by zero
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3317374579  0.3328313253  0.3313253012  0.3317307692  0.3293556086  0.3328313253  0.3313253012  0.3323308271  0.3293413174  0.3323308271  0.3293413174  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4467766117  0.4431137725  0.3325301205  0.3333333333  0.0000000000  9.0172615051  0.1717882156  0             0.5933041573 
0.6292228854  0.9442771084  0.9096385542  0.6490384615  0.5918854415  0.8132530120  0.7590361446  0.9654135338  0.9461077844  0.9037593985  0.9041916168  0.8343373494  0.8554216867  0.8087349398  0.8734939759  0.8689759036  0.8855421687  0.9175412294  0.8982035928  0.7493975904  0.5265700483  2.4096385542  5.1270033455  0.3231587410  50            0.2568977070 
0.7315648830  0.9638554217  0.9638554217  0.7379807692  0.6587112172  0.8132530120  0.7590361446  0.9744360902  0.9700598802  0.9759398496  0.9640718563  0.9201807229  0.9578313253  0.7575301205  0.7891566265  0.9262048193  0.9578313253  0.9640179910  0.9401197605  0.7590361446  0.7705314010  4.8192771084  2.3965737319  0.3231587410  100           0.2561784554 
0.7231640876  0.9774096386  0.9698795181  0.7475961538  0.6396181384  0.8117469880  0.7530120482  0.9864661654  0.9820359281  0.9789473684  0.9640718563  0.9066265060  0.9518072289  0.7771084337  0.8072289157  0.9201807229  0.9578313253  0.9595202399  0.9341317365  0.7469879518  0.7584541063  7.2289156627  1.9586850357  0.3231587410  150           0.2530376482 
0.7201534180  0.9774096386  0.9698795181  0.7451923077  0.6372315036  0.8147590361  0.7650602410  0.9864661654  0.9820359281  0.9849624060  0.9760479042  0.9051204819  0.9457831325  0.7801204819  0.8072289157  0.9141566265  0.9578313253  0.9625187406  0.9461077844  0.7493975904  0.7487922705  9.6385542169  1.9242012119  0.3232221603  200           0.2556837606 
0.7153312722  0.9774096386  0.9698795181  0.7355769231  0.6396181384  0.8222891566  0.7590361446  0.9864661654  0.9820359281  0.9879699248  0.9760479042  0.9036144578  0.9397590361  0.7846385542  0.8132530120  0.9141566265  0.9578313253  0.9685157421  0.9461077844  0.7421686747  0.7439613527  12.048192771  1.9226165771  0.3232221603  250           0.2601941824 
0.7093343435  0.9819277108  0.9759036145  0.7379807692  0.6229116945  0.8117469880  0.7650602410  0.9834586466  0.9820359281  0.9879699248  0.9760479042  0.8945783133  0.9277108434  0.7786144578  0.8132530120  0.9021084337  0.9457831325  0.9685157421  0.9461077844  0.7518072289  0.7246376812  14.457831325  1.8263639975  0.3232221603  300           0.2573576498 
0.7015072305  0.9819277108  0.9759036145  0.7331730769  0.6181384248  0.8207831325  0.7650602410  0.9834586466  0.9820359281  0.9909774436  0.9760479042  0.8915662651  0.9277108434  0.7831325301  0.8072289157  0.9036144578  0.9397590361  0.9730134933  0.9520958084  0.7469879518  0.7077294686  16.867469879  1.9552371144  0.3232221603  350           0.2567255688 
0.7063352382  0.9819277108  0.9759036145  0.7331730769  0.6181384248  0.8237951807  0.7650602410  0.9834586466  0.9820359281  0.9939849624  0.9760479042  0.8915662651  0.9277108434  0.7966867470  0.8253012048  0.8990963855  0.9457831325  0.9700149925  0.9401197605  0.7518072289  0.7222222222  19.277108433  1.8560969591  0.3232221603  400           0.2611820793 
0.7021024896  0.9849397590  0.9638554217  0.7283653846  0.6229116945  0.8222891566  0.7590361446  0.9834586466  0.9820359281  0.9939849624  0.9760479042  0.8945783133  0.9156626506  0.7921686747  0.8192771084  0.9051204819  0.9457831325  0.9730134933  0.9520958084  0.7469879518  0.7101449275  21.686746988  1.8484721470  0.3232221603  450           0.2546431017 
0.6936671052  0.9834337349  0.9819277108  0.7331730769  0.6157517900  0.8117469880  0.7530120482  0.9864661654  0.9820359281  0.9879699248  0.9760479042  0.8930722892  0.9337349398  0.7665662651  0.8012048193  0.9036144578  0.9397590361  0.9700149925  0.9520958084  0.7421686747  0.6835748792  24.096385542  1.8732331800  0.3232221603  500           0.2566562557 
0.7027105463  0.9864457831  0.9819277108  0.7403846154  0.6157517900  0.8102409639  0.7469879518  0.9864661654  0.9820359281  0.9909774436  0.9760479042  0.8960843373  0.9337349398  0.7650602410  0.7951807229  0.9036144578  0.9397590361  0.9700149925  0.9520958084  0.7518072289  0.7028985507  26.506024096  1.7839850807  0.3232221603  550           0.2608452749 
0.7069376549  0.9849397590  0.9759036145  0.7355769231  0.6181384248  0.8222891566  0.7590361446  0.9834586466  0.9820359281  0.9939849624  0.9760479042  0.8960843373  0.9337349398  0.7831325301  0.8072289157  0.9051204819  0.9457831325  0.9730134933  0.9520958084  0.7493975904  0.7246376812  28.915662650  1.9229835701  0.3232221603  600           0.2542356968 
0.7051261161  0.9849397590  0.9759036145  0.7307692308  0.6205250597  0.8222891566  0.7590361446  0.9834586466  0.9820359281  0.9939849624  0.9760479042  0.9051204819  0.9337349398  0.7876506024  0.8132530120  0.9051204819  0.9457831325  0.9730134933  0.9520958084  0.7469879518  0.7222222222  31.325301204  1.8689304352  0.3232221603  650           0.2653101540 
0.6991047910  0.9849397590  0.9759036145  0.7307692308  0.6157517900  0.8222891566  0.7590361446  0.9834586466  0.9820359281  0.9939849624  0.9760479042  0.8945783133  0.9277108434  0.7876506024  0.8132530120  0.9051204819  0.9457831325  0.9760119940  0.9520958084  0.7421686747  0.7077294686  33.734939759  1.8414376521  0.3232221603  700           0.2687914324 
0.7129475890  0.9789156627  0.9759036145  0.7259615385  0.6324582339  0.8237951807  0.7650602410  0.9804511278  0.9820359281  0.9939849624  0.9760479042  0.9066265060  0.9518072289  0.8117469880  0.8253012048  0.9051204819  0.9457831325  0.9760119940  0.9520958084  0.7445783133  0.7487922705  36.144578313  1.8419864440  0.3232221603  750           0.2511416912 
0.6972817574  0.9849397590  0.9638554217  0.7283653846  0.6229116945  0.8192771084  0.7590361446  0.9834586466  0.9820359281  0.9939849624  0.9760479042  0.8945783133  0.9156626506  0.7771084337  0.8072289157  0.9081325301  0.9457831325  0.9730134933  0.9520958084  0.7301204819  0.7077294686  38.554216867  1.8311324286  0.3232221603  800           0.2590255451 
0.7021068479  0.9849397590  0.9638554217  0.7259615385  0.6229116945  0.8222891566  0.7590361446  0.9834586466  0.9820359281  0.9939849624  0.9760479042  0.9066265060  0.9277108434  0.7846385542  0.8132530120  0.9111445783  0.9457831325  0.9760119940  0.9520958084  0.7445783133  0.7149758454  40.963855421  1.8235232544  0.3232221603  850           0.2562761259 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3317374579  0.3328313253  0.3313253012  0.3317307692  0.3293556086  0.3328313253  0.3313253012  0.3323308271  0.3293413174  0.3323308271  0.3293413174  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4427710843  0.4457831325  0.4467766117  0.4431137725  0.3325301205  0.3333333333  0.0000000000  9.0172615051  0.1717882156  0             0.5452287197 
0.6292228854  0.9442771084  0.9096385542  0.6490384615  0.5918854415  0.8132530120  0.7590361446  0.9654135338  0.9461077844  0.9037593985  0.9041916168  0.8343373494  0.8554216867  0.8087349398  0.8734939759  0.8689759036  0.8855421687  0.9175412294  0.8982035928  0.7493975904  0.5265700483  2.4096385542  5.1270033455  0.3231587410  50            0.2530623674 
0.7315648830  0.9638554217  0.9638554217  0.7379807692  0.6587112172  0.8132530120  0.7590361446  0.9744360902  0.9700598802  0.9759398496  0.9640718563  0.9201807229  0.9578313253  0.7575301205  0.7891566265  0.9262048193  0.9578313253  0.9640179910  0.9401197605  0.7590361446  0.7705314010  4.8192771084  2.3965737319  0.3231587410  100           0.2538855696 
0.7147045007  0.9849397590  0.9759036145  0.7211538462  0.6539379475  0.8328313253  0.7771084337  0.9849624060  0.9880239521  0.9954887218  0.9820359281  0.9668674699  0.9759036145  0.7725903614  0.8253012048  0.9578313253  0.9759036145  0.9865067466  0.9700598802  0.7373493976  0.7463768116  7.2289156627  1.6941904831  0.3231587410  150           0.2509675646 
0.7134898635  0.9939759036  0.9638554217  0.7043269231  0.6682577566  0.9006024096  0.8313253012  0.9849624060  0.9880239521  0.9954887218  0.9820359281  0.9819277108  0.9638554217  0.8162650602  0.8433734940  0.9638554217  0.9759036145  0.9970014993  0.9760479042  0.7108433735  0.7705314010  9.6385542169  1.2628002000  0.3231587410  200           0.2526069212 
0.7327742894  0.9864457831  0.9698795181  0.7187500000  0.6682577566  0.9156626506  0.8554216867  0.9849624060  0.9880239521  0.9954887218  0.9820359281  0.9819277108  0.9638554217  0.7695783133  0.8253012048  0.9834337349  0.9819277108  0.9955022489  0.9700598802  0.7469879518  0.7971014493  12.048192771  1.0008898616  0.3231587410  250           0.2719478178 
0.6990995323  0.9984939759  0.9819277108  0.6850961538  0.6396181384  0.9533132530  0.8855421687  0.9924812030  0.9820359281  0.9984962406  0.9820359281  0.9894578313  0.9819277108  0.8283132530  0.8554216867  0.9909638554  0.9879518072  0.9985007496  0.9820359281  0.7228915663  0.7487922705  14.457831325  0.7816590208  0.3231587410  300           0.2529237986 
0.7026276153  0.9954819277  0.9819277108  0.6947115385  0.6706443914  0.9593373494  0.8855421687  0.9864661654  0.9940119760  0.9984962406  0.9820359281  0.9909638554  0.9879518072  0.8539156627  0.8855421687  0.9879518072  0.9879518072  0.9985007496  0.9820359281  0.7060240964  0.7391304348  16.867469879  0.7193932694  0.3232340813  350           0.2591093063 
0.6779918020  0.9984939759  0.9819277108  0.6706730769  0.6372315036  0.9743975904  0.9096385542  0.9954887218  0.9820359281  1.0000000000  0.9880239521  0.9879518072  0.9879518072  0.8659638554  0.8855421687  0.9774096386  0.9819277108  1.0000000000  0.9880239521  0.7180722892  0.6859903382  19.277108433  0.5443392634  0.3232340813  400           0.2567394400 
0.7014039562  0.9984939759  0.9819277108  0.6971153846  0.6730310263  0.9367469880  0.8795180723  0.9924812030  0.9820359281  1.0000000000  0.9880239521  0.9939759036  0.9879518072  0.8328313253  0.8614457831  0.9969879518  0.9879518072  0.9970014993  0.9880239521  0.7156626506  0.7198067633  21.686746988  0.4804688668  0.3232340813  450           0.2523361683 
0.6881668248  0.9984939759  0.9819277108  0.6778846154  0.6706443914  0.9804216867  0.9096385542  0.9984962406  0.9940119760  1.0000000000  0.9880239521  0.9939759036  0.9879518072  0.9126506024  0.9277108434  0.9969879518  0.9879518072  1.0000000000  0.9880239521  0.6867469880  0.7173913043  24.096385542  0.4402279842  0.3232340813  500           0.2572715807 
0.6899609303  1.0000000000  0.9879518072  0.6923076923  0.6682577566  0.9804216867  0.8975903614  0.9984962406  0.9940119760  0.9984962406  0.9820359281  0.9954819277  0.9939759036  0.8689759036  0.8855421687  0.9969879518  0.9879518072  0.9985007496  0.9820359281  0.6987951807  0.7004830918  26.506024096  0.3470992495  0.3232340813  550           0.2654318237 
0.6906688249  1.0000000000  0.9879518072  0.6538461538  0.6467780430  0.9849397590  0.9036144578  0.9954887218  0.9940119760  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.9337349398  0.9277108434  0.9969879518  0.9879518072  1.0000000000  0.9880239521  0.7108433735  0.7512077295  28.915662650  0.3541929030  0.3232340813  600           0.2470758486 
0.6882446149  1.0000000000  0.9759036145  0.6706730769  0.6443914081  0.9834337349  0.9096385542  0.9984962406  0.9940119760  1.0000000000  0.9880239521  1.0000000000  0.9879518072  0.9081325301  0.8975903614  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.7036144578  0.7342995169  31.325301204  0.2789388743  0.3232340813  650           0.2550337410 
0.6767480009  1.0000000000  0.9879518072  0.6802884615  0.6467780430  0.9804216867  0.9096385542  0.9954887218  0.9820359281  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.8900602410  0.8855421687  0.9969879518  0.9879518072  1.0000000000  0.9880239521  0.7108433735  0.6690821256  33.734939759  0.2651921733  0.3232340813  700           0.2575186157 
0.6948324305  1.0000000000  0.9879518072  0.6610576923  0.6658711217  0.9894578313  0.9337349398  0.9954887218  0.9820359281  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.9442771084  0.9216867470  0.9969879518  0.9879518072  1.0000000000  0.9880239521  0.7060240964  0.7463768116  36.144578313  0.2226644331  0.3232340813  750           0.2599140120 
0.6791868658  1.0000000000  0.9879518072  0.6514423077  0.6539379475  0.9939759036  0.9397590361  1.0000000000  1.0000000000  1.0000000000  0.9880239521  1.0000000000  1.0000000000  0.9246987952  0.9156626506  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.6939759036  0.7173913043  38.554216867  0.2047778799  0.3232340813  800           0.2514172411 
0.7026914026  0.9984939759  0.9698795181  0.6610576923  0.6658711217  0.9984939759  0.9457831325  0.9984962406  0.9940119760  1.0000000000  0.9880239521  0.9984939759  0.9819277108  0.9774096386  0.9578313253  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6915662651  0.7922705314  40.963855421  0.1908246151  0.3232340813  850           0.2535644007 
0.6936435478  1.0000000000  0.9759036145  0.6610576923  0.6634844869  0.9894578313  0.9216867470  0.9984962406  0.9940119760  1.0000000000  0.9880239521  1.0000000000  0.9879518072  0.9307228916  0.9277108434  0.9984939759  0.9939759036  1.0000000000  0.9880239521  0.6867469880  0.7632850242  43.373493975  0.1707026331  0.3232340813  900           0.2567507362 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
A->B
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.0000000000  4.8320808411  0.0961599350  0             0.4216921329 
0.7722222222  0.9475308642  0.9444444444  0.9706790123  0.9629629630  0.9598765432  0.9506172840  0.9614197531  0.9074074074  0.7641975309  0.7283950617  0.7777777778  0.8185185185  2.4691358025  1.5545521456  0.2517971992  50            0.1048645973 
0.7756172840  0.9768518519  0.9753086420  0.9938271605  0.9876543210  0.9768518519  0.9814814815  0.9799382716  0.9567901235  0.7518518519  0.7432098765  0.7876543210  0.8197530864  4.9382716049  0.4660088164  0.2518033981  100           0.1091183996 
0.7509259259  0.9861111111  0.9814814815  0.9953703704  0.9938271605  0.9876543210  0.9814814815  0.9830246914  0.9567901235  0.7234567901  0.7086419753  0.7716049383  0.8000000000  7.4074074074  0.2939577638  0.2518033981  150           0.1102959681 
0.7679012346  0.9953703704  0.9876543210  1.0000000000  1.0000000000  0.9907407407  0.9814814815  0.9922839506  0.9629629630  0.7395061728  0.7185185185  0.7876543210  0.8259259259  9.8765432099  0.2039447868  0.2518033981  200           0.1075354576 
0.7950617284  0.9953703704  0.9938271605  1.0000000000  1.0000000000  0.9984567901  1.0000000000  0.9984567901  0.9753086420  0.7950617284  0.7320987654  0.8086419753  0.8444444444  12.345679012  0.1077132694  0.2518033981  250           0.1068232489 
0.7672839506  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9876543210  0.9984567901  0.9629629630  0.7382716049  0.7197530864  0.7888888889  0.8222222222  14.814814814  0.0839678766  0.2518033981  300           0.1074602795 
0.7728395062  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9984567901  0.9938271605  1.0000000000  0.9629629630  0.7432098765  0.7222222222  0.7987654321  0.8271604938  17.283950617  0.0452058194  0.2518033981  350           0.1049457312 
0.7648148148  0.9984567901  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.7345679012  0.7135802469  0.7876543210  0.8234567901  19.753086419  0.0326442203  0.2518033981  400           0.1037118530 
0.7793209877  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.7543209877  0.7148148148  0.8061728395  0.8419753086  22.222222222  0.0293277282  0.2518033981  450           0.1054888201 
0.7675925926  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.7259259259  0.7197530864  0.7901234568  0.8345679012  24.691358024  0.0249736027  0.2518033981  500           0.1052202559 
0.7910493827  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.7864197531  0.7209876543  0.8148148148  0.8419753086  27.160493827  0.0261221467  0.2518033981  550           0.1057898188 
0.7580246914  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9629629630  0.7222222222  0.7185185185  0.7864197531  0.8049382716  29.629629629  0.0212848244  0.3294091225  600           0.1069871902 
0.7712962963  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.7395061728  0.7148148148  0.8000000000  0.8308641975  32.098765432  0.0141886200  0.3294091225  650           0.1070203924 
0.7706790123  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  0.7382716049  0.7123456790  0.8000000000  0.8320987654  34.567901234  0.0075560460  0.3294091225  700           0.1065690374 
0.7879629630  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  0.7641975309  0.7246913580  0.8197530864  0.8432098765  37.037037037  0.0154783844  0.3294091225  750           0.1069740629 
0.7790123457  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9814814815  0.7802469136  0.7061728395  0.8037037037  0.8259259259  39.506172839  0.0097254417  0.3294091225  800           0.1104548454 
0.8101851852  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9876543210  0.9984567901  0.9753086420  0.8308641975  0.7283950617  0.8419753086  0.8395061728  41.975308642  0.0111373715  0.3294091225  850           0.1088114262 

trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 0.5
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.00025
	lr_omega: 0.007
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.3333333333  0.0000000000  4.9121079445  0.0961599350  0             0.4635732174 
0.8015432099  0.9521604938  0.9567901235  0.8750000000  0.8456790123  0.8966049383  0.9074074074  0.9058641975  0.9444444444  0.7888888889  0.8000000000  0.8148148148  0.8024691358  2.4691358025  1.8790953898  0.2517971992  50            0.1093549871 
0.8074074074  0.9722222222  0.9629629630  0.9660493827  0.9506172840  0.9552469136  0.9382716049  0.9475308642  0.9567901235  0.8024691358  0.8172839506  0.8049382716  0.8049382716  4.9382716049  0.8659567618  0.2518649101  100           0.1105178785 
0.7959876543  0.9753086420  0.9691358025  0.9814814815  0.9814814815  0.9783950617  0.9506172840  0.9783950617  0.9753086420  0.7913580247  0.7950617284  0.8049382716  0.7925925926  7.4074074074  0.5073612651  0.2518649101  150           0.1114990520 
0.8012345679  0.9953703704  0.9876543210  0.9984567901  0.9876543210  0.9922839506  0.9567901235  0.9891975309  0.9876543210  0.7938271605  0.8037037037  0.8061728395  0.8012345679  9.8765432099  0.3189231262  0.2518649101  200           0.1105787420 
0.8030864198  0.9938271605  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9506172840  0.9953703704  0.9814814815  0.7913580247  0.8037037037  0.8086419753  0.8086419753  12.345679012  0.1895463903  0.2518649101  250           0.1146722317 
0.7962962963  0.9953703704  0.9691358025  0.9984567901  1.0000000000  0.9984567901  0.9691358025  0.9969135802  0.9567901235  0.7864197531  0.7938271605  0.7975308642  0.8074074074  14.814814814  0.1345147306  0.2518649101  300           0.1143406725 
0.8145061728  1.0000000000  0.9876543210  1.0000000000  0.9938271605  1.0000000000  0.9506172840  0.9984567901  0.9876543210  0.8012345679  0.8172839506  0.8160493827  0.8234567901  17.283950617  0.0842218009  0.2518649101  350           0.1047481251 
0.7925925926  0.9922839506  0.9814814815  1.0000000000  0.9938271605  1.0000000000  0.9691358025  0.9984567901  0.9938271605  0.7839506173  0.7913580247  0.7925925926  0.8024691358  19.753086419  0.0757909235  0.2518649101  400           0.1087274313 
0.8108024691  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9506172840  1.0000000000  0.9938271605  0.7925925926  0.8160493827  0.8160493827  0.8185185185  22.222222222  0.0541494194  0.2518649101  450           0.1101384974 
0.8293209877  1.0000000000  1.0000000000  1.0000000000  0.9938271605  1.0000000000  0.9567901235  1.0000000000  0.9938271605  0.8024691358  0.8358024691  0.8333333333  0.8456790123  24.691358024  0.0364893246  0.2518649101  500           0.1099396515 
0.8157407407  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9938271605  0.7938271605  0.8197530864  0.8209876543  0.8283950617  27.160493827  0.0275079143  0.2518649101  550           0.1078910160 
0.8111111111  1.0000000000  0.9938271605  1.0000000000  0.9938271605  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.7950617284  0.8086419753  0.8160493827  0.8246913580  29.629629629  0.0226349887  0.2518649101  600           0.1030300140 
0.8015432099  0.9984567901  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9814814815  1.0000000000  0.9876543210  0.7827160494  0.8012345679  0.8049382716  0.8172839506  32.098765432  0.0280348585  0.2518649101  650           0.1066289949 
0.8324074074  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8086419753  0.8259259259  0.8432098765  0.8518518519  34.567901234  0.0267839190  0.2518649101  700           0.1108829403 
0.8296296296  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  1.0000000000  0.8123456790  0.8246913580  0.8345679012  0.8469135802  37.037037037  0.0181982674  0.2518649101  750           0.1084713125 
0.8290123457  1.0000000000  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9876543210  0.8061728395  0.8259259259  0.8308641975  0.8530864198  39.506172839  0.0086773813  0.2518649101  800           0.1123722553 
0.8376543210  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9629629630  1.0000000000  0.9938271605  0.8111111111  0.8333333333  0.8456790123  0.8604938272  41.975308642  0.0101675263  0.2518649101  850           0.1116288662 
0.8317901235  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9753086420  1.0000000000  0.9938271605  0.8074074074  0.8246913580  0.8419753086  0.8530864198  44.444444444  0.0100161721  0.2518649101  900           0.1090517092 
0.8197530864  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9753086420  1.0000000000  0.9938271605  0.7950617284  0.8148148148  0.8296296296  0.8395061728  46.913580246  0.0055345187  0.2518649101  950           0.1110142899 
0.8216049383  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9753086420  1.0000000000  0.9938271605  0.7975308642  0.8209876543  0.8246913580  0.8432098765  49.382716049  0.0099468817  0.2518649101  1000          0.1111968184 
0.8219135802  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9938271605  0.7975308642  0.8234567901  0.8283950617  0.8382716049  51.851851851  0.0066662680  0.2518649101  1050          0.1132709551 
0.8231481481  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9938271605  0.8000000000  0.8209876543  0.8283950617  0.8432098765  54.320987654  0.0042414284  0.2518649101  1100          0.1140671253 
0.8175925926  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  0.9938271605  0.7913580247  0.8160493827  0.8209876543  0.8419753086  56.790123456  0.0055497856  0.2518649101  1150          0.1114549303 
0.8256172840  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9691358025  1.0000000000  1.0000000000  0.7950617284  0.8308641975  0.8358024691  0.8407407407  59.259259259  0.0049472703  0.2518649101  1200          0.1121381044 
0.8237654321  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  1.0000000000  0.9938271605  0.7975308642  0.8234567901  0.8283950617  0.8456790123  61.728395061  0.0059253100  0.2518649101  1250          0.1149101734 
0.8361111111  1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.9753086420  1.0000000000  1.0000000000  0.8098765432  0.8358024691  0.8419753086  0.8567901235  64.197530864  0.0044236484  0.2518649101  1300          0.1114278078 
0.8225308642  0.9984567901  0.9876543210  1.0000000000  1.0000000000  1.0000000000  0.9814814815  1.0000000000  0.9938271605  0.7950617284  0.8197530864  0.8259259259  0.8493827160  66.666666666  0.0112621594  0.2518649101  1350          0.1133405209 
0.8225308642  1.0000000000  0.9938271605  1.0000000000  1.0000000000  1.0000000000  0.9753086420  1.0000000000  0.9938271605  0.7950617284  0.8197530864  0.8271604938  0.8481481481  69.135802469  0.0051260190  0.2519006729  1400          0.1140973854 
Traceback (most recent call last):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1182, in _next_data
    idx, data = self._get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1148, in _get_data
    success, data = self._try_get_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py", line 113, in get
    return _ForkingPickler.loads(res)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 282, in rebuild_storage_fd
    fd = df.detach()  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 487, in Client
    c = SocketClient(address)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 614, in SocketClient
    s.connect(address)
FileNotFoundError: [Errno 2] No such file or directory
trails: 0
Args:
	algorithm: MMD
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.00025
	mmd_gamma: 1.5
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
B->A
Start training
Python 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.16.1 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 7.16.1

Out[1]: torch.Size([1, 32])
Out[4]: 
tensor([[-1.0591e-06, -3.3152e-03, -3.2508e-03,  ..., -2.3621e-03,
         -3.4600e-03, -3.2132e-03],
        [-3.3152e-03, -7.7724e-07, -1.8370e-03,  ..., -2.3616e-03,
         -3.1456e-03, -1.6118e-03],
        [-3.2508e-03, -1.8370e-03, -1.0000e-33,  ..., -2.4650e-03,
         -2.3315e-03, -1.8247e-03],
        ...,
        [-2.3621e-03, -2.3616e-03, -2.4650e-03,  ..., -1.0000e-33,
         -2.6253e-03, -2.2340e-03],
        [-3.4600e-03, -3.1456e-03, -2.3315e-03,  ..., -2.6253e-03,
         -6.4230e-07, -3.0205e-03],
        [-3.2132e-03, -1.6118e-03, -1.8247e-03,  ..., -2.2340e-03,
         -3.0205e-03, -4.1533e-07]], device='cuda:0', grad_fn=<MulBackward0>)
Out[5]: 
tensor([[1.0591e-03, 3.3152e+00, 3.2508e+00,  ..., 2.3621e+00, 3.4600e+00,
         3.2132e+00],
        [3.3152e+00, 7.7724e-04, 1.8370e+00,  ..., 2.3616e+00, 3.1456e+00,
         1.6118e+00],
        [3.2508e+00, 1.8370e+00, 1.0000e-30,  ..., 2.4650e+00, 2.3315e+00,
         1.8247e+00],
        ...,
        [2.3621e+00, 2.3616e+00, 2.4650e+00,  ..., 1.0000e-30, 2.6253e+00,
         2.2340e+00],
        [3.4600e+00, 3.1456e+00, 2.3315e+00,  ..., 2.6253e+00, 6.4230e-04,
         3.0205e+00],
        [3.2132e+00, 1.6118e+00, 1.8247e+00,  ..., 2.2340e+00, 3.0205e+00,
         4.1533e-04]], device='cuda:0', grad_fn=<ClampMinBackward>)
Out[6]: 
tensor([[-1.0591e-06, -3.3152e-03, -3.2508e-03,  ..., -2.3621e-03,
         -3.4600e-03, -3.2132e-03],
        [-3.3152e-03, -7.7724e-07, -1.8370e-03,  ..., -2.3616e-03,
         -3.1456e-03, -1.6118e-03],
        [-3.2508e-03, -1.8370e-03, -1.0000e-33,  ..., -2.4650e-03,
         -2.3315e-03, -1.8247e-03],
        ...,
        [-2.3621e-03, -2.3616e-03, -2.4650e-03,  ..., -1.0000e-33,
         -2.6253e-03, -2.2340e-03],
        [-3.4600e-03, -3.1456e-03, -2.3315e-03,  ..., -2.6253e-03,
         -6.4230e-07, -3.0205e-03],
        [-3.2132e-03, -1.6118e-03, -1.8247e-03,  ..., -2.2340e-03,
         -3.0205e-03, -4.1533e-07]], device='cuda:0', grad_fn=<MulBackward0>)
Out[9]: 
tensor([[True, True, True,  ..., True, True, True],
        [True, True, True,  ..., True, True, True],
        [True, True, True,  ..., True, True, True],
        ...,
        [True, True, True,  ..., True, True, True],
        [True, True, True,  ..., True, True, True],
        [True, True, True,  ..., True, True, True]], device='cuda:0')
Out[11]: 
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
Out[13]: tensor([[1, 1, 1, 1]])
Out[14]: 
tensor([[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]])
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 702, in update
    penalty += self.mmd(features[i], features[j])
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 674, in mmd
    Kyy = self.gaussian_kernel(y, y).mean()
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 663, in gaussian_kernel
    D = self.my_cdist(x, y)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 660, in my_cdist
    return res.clamp_min_(1e-30)  # mintensor, min
KeyboardInterrupt
trails: 0
Args:
	algorithm: MMD
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.0001
	mmd_gamma: 1.5
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        penalty       step          step_time    
0.3341572197  0.3328313253  0.3313253012  0.3403614458  0.3373493976  0.3323308271  0.3293413174  0.3413533835  0.3293413174  0.3325301205  0.3429951691  0.3341346154  0.3269689737  0.0000000000  1.1827754974  0.0521540642  0.2556902766  0             13.543980121 
0.5525754971  0.9578313253  0.9277108434  0.8765060241  0.8313253012  0.9563909774  0.9341317365  0.9984962406  0.9700598802  0.5469879518  0.5289855072  0.4278846154  0.7064439141  2.4096385542  0.6044519666  0.0585489273  0.2734213525  50            0.0427990770 
0.6424490986  0.9864457831  0.9819277108  0.9021084337  0.8253012048  0.9864661654  0.9700598802  0.9984962406  0.9820359281  0.6602409639  0.7149758454  0.5096153846  0.6849642005  4.8192771084  0.1669485277  0.0585489273  0.2814169288  100           0.0427402353 
0.6376876221  0.9984939759  0.9819277108  0.9231927711  0.8373493976  0.9909774436  0.9760479042  0.9984962406  0.9820359281  0.6433734940  0.7439613527  0.4903846154  0.6730310263  7.2289156627  0.0939122462  0.0586481094  0.2714879957  150           0.0402579165 
0.6641413826  0.9969879518  0.9879518072  0.9367469880  0.8554216867  0.9984962406  0.9940119760  1.0000000000  1.0000000000  0.6819277108  0.7270531401  0.5793269231  0.6682577566  9.6385542169  0.0608804992  0.0586481094  0.2649414587  200           0.0421664476 
0.6598711903  0.9969879518  0.9759036145  0.9563253012  0.8614457831  0.9984962406  0.9820359281  1.0000000000  1.0000000000  0.6867469880  0.7004830918  0.5649038462  0.6873508353  12.048192771  0.0485108648  0.0586481094  0.2659849757  250           0.0414932346 
0.6497539061  1.0000000000  1.0000000000  0.9638554217  0.8795180723  0.9984962406  0.9820359281  1.0000000000  0.9880239521  0.6915662651  0.7053140097  0.5649038462  0.6372315036  14.457831325  0.0371733032  0.0586481094  0.2607142901  300           0.0420257711 
0.6545486275  1.0000000000  1.0000000000  0.9668674699  0.8795180723  0.9984962406  0.9940119760  1.0000000000  1.0000000000  0.6987951807  0.6980676329  0.5793269231  0.6420047733  16.867469879  0.0303124807  0.0586481094  0.2604900149  350           0.0414135647 
0.6509485192  0.9984939759  0.9939759036  0.9849397590  0.8915662651  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.6915662651  0.6956521739  0.5817307692  0.6348448687  19.277108433  0.0268954700  0.0586481094  0.2468106842  400           0.0415047073 
0.6521347206  1.0000000000  1.0000000000  0.9804216867  0.8855421687  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7012048193  0.6956521739  0.5649038462  0.6467780430  21.686746988  0.0257899500  0.0587100983  0.2443074429  450           0.0420231724 
0.6527239654  1.0000000000  1.0000000000  0.9563253012  0.8373493976  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7012048193  0.6787439614  0.5889423077  0.6420047733  24.096385542  0.0233596577  0.0587100983  0.2434745422  500           0.0418704939 
0.6502983972  1.0000000000  1.0000000000  0.9759036145  0.8795180723  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.7012048193  0.6642512077  0.5913461538  0.6443914081  26.506024096  0.0184715171  0.0616445541  0.2442637706  550           0.0407301188 
0.6502967967  1.0000000000  1.0000000000  0.9728915663  0.8554216867  1.0000000000  1.0000000000  1.0000000000  0.9880239521  0.6963855422  0.6545893720  0.6129807692  0.6372315036  28.915662650  0.0193084846  0.0616445541  0.2485095888  600           0.0422344255 
0.6316929654  1.0000000000  1.0000000000  0.9789156627  0.8915662651  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.6939759036  0.6666666667  0.5360576923  0.6300715990  31.325301204  0.0182513319  0.0616445541  0.2351402304  650           0.0414620209 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 176, in accuracy
    p = network.predict(x)   # ARMpredict
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 128, in predict
    return self.network(x)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/networks.py", line 289, in forward
    x = self.layer5(x)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
KeyboardInterrupt
trails: 0
Args:
	algorithm: MMD
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 0.00025
	mmd_gamma: 1.5
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        penalty       step          step_time    
0.4062694922  0.3719879518  0.4277108434  0.5045180723  0.4759036145  0.4030075188  0.4550898204  0.3082706767  0.3053892216  0.4072289157  0.3961352657  0.4326923077  0.3890214797  0.0000000000  1.1827754974  0.0521540642  0.2556902766  0             0.3355553150 
0.6556903729  0.9743975904  0.9096385542  0.8719879518  0.8012048193  0.9789473684  0.9520958084  0.9699248120  0.9520958084  0.7277108434  0.6908212560  0.5240384615  0.6801909308  2.4096385542  0.3759470865  0.0585470200  0.2825606355  50            0.0418040466 
0.6599170733  1.0000000000  1.0000000000  0.9307228916  0.8313253012  0.9969924812  0.9760479042  0.9984962406  0.9940119760  0.6843373494  0.7004830918  0.5937500000  0.6610978520  4.8192771084  0.0806576190  0.0585470200  0.2757770717  100           0.0406754494 
0.6256070010  1.0000000000  1.0000000000  0.8855421687  0.7831325301  0.9984962406  0.9940119760  1.0000000000  1.0000000000  0.6843373494  0.6473429952  0.5144230769  0.6563245823  7.2289156627  0.0442526428  0.0585470200  0.2640842628  150           0.0413147163 
0.6406970159  1.0000000000  1.0000000000  0.9307228916  0.8433734940  1.0000000000  1.0000000000  1.0000000000  1.0000000000  0.6963855422  0.6425120773  0.6033653846  0.6205250597  9.6385542169  0.0299725392  0.0585470200  0.2608199283  200           0.0414060497 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1171, in _next_data
    self._shutdown_workers()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1297, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 124, in join
    res = self._popen.wait(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 47, in wait
    if not wait([self.sentinel], timeout):
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_test_ac  env5_test_ac  env6_test_ac  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.3353577431  0.3358433735  0.3313253012  0.3509036145  0.3433734940  0.3353383459  0.3293413174  0.3413533835  0.3413173653  0.3325301205  0.3454106280  0.3341346154  0.3293556086  0.0000000000  4.7056450844  0.1273574829  0             0.4369065762 
0.5664970272  0.9713855422  0.9216867470  0.8509036145  0.8012048193  0.9759398496  0.9520958084  0.9834586466  0.9580838323  0.5951807229  0.5410628019  0.4543269231  0.6754176611  2.4096385542  2.1231309021  0.3453803062  50            0.0989723110 
0.6291744125  0.9894578313  0.9698795181  0.9111445783  0.8253012048  0.9924812030  0.9820359281  1.0000000000  0.9880239521  0.6457831325  0.6884057971  0.4903846154  0.6921241050  4.8192771084  0.5639641589  0.3453803062  100           0.1010461664 
0.6406245060  0.9909638554  0.9759036145  0.9427710843  0.8433734940  0.9984962406  0.9940119760  1.0000000000  0.9760479042  0.6506024096  0.7053140097  0.5192307692  0.6873508353  7.2289156627  0.3028559899  0.3454389572  150           0.0992464399 
0.6484643400  0.9984939759  0.9819277108  0.9548192771  0.8674698795  0.9984962406  0.9940119760  1.0000000000  1.0000000000  0.6771084337  0.7004830918  0.5384615385  0.6778042959  9.6385542169  0.1812154056  0.3454389572  200           0.1004140759 
0.6394943588  1.0000000000  1.0000000000  0.9608433735  0.8674698795  0.9984962406  0.9940119760  1.0000000000  1.0000000000  0.6674698795  0.7028985507  0.5360576923  0.6515513126  12.048192771  0.1091912799  0.3454389572  250           0.1006701756 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 173, in accuracy
    for x, y in loader:
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 355, in __iter__
    return self._get_iterator()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 914, in __init__
    w.start()
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [4, 5, 6, 7]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env4_test_ac  env5_in_acc   env5_out_acc  env5_test_ac  env6_in_acc   env6_out_acc  env6_test_ac  env7_in_acc   env7_out_acc  env7_test_ac  epoch         loss          mem_gb        step          step_time    
0.4362211436  0.3343373494  0.3373493976  0.3388554217  0.3433734940  0.3338345865  0.3353293413  0.3413533835  0.3413173653  0.3358433735  0.3433734940  0.4433734940  0.3610271903  0.3433734940  0.4240963855  0.3338345865  0.3353293413  0.4409638554  0.3402985075  0.3214285714  0.4364508393  0.0000000000  9.7366523743  0.2341670990  0             0.5423059464 
0.5373768455  0.8825301205  0.8313253012  0.7650602410  0.6867469880  0.9654135338  0.9341317365  0.9263157895  0.9221556886  0.9894578313  0.9939759036  0.6554216867  0.8912386707  0.9036144578  0.4096385542  0.9353383459  0.9461077844  0.6072289157  0.9537313433  0.9583333333  0.4772182254  2.4169184290  4.5223428011  0.4479155540  50            0.2378231859 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 330, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/lib/misc.py", line 176, in accuracy
    p = network.predict(x)   # ARMpredict
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2366, in predict
    return self.phi(self.featurizer(x))
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 94, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/yfy/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1753, in linear
    return torch._C._nn.linear(input, weight, bias)
KeyboardInterrupt
trails: 0
Args:
	algorithm: FC
	checkpoint_freq: None
	data_dir: /home/yfy/Desktop/Dataset/
	dataset: Bearing
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	normlizetype: 0-1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [8, 9, 10, 11]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	beta: 1
	class_balanced: False
	data_augmentation: True
	heldout_p: 100
	lr: 0.0001
	lr_omega: 0.005
	meta_step: 1
	nonlinear_classifier: True
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0008
Load Data.
pu1,2->pu3
Start training
kernel: coral
average_acc   env0_in_acc   env0_out_acc  env10_test_a  env11_test_a  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  env7_in_acc   env7_out_acc  env8_test_ac  env9_test_ac  epoch         loss          mem_gb        step          step_time    
0.3311263872  0.3328313253  0.3313253012  0.3317307692  0.3317422434  0.3328313253  0.3313253012  0.3323308271  0.3293413174  0.3323308271  0.3293413174  0.4427710843  0.4457831325  0.4397590361  0.4457831325  0.4427710843  0.4457831325  0.4467766117  0.4431137725  0.3325301205  0.3285024155  0.0000000000  9.3139867783  0.2341670990  0             0.5503518581 
0.7242968951  0.9382530120  0.9337349398  0.7403846154  0.6658711217  0.8719879518  0.8253012048  0.9293233083  0.8982035928  0.8947368421  0.9161676647  0.7725903614  0.7771084337  0.8117469880  0.8493975904  0.7801204819  0.7349397590  0.8770614693  0.8802395210  0.7542168675  0.7367149758  2.4096385542  5.3745398045  0.4479155540  50            0.2319908428 
0.7532293068  0.9563253012  0.9216867470  0.7331730769  0.6921241050  0.8900602410  0.8253012048  0.9458646617  0.9161676647  0.9849624060  0.9640718563  0.9427710843  0.9277108434  0.8177710843  0.8373493976  0.9337349398  0.9156626506  0.9745127436  0.9580838323  0.7253012048  0.8623188406  4.8192771084  2.1609104395  0.4479155540  100           0.2304774237 
0.7224713541  0.9939759036  0.9759036145  0.7331730769  0.6873508353  0.9397590361  0.8674698795  0.9819548872  0.9640718563  0.9939849624  0.9760479042  0.9774096386  0.9819277108  0.8057228916  0.8253012048  0.9487951807  0.9518072289  0.9835082459  0.9700598802  0.6843373494  0.7850241546  7.2289156627  1.4387481725  0.4479293823  150           0.2382314348 
0.7231454836  0.9894578313  0.9457831325  0.7355769231  0.6515513126  0.9427710843  0.8674698795  0.9834586466  0.9700598802  0.9939849624  0.9760479042  0.9849397590  0.9518072289  0.7891566265  0.8192771084  0.9759036145  0.9638554217  0.9880059970  0.9640718563  0.7421686747  0.7632850242  9.6385542169  0.9780522358  0.4479293823  200           0.2332685280 
0.7182569940  0.9984939759  0.9939759036  0.7379807692  0.6730310263  0.9608433735  0.8795180723  0.9969924812  0.9880239521  0.9969924812  0.9760479042  0.9939759036  0.9879518072  0.7454819277  0.7891566265  0.9894578313  0.9819277108  0.9940029985  0.9640718563  0.7253012048  0.7367149758  12.048192771  0.7762503225  0.4479293823  250           0.2329193974 
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 307, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)   # loss 
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/algorithms.py", line 2315, in update
    theta_updated_new[k] = v - self.hparams['lr'] * grad_theta[k]
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yfy/Desktop/my_project/DomainBed-IFD/domainbed/scripts/train.py", line 82, in <module>
    for trails in range(1):
KeyboardInterrupt
